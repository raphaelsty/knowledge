{
    "https://github.com/MaxHalford/openbikes": {
        "extra-tags": [],
        "date": "2023-03-09",
        "title": "openbikes",
        "summary": "\ud83d\udeb2 Collecting and publishing bike sharing data stored at https://github.com/MaxHalford/openbikes-data",
        "tags": [
            "python"
        ]
    },
    "https://github.com/taichi-dev/taichi": {
        "extra-tags": [],
        "date": "2016-11-24",
        "title": "taichi",
        "summary": "Productive & portable high-performance programming in Python.",
        "tags": [
            "gpu",
            "differentiable-programming",
            "gpu-programming",
            "sparse-computation",
            "taichi",
            "c++",
            "computer-graphics"
        ]
    },
    "https://github.com/luyug/COIL": {
        "extra-tags": [
            "retriever"
        ],
        "date": "2021-04-12",
        "title": "COIL",
        "summary": "NAACL2021 - COIL Contextualized Lexical Retriever ",
        "tags": [
            "retrieval",
            "python"
        ]
    },
    "https://github.com/luyug/Reranker": {
        "extra-tags": [],
        "date": "2021-01-21",
        "title": "Reranker",
        "summary": "Build Text Rerankers with Deep Language Models ",
        "tags": [
            "python"
        ]
    },
    "https://github.com/savannahostrowski/gruyere": {
        "extra-tags": [],
        "date": "2023-02-20",
        "title": "gruyere",
        "summary": "A tiny (and pretty) program for viewing + killing listening ports",
        "tags": [
            "go",
            "lipgloss",
            "bubbles",
            "charmbracelet",
            "tui",
            "golang",
            "bubbletea"
        ]
    },
    "https://github.com/facebookresearch/llama": {
        "extra-tags": [],
        "date": "2023-02-14",
        "title": "llama",
        "summary": "Inference code for LLaMA models",
        "tags": [
            "python"
        ]
    },
    "https://github.com/owulveryck/onnx-go": {
        "extra-tags": [],
        "date": "2018-08-28",
        "title": "onnx-go",
        "summary": "onnx-go gives the ability to import a pre-trained neural network within Go without being linked to a framework or library.",
        "tags": [
            "neural-network",
            "protobuf",
            "go",
            "gorgonia",
            "open-source",
            "software2",
            "machine-learning",
            "onnx"
        ]
    },
    "https://github.com/lllyasviel/ControlNet": {
        "extra-tags": [],
        "date": "2023-02-01",
        "title": "ControlNet",
        "summary": "Let us control diffusion models!",
        "tags": [
            "python"
        ]
    },
    "https://github.com/explosion/radicli": {
        "extra-tags": [],
        "date": "2023-02-05",
        "title": "radicli",
        "summary": "\ud83d\udd4a\ufe0f Radically lightweight command-line interfaces",
        "tags": [
            "python",
            "command-line",
            "cli",
            "argparse",
            "command-line-interface"
        ]
    },
    "https://github.com/huggingface/peft": {
        "extra-tags": [
            "state-of-the-art",
            "tuning"
        ],
        "date": "2022-11-25",
        "title": "peft",
        "summary": "\ud83e\udd17 PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.",
        "tags": [
            "python",
            "parameter-efficient-learning",
            "diffusion",
            "llm",
            "adapter",
            "transformers",
            "pytorch"
        ]
    },
    "https://github.com/evandempsey/fp-growth": {
        "extra-tags": [
            "algorithm"
        ],
        "date": "2013-08-15",
        "title": "fp-growth",
        "summary": "Python implementation of the Frequent Pattern Growth algorithm",
        "tags": [
            "python"
        ]
    },
    "https://github.com/dennisbakhuis/pigeonXT": {
        "extra-tags": [],
        "date": "2020-05-07",
        "title": "pigeonXT",
        "summary": "\ud83d\udc26 Quickly annotate data from the comfort of your Jupyter notebook",
        "tags": [
            "python"
        ]
    },
    "https://github.com/tobymao/sqlglot": {
        "extra-tags": [],
        "date": "2021-03-13",
        "title": "sqlglot",
        "summary": "Python SQL Parser and Transpiler",
        "tags": [
            "presto",
            "translation",
            "duckdb",
            "snowflake",
            "tsql",
            "transpiler",
            "bigquery",
            "clickhouse",
            "sqlparser",
            "hive",
            "mysql",
            "postgres",
            "python",
            "optimizer",
            "spark",
            "redshift",
            "sqlite",
            "parser",
            "sql",
            "trino"
        ]
    },
    "https://github.com/facebookresearch/tart": {
        "extra-tags": [],
        "date": "2022-12-01",
        "title": "tart",
        "summary": "Code and model release for the paper \"Task-aware Retrieval with Instructions\" by Asai et al.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/promptslab/Promptify": {
        "extra-tags": [],
        "date": "2022-12-12",
        "title": "Promptify",
        "summary": "Prompt Engineering | Use GPT or other prompt based models to get structured output. Join our discord for Prompt-Engineering, LLMs and other latest research",
        "tags": [
            "openai",
            "machine-learning",
            "chatgpt3",
            "chatgpt-api",
            "prompt-tuning",
            "gpt-neo",
            "transformers",
            "gpt-3-prompts",
            "nlp",
            "bert",
            "prompts",
            "chatgpt",
            "prompting",
            "gpt3-library",
            "chatgpt-python",
            "python",
            "prompt-engineering",
            "prompt-toolkit",
            "large-language-models",
            "gpt-3",
            "gpt-2"
        ]
    },
    "https://github.com/krishnap25/mauve": {
        "extra-tags": [],
        "date": "2021-02-16",
        "title": "mauve",
        "summary": "Package to compute Mauve, a similarity score between neural text and human text. Install with `pip install mauve-text`.",
        "tags": [
            "python",
            "nlp",
            "text-generation",
            "pytorch",
            "deep-learning",
            "huggingface-transformers"
        ]
    },
    "https://github.com/mmschlk/iXAI": {
        "extra-tags": [
            "xai"
        ],
        "date": "2022-09-02",
        "title": "iXAI",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/brycedrennan/imaginAIry": {
        "extra-tags": [],
        "date": "2022-09-12",
        "title": "imaginAIry",
        "summary": "AI imagined images. Pythonic generation of stable diffusion images.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/AmineZouitine/regard.rs": {
        "extra-tags": [],
        "date": "2022-12-17",
        "title": "regard.rs",
        "summary": "Regard is a self-hosted tool written in Rust and React that tracks the time you spend working on specific projects and displays it using a GUI/CLI.  \ud83e\udd80\u269b\ufe0f ",
        "tags": [
            "contributions-welcome",
            "cli",
            "javascript",
            "react",
            "rust",
            "server",
            "time-tracker",
            "help-wanted",
            "tauri",
            "tauri-app",
            "time",
            "gui"
        ]
    },
    "https://github.com/IBM/zshot": {
        "extra-tags": [],
        "date": "2022-02-11",
        "title": "zshot",
        "summary": "Zero and Few shot named entity & relationships recognition",
        "tags": [
            "named-entity-recognition",
            "machine-learning",
            "nlp-library",
            "ned",
            "few-shot-learning",
            "zero-shot",
            "relationship-extraction",
            "nlp",
            "natural-language-understanding",
            "spacy",
            "relation-extraction",
            "python",
            "few-shot",
            "transformer",
            "ner",
            "ai",
            "deep-learning",
            "natural-language-processing",
            "zero-shot-learning",
            "pytorch"
        ]
    },
    "https://github.com/approximatelabs/sketch": {
        "extra-tags": [],
        "date": "2022-07-14",
        "title": "sketch",
        "summary": "AI code-writing assistant that understands data content",
        "tags": [
            "tabular-data",
            "datasketch",
            "python",
            "datasketches",
            "sketches",
            "data",
            "ai",
            "df",
            "data-science",
            "codex",
            "copilot",
            "pandas",
            "ds",
            "dataframe",
            "lambdaprompt",
            "gpt3"
        ]
    },
    "https://github.com/jerryjliu/gpt_index": {
        "extra-tags": [],
        "date": "2022-11-02",
        "title": "gpt_index",
        "summary": "LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM's with external data.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/lvwerra/trl": {
        "extra-tags": [],
        "date": "2020-03-27",
        "title": "trl",
        "summary": "Train transformer language models with reinforcement learning.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/MaxHalford/myriade": {
        "extra-tags": [
            "multi-label",
            "classification"
        ],
        "date": "2020-09-30",
        "title": "myriade",
        "summary": "\u2728\ud83c\udf32 Hierarchical extreme multiclass and multi-label classification.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/CarperAI/trlx": {
        "extra-tags": [],
        "date": "2022-10-03",
        "title": "trlx",
        "summary": "A repo for distributed training of language models with Reinforcement Learning via Human Feedback (RLHF)",
        "tags": [
            "reinforcement-learning",
            "python",
            "machine-learning",
            "pytorch"
        ]
    },
    "https://github.com/online-ml/light-river": {
        "extra-tags": [
            "river"
        ],
        "date": "2022-12-06",
        "title": "light-river",
        "summary": "WIP",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/f/awesome-chatgpt-prompts": {
        "extra-tags": [],
        "date": "2022-12-05",
        "title": "awesome-chatgpt-prompts",
        "summary": "This repo includes ChatGPT prompt curation to use ChatGPT better.",
        "tags": [
            "chatgpt",
            "html",
            "chatbot",
            "chatgpt-api",
            "bots",
            "language"
        ]
    },
    "https://github.com/sebp/scikit-survival": {
        "extra-tags": [],
        "date": "2016-12-26",
        "title": "scikit-survival",
        "summary": "Survival analysis built on top of scikit-learn",
        "tags": [
            "scikit-learn",
            "survival-analysis",
            "python",
            "machine-learning"
        ]
    },
    "https://github.com/greenwolf-nsk/yandex-cup-2022-recsys": {
        "extra-tags": [],
        "date": "2022-11-15",
        "title": "yandex-cup-2022-recsys",
        "summary": "2nd place solution for Next Like prediction task",
        "tags": [
            "python"
        ]
    },
    "https://github.com/UKPLab/sentence-transformers": {
        "extra-tags": [],
        "date": "2019-07-24",
        "title": "sentence-transformers",
        "summary": "Multilingual Sentence & Image Embeddings with BERT",
        "tags": [
            "python"
        ]
    },
    "https://github.com/scastiel/github-business-card": {
        "extra-tags": [
            "github"
        ],
        "date": "2022-11-12",
        "title": "github-business-card",
        "summary": "",
        "tags": [
            "typescript"
        ]
    },
    "https://github.com/microsoft/GODEL": {
        "extra-tags": [
            "large-scale",
            "models"
        ],
        "date": "2022-05-10",
        "title": "GODEL",
        "summary": "Large-scale pretrained models for goal-directed dialog",
        "tags": [
            "python",
            "language-grounding",
            "conversational-ai",
            "text-data",
            "dialogue-systems",
            "transformer",
            "machine-learning",
            "dialogue",
            "transformers",
            "dialogpt",
            "text-generation",
            "pytorch",
            "grounded-generation",
            "data-processing",
            "pretrained-model",
            "language-model"
        ]
    },
    "https://github.com/dmmiller612/bert-extractive-summarizer": {
        "extra-tags": [],
        "date": "2019-05-11",
        "title": "bert-extractive-summarizer",
        "summary": "Easy to use extractive text summarization with BERT",
        "tags": [
            "coreference",
            "python",
            "extractive-summarization",
            "pytorch",
            "bert"
        ]
    },
    "https://github.com/microsoft/LightGBM": {
        "extra-tags": [],
        "date": "2016-08-05",
        "title": "LightGBM",
        "summary": "A fast, distributed, high performance gradient boosting (GBT, GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.",
        "tags": [
            "gradient-boosting",
            "data-mining",
            "python",
            "gbm",
            "gbrt",
            "parallel",
            "microsoft",
            "gbdt",
            "kaggle",
            "machine-learning",
            "r",
            "distributed",
            "decision-trees",
            "c++",
            "lightgbm"
        ]
    },
    "https://github.com/apache/streampipes": {
        "extra-tags": [],
        "date": "2018-04-22",
        "title": "streampipes",
        "summary": "Apache StreamPipes - A self-service (Industrial) IoT toolbox to enable non-technical users to connect, analyze and explore IoT data streams.",
        "tags": [
            "analytics",
            "java",
            "iiot",
            "self-service",
            "edge",
            "iot",
            "stream-processing"
        ]
    },
    "https://github.com/valeriansaliou/sonic": {
        "extra-tags": [],
        "date": "2019-02-26",
        "title": "sonic",
        "summary": "\ud83e\udd94 Fast, lightweight & schema-less search backend. An alternative to Elasticsearch that runs on a few MBs of RAM.",
        "tags": [
            "search",
            "infrastructure",
            "rust",
            "search-engine",
            "server",
            "index",
            "graph",
            "backend",
            "search-server",
            "database"
        ]
    },
    "https://github.com/NVIDIA-Merlin/Transformers4Rec": {
        "extra-tags": [],
        "date": "2021-04-14",
        "title": "Transformers4Rec",
        "summary": "Transformers4Rec is a flexible and efficient library for sequential and session-based recommendation and works with PyTorch.",
        "tags": [
            "session-based-recommendation",
            "tabular-data",
            "python",
            "huggingface",
            "recsys",
            "transformer",
            "seq2seq",
            "nlp",
            "gtp",
            "pytorch",
            "recommender-system",
            "bert",
            "xlnet",
            "language-model"
        ]
    },
    "https://github.com/benfred/implicit": {
        "extra-tags": [],
        "date": "2016-04-17",
        "title": "implicit",
        "summary": "Fast Python Collaborative Filtering for Implicit Feedback Datasets",
        "tags": [
            "python",
            "recommendation",
            "collaborative-filtering",
            "matrix-factorization",
            "machine-learning",
            "recommendation-system",
            "recommender-system"
        ]
    },
    "https://github.com/typesense/typesense": {
        "extra-tags": [],
        "date": "2017-01-18",
        "title": "typesense",
        "summary": "Open Source alternative to Algolia and an Easier-to-Use alternative to ElasticSearch \u26a1 \ud83d\udd0d \u2728 Fast, typo tolerant, in-memory fuzzy Search Engine for building delightful search experiences",
        "tags": [
            "full-text-search",
            "typo-tolerance",
            "search-engine",
            "datastore",
            "cpp",
            "merchandising",
            "fuzzy-search",
            "synonyms",
            "in-memory",
            "instantsearch",
            "raft",
            "site-search",
            "algolia",
            "elasticsearch",
            "app-search",
            "faceting",
            "search",
            "geosearch",
            "enterprise-search",
            "database",
            "c++"
        ]
    },
    "https://github.com/coady/lupyne": {
        "extra-tags": [
            "search"
        ],
        "date": "2015-09-20",
        "title": "lupyne",
        "summary": "Pythonic search engine based on PyLucene.",
        "tags": [
            "python",
            "strawberry-graphql",
            "search-engine",
            "fastapi",
            "lucene",
            "pylucene"
        ]
    },
    "https://github.com/aksnzhy/xlearn": {
        "extra-tags": [],
        "date": "2017-06-10",
        "title": "xlearn",
        "summary": "High performance, easy-to-use, and scalable machine learning (ML) package, including linear model (LR), factorization machines (FM), and field-aware factorization machines (FFM) for Python and CLI interface.",
        "tags": [
            "ffm",
            "factorization-machines",
            "machine-learning",
            "data-analysis",
            "statistics",
            "data-science",
            "fm",
            "c++"
        ]
    },
    "https://github.com/AmineZouitine/rmt.rs": {
        "extra-tags": [],
        "date": "2022-09-24",
        "title": "rmt.rs",
        "summary": "Rmt is similar to the rm command but saves the deleted elements in the trash and restores them. Rmt is written in Rust \ud83e\udd80",
        "tags": [
            "contributions-welcome",
            "command-line",
            "cli",
            "open-source",
            "rust",
            "help-wanted",
            "command-line-tool",
            "rm"
        ]
    },
    "https://github.com/tremorlabs/tremor": {
        "extra-tags": [],
        "date": "2022-04-19",
        "title": "tremor",
        "summary": "The React library to build dashboards fast.",
        "tags": [
            "tailwindcss",
            "reactjs",
            "react-components",
            "ui-system",
            "typescript"
        ]
    },
    "https://github.com/huggingface/setfit": {
        "extra-tags": [],
        "date": "2022-06-30",
        "title": "setfit",
        "summary": "Efficient few-shot learning with Sentence Transformers",
        "tags": [
            "few-shot-learning",
            "nlp",
            "jupyter notebook",
            "sentence-transformers"
        ]
    },
    "https://github.com/aws-samples/online-machine-learning-with-river-in-aws-lambda": {
        "extra-tags": [],
        "date": "2022-09-26",
        "title": "online-machine-learning-with-river-in-aws-lambda",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/textflint/textflint": {
        "extra-tags": [],
        "date": "2021-03-06",
        "title": "textflint",
        "summary": "Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing",
        "tags": [
            "text-transformations",
            "python",
            "text-augmentation",
            "subpopulation",
            "model-robustness",
            "adversarial-samples",
            "transformation",
            "data-augmentation",
            "attack",
            "robustness-analysis"
        ]
    },
    "https://github.com/koaning/embetter": {
        "extra-tags": [
            "embeddings"
        ],
        "date": "2021-10-31",
        "title": "embetter",
        "summary": "just a bunch of useful embeddings",
        "tags": [
            "python"
        ]
    },
    "https://github.com/MaxHalford/taxi-demo-rp-mz-rv-rd-st": {
        "extra-tags": [],
        "date": "2022-08-31",
        "title": "taxi-demo-rp-mz-rv-rd-st",
        "summary": "\ud83d\ude95 Self-contained demo using Redpanda, Materialize, River, Redis, and Streamlit to predict taxi trip durations",
        "tags": [
            "real-time",
            "online-learning",
            "materialize",
            "redpanda",
            "python",
            "river",
            "machine-learning",
            "streamlit",
            "stream-processing"
        ]
    },
    "https://github.com/jesseduffield/lazydocker": {
        "extra-tags": [
            "docker"
        ],
        "date": "2019-05-18",
        "title": "lazydocker",
        "summary": "The lazier way to manage everything docker",
        "tags": [
            "go"
        ]
    },
    "https://github.com/penpot/penpot": {
        "extra-tags": [],
        "date": "2015-12-29",
        "title": "penpot",
        "summary": "Penpot - The Open-Source design & prototyping platform",
        "tags": [
            "design",
            "ui",
            "clojurescript",
            "ux-design",
            "ux-experience",
            "prototyping",
            "clojure"
        ]
    },
    "https://github.com/heartexlabs/label-studio": {
        "extra-tags": [],
        "date": "2019-06-19",
        "title": "label-studio",
        "summary": "Label Studio is a multi-type data labeling and annotation tool with standardized output format",
        "tags": [
            "image-labelling-tool",
            "annotation",
            "labeling-tool",
            "datasets",
            "image-labeling",
            "mlops",
            "annotation-tool",
            "label-studio",
            "text-annotation",
            "annotations",
            "yolo",
            "image-classification",
            "python",
            "data-labeling",
            "image-annotation",
            "deep-learning",
            "semantic-segmentation",
            "dataset",
            "labeling",
            "computer-vision",
            "boundingbox"
        ]
    },
    "https://github.com/KGQA/leaderboard": {
        "extra-tags": [],
        "date": "2021-12-16",
        "title": "leaderboard",
        "summary": "You can find the most recent KGQA benchmark numbers from publications here.",
        "tags": [
            "benchmark",
            "question-answering",
            "kgqa",
            "dataset",
            "jupyter notebook",
            "kbqa"
        ]
    },
    "https://github.com/Aquila-Network/AquilaDB": {
        "extra-tags": [],
        "date": "2019-04-19",
        "title": "AquilaDB",
        "summary": "An easy to use Neural Search Engine. Index latent vectors along with JSON metadata and do efficient k-NN search.",
        "tags": [
            "feature-vectors",
            "search-engine",
            "neural-information-retrieval",
            "nearest-neighbor-search",
            "faiss",
            "aquila",
            "vector-database",
            "python",
            "embedding",
            "approximate-nearest-neighbor-search",
            "similarity-search",
            "information-retrieval",
            "image-search",
            "video-search",
            "knn-search",
            "retrieval",
            "similarity-searches",
            "information-retrieval-engine",
            "neural-search"
        ]
    },
    "https://github.com/ferencberes/online-node2vec": {
        "extra-tags": [],
        "date": "2018-09-17",
        "title": "online-node2vec",
        "summary": "Node Embeddings in Dynamic Graphs",
        "tags": [
            "network-embedding",
            "twitter-data-analysis",
            "embeddings",
            "research",
            "representation-learning",
            "twitter-dataset",
            "jupyter notebook",
            "twitter-data",
            "temporal-networks"
        ]
    },
    "https://github.com/makcedward/nlpaug": {
        "extra-tags": [],
        "date": "2019-03-21",
        "title": "nlpaug",
        "summary": "Data augmentation for NLP ",
        "tags": [
            "natural-language-processing",
            "adversarial-attacks",
            "adversarial-example",
            "machine-learning",
            "ai",
            "augmentation",
            "nlp",
            "jupyter notebook",
            "data-science",
            "ml",
            "artificial-intelligence"
        ]
    },
    "https://github.com/AykutSarac/github-rater": {
        "extra-tags": [],
        "date": "2021-07-26",
        "title": "github-rater",
        "summary": "\ud83d\udcca Check your GitHub rating, view results and enhance your profile quality.",
        "tags": [
            "github-api",
            "react",
            "github",
            "github-rater",
            "typescript"
        ]
    },
    "https://github.com/upscayl/upscayl": {
        "extra-tags": [],
        "date": "2022-07-30",
        "title": "upscayl",
        "summary": "\ud83c\udd99 Upscayl - Free and Open Source AI Image Upscaler for Linux, MacOS and Windows built with Linux-First philosophy.",
        "tags": [
            "esrgan",
            "image",
            "gigapixel-images",
            "upscale",
            "upscayl",
            "ai",
            "typescript",
            "gigapixel",
            "upscalerimage",
            "topaz",
            "electron",
            "image-upscaling"
        ]
    },
    "https://github.com/milvus-io/milvus": {
        "extra-tags": [],
        "date": "2019-09-16",
        "title": "milvus",
        "summary": "Vector database for scalable similarity search and AI applications.",
        "tags": [
            "tensor-search",
            "nearest-neighbor-search",
            "embeddings-similarity",
            "go",
            "approximate-nearest-neighbor-search",
            "vector-search",
            "anns",
            "similarity-search",
            "faiss",
            "hnsw",
            "milvus",
            "image-search",
            "video-search",
            "database",
            "similarity-searches",
            "vector-database"
        ]
    },
    "https://github.com/zinclabs/zincsearch": {
        "extra-tags": [],
        "date": "2021-12-02",
        "title": "zincsearch",
        "summary": "ZincSearch . A lightweight alternative to elasticsearch that requires minimal resources, written in Go.",
        "tags": [
            "splunk",
            "go",
            "search",
            "logs",
            "elasticsearch",
            "logging",
            "modern",
            "golang",
            "opensearch",
            "security-tools",
            "vuejs",
            "searchengine",
            "log-analytics"
        ]
    },
    "https://github.com/janfreyberg/superintendent": {
        "extra-tags": [],
        "date": "2017-11-27",
        "title": "superintendent",
        "summary": "Practical active learning in python",
        "tags": [
            "python"
        ]
    },
    "https://github.com/AmineZouitine/React-Forum-Training": {
        "extra-tags": [],
        "date": "2022-08-15",
        "title": "React-Forum-Training",
        "summary": "\ud83e\uddd1\u200d\ud83d\udcbb\u200b Small training project in React, NodeJs, Express and MangoDB \ud83e\uddd1\u200d\ud83d\udcbb\u200b ",
        "tags": [
            "javascript"
        ]
    },
    "https://github.com/Nixtla/statsforecast": {
        "extra-tags": [],
        "date": "2021-11-24",
        "title": "statsforecast",
        "summary": "Lightning \u26a1\ufe0f fast forecasting with statistical and econometric models.",
        "tags": [
            "theta",
            "machine-learning",
            "ets",
            "seasonal-naive",
            "forecasting",
            "time-series",
            "automl",
            "arima",
            "neuralprophet",
            "predictions",
            "python",
            "econometrics",
            "prophet",
            "data-science",
            "mstl",
            "exponential-smoothing",
            "baselines",
            "fbprophet",
            "statistics",
            "naive"
        ]
    },
    "https://github.com/towhee-io/towhee": {
        "extra-tags": [],
        "date": "2021-07-13",
        "title": "towhee",
        "summary": "Towhee is a framework that is dedicated to making neural data processing pipelines simple and fast.",
        "tags": [
            "feature-extraction",
            "python",
            "unstructured-data",
            "feature-vector",
            "transformer",
            "vit",
            "embeddings",
            "embedding-vectors",
            "machine-learning",
            "image-processing",
            "milvus",
            "image-retrieval",
            "video-processing",
            "pipeline",
            "computer-vision",
            "convolutional-networks",
            "towhee",
            "vision-transformer"
        ]
    },
    "https://github.com/Machine-Learning-Tokyo/Interactive_Tools": {
        "extra-tags": [],
        "date": "2019-04-29",
        "title": "Interactive_Tools",
        "summary": "Interactive Tools for Machine Learning, Deep Learning and Math",
        "tags": [
            "interactive-tools",
            "machine-learning",
            "deep-learning"
        ]
    },
    "https://github.com/vasturiano/3d-force-graph": {
        "extra-tags": [],
        "date": "2017-02-27",
        "title": "3d-force-graph",
        "summary": "3D force-directed graph component using ThreeJS/WebGL",
        "tags": [
            "force-directed-graphs",
            "d3js",
            "threejs",
            "webgl",
            "3d-force-graph",
            "html",
            "data-visualization",
            "3d"
        ]
    },
    "https://github.com/CompVis/stable-diffusion": {
        "extra-tags": [],
        "date": "2022-08-10",
        "title": "stable-diffusion",
        "summary": "A latent text-to-image diffusion model",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/rmsolgi/geneticalgorithm": {
        "extra-tags": [],
        "date": "2020-05-01",
        "title": "geneticalgorithm",
        "summary": "Genetic Algorithm Package for Python ",
        "tags": [
            "python",
            "genetic-algorithm",
            "optimization",
            "evolutionary-algorithms",
            "black-box-optimization"
        ]
    },
    "https://github.com/vasturiano/react-force-graph": {
        "extra-tags": [],
        "date": "2018-03-14",
        "title": "react-force-graph",
        "summary": "React component for 2D, 3D, VR and AR force directed graphs",
        "tags": [
            "force-directed-graphs",
            "webgl",
            "html",
            "react",
            "vr",
            "augmented-reality",
            "3d",
            "d3-force",
            "canvas"
        ]
    },
    "https://github.com/Jezzamonn/fourier": {
        "extra-tags": [
            "interactive"
        ],
        "date": "2018-07-28",
        "title": "fourier",
        "summary": "An Interactive Introduction to Fourier Transforms",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/highcharts/highcharts": {
        "extra-tags": [
            "javascript",
            "framework"
        ],
        "date": "2010-06-11",
        "title": "highcharts",
        "summary": "Highcharts JS, the JavaScript charting framework",
        "tags": [
            "typescript"
        ]
    },
    "https://github.com/CorentinJ/Real-Time-Voice-Cloning": {
        "extra-tags": [],
        "date": "2019-05-26",
        "title": "Real-Time-Voice-Cloning",
        "summary": "Clone a voice in 5 seconds to generate arbitrary speech in real-time",
        "tags": [
            "python",
            "tensorflow",
            "tts",
            "pytorch",
            "deep-learning",
            "voice-cloning"
        ]
    },
    "https://github.com/Textualize/textual": {
        "extra-tags": [],
        "date": "2021-04-08",
        "title": "textual",
        "summary": "Textual is a TUI (Text User Interface) framework for Python inspired by modern web development.",
        "tags": [
            "python",
            "terminal",
            "cli",
            "framework",
            "tui",
            "rich"
        ]
    },
    "https://github.com/molly/web3-is-going-great": {
        "extra-tags": [],
        "date": "2021-12-08",
        "title": "web3-is-going-great",
        "summary": "A timeline of some of the greatest hits in cryptocurrencies, NFTs, and other web3 projects since the beginning of 2021",
        "tags": [
            "javascript",
            "timeline",
            "web3"
        ]
    },
    "https://github.com/charmbracelet/gum": {
        "extra-tags": [
            "tool"
        ],
        "date": "2022-06-10",
        "title": "gum",
        "summary": "A tool for glamorous shell scripts \ud83c\udf80",
        "tags": [
            "hacktoberfest",
            "bash",
            "go",
            "shell"
        ]
    },
    "https://github.com/amoffat/sh": {
        "extra-tags": [],
        "date": "2012-01-15",
        "title": "sh",
        "summary": "Python process launching",
        "tags": [
            "devops",
            "subprocess",
            "python"
        ]
    },
    "https://github.com/airbytehq/airbyte": {
        "extra-tags": [],
        "date": "2020-07-27",
        "title": "airbyte",
        "summary": "Data integration platform for ELT pipelines from APIs, databases & files to warehouses & lakes.",
        "tags": [
            "data-collection",
            "python",
            "pipeline",
            "java",
            "change-data-capture",
            "elt",
            "bigquery",
            "data-integration",
            "data-analysis",
            "data",
            "redshift",
            "data-engineering",
            "data-ingestion",
            "airbyte",
            "snowflake",
            "etl"
        ]
    },
    "https://github.com/penrose/penrose": {
        "extra-tags": [],
        "date": "2016-09-22",
        "title": "penrose",
        "summary": "Create beautiful diagrams just by typing mathematical notation in plain text.",
        "tags": [
            "domain-specific-language",
            "diagrams",
            "typescript",
            "mathematics",
            "programming-language",
            "visualization"
        ]
    },
    "https://github.com/manticoresoftware/manticoresearch": {
        "extra-tags": [],
        "date": "2017-06-28",
        "title": "manticoresearch",
        "summary": "Easy to use open source fast database for search | Good alternative to Elasticsearch now | Drop-in replacement for E in the ELK soon",
        "tags": [
            "full-text-search",
            "search",
            "bm25",
            "search-engine",
            "sql",
            "search-api",
            "search-server",
            "database",
            "sphinxsearch",
            "json",
            "stream-filtering",
            "api",
            "cpp",
            "c++",
            "mysql"
        ]
    },
    "https://github.com/adapter-hub/adapter-transformers": {
        "extra-tags": [],
        "date": "2020-04-21",
        "title": "adapter-transformers",
        "summary": "Huggingface Transformers + Adapters = \u2764\ufe0f",
        "tags": [
            "natural-language-processing",
            "python",
            "adapters",
            "transformers",
            "nlp",
            "pytorch",
            "bert"
        ]
    },
    "https://github.com/redwoodjs/redwood": {
        "extra-tags": [
            "framework"
        ],
        "date": "2019-06-09",
        "title": "redwood",
        "summary": "The App Framework for Startups",
        "tags": [
            "graphql",
            "apollo",
            "jamstack",
            "react",
            "hacktoberfest",
            "prisma",
            "typescript"
        ]
    },
    "https://github.com/gbolmier/sklearn-neighbors-benchmark": {
        "extra-tags": [],
        "date": "2020-04-04",
        "title": "sklearn-neighbors-benchmark",
        "summary": ":bar_chart: Scikit-learn nearest neighbors algorithms benchmark",
        "tags": [
            "benchmark",
            "scikit-learn",
            "nearest-neighbors-algorithms",
            "jupyter notebook"
        ]
    },
    "https://github.com/online-ml/watermill.rs": {
        "extra-tags": [],
        "date": "2022-06-14",
        "title": "watermill.rs",
        "summary": "\ud83e\udd80 Online statistics in Rust",
        "tags": [
            "crate",
            "rust",
            "online-statistics",
            "statistics",
            "streaming",
            "stream-processing"
        ]
    },
    "https://github.com/mberr/torch-ppr": {
        "extra-tags": [],
        "date": "2022-05-06",
        "title": "torch-ppr",
        "summary": "(Personalized) Page-Rank computation using PyTorch",
        "tags": [
            "pagerank",
            "python",
            "gpu",
            "personalized-pagerank",
            "pytorch"
        ]
    },
    "https://github.com/krzysiekfonal/textpipeliner": {
        "extra-tags": [],
        "date": "2016-09-06",
        "title": "textpipeliner",
        "summary": "Python library for advanced text mining",
        "tags": [
            "python"
        ]
    },
    "https://github.com/osainz59/Ask2Transformers": {
        "extra-tags": [],
        "date": "2020-06-10",
        "title": "Ask2Transformers",
        "summary": "A Framework for Textual Entailment based Zero Shot text classification",
        "tags": [
            "natural-language-processing",
            "relation-extraction",
            "python",
            "nlp-tools",
            "mnli",
            "text-classification",
            "topic-classification",
            "zero-shot",
            "transformers",
            "topic-modeling",
            "nlp",
            "pytorch",
            "deep-learning",
            "nlp-tool"
        ]
    },
    "https://github.com/kserve/kserve": {
        "extra-tags": [],
        "date": "2019-03-27",
        "title": "kserve",
        "summary": "Standardized Serverless ML Inference Platform on Kubernetes",
        "tags": [
            "kubernetes",
            "machine-learning",
            "kserve",
            "kubeflow-pipelines",
            "k8s",
            "hacktoberfest2022",
            "mlops",
            "tensorflow",
            "model-interpretability",
            "artificial-intelligence",
            "python",
            "knative",
            "kubeflow",
            "model-serving",
            "service-mesh",
            "istio",
            "hacktoberfest",
            "sklearn",
            "xgboost",
            "pytorch",
            "mlops-community"
        ]
    },
    "https://github.com/HarrieO/2022-SIGIR-plackett-luce": {
        "extra-tags": [],
        "date": "2022-04-22",
        "title": "2022-SIGIR-plackett-luce",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/pocketbase/pocketbase": {
        "extra-tags": [],
        "date": "2022-07-05",
        "title": "pocketbase",
        "summary": "Open Source realtime backend in 1 file",
        "tags": [
            "realtime",
            "go",
            "backend",
            "authentication",
            "golang"
        ]
    },
    "https://github.com/jina-ai/discoart": {
        "extra-tags": [],
        "date": "2022-06-30",
        "title": "discoart",
        "summary": "\ud83e\udea9 Create Disco Diffusion artworks in one line",
        "tags": [
            "dalle",
            "creative-art",
            "latent-diffusion",
            "discodiffusion",
            "stable-diffusion",
            "python",
            "diffusion",
            "cross-modal",
            "disco-diffusion",
            "creative-ai",
            "multimodal",
            "generative-art",
            "midjourney",
            "clip-guided-diffusion",
            "imgen",
            "prompts"
        ]
    },
    "https://github.com/selimfirat/pysad": {
        "extra-tags": [],
        "date": "2020-06-05",
        "title": "pysad",
        "summary": "Streaming Anomaly Detection Framework in Python (Outlier Detection for Streaming Data)",
        "tags": [
            "real-time",
            "python",
            "outlier-detection",
            "fraud-detection",
            "anomaly-detection",
            "incremental-learning",
            "machine-learning",
            "streaming-data",
            "anomaly",
            "intrusion-detection",
            "unsupervised-learning",
            "outliers"
        ]
    },
    "https://github.com/MaterializeInc/demos": {
        "extra-tags": [],
        "date": "2022-01-13",
        "title": "demos",
        "summary": "Demos of Materialize, the streaming database you already know how to use!",
        "tags": [
            "typescript"
        ]
    },
    "https://github.com/datascopeanalytics/traces": {
        "extra-tags": [],
        "date": "2015-03-14",
        "title": "traces",
        "summary": "A Python library for unevenly-spaced time series analysis",
        "tags": [
            "python"
        ]
    },
    "https://github.com/koaning/bulk": {
        "extra-tags": [
            "simple",
            "tool"
        ],
        "date": "2022-05-25",
        "title": "bulk",
        "summary": "A Simple Bulk Labelling Tool",
        "tags": [
            "python"
        ]
    },
    "https://github.com/deepnlpf/deepnlpf": {
        "extra-tags": [],
        "date": "2020-03-04",
        "title": "deepnlpf",
        "summary": "A Framework for Integrating Linguistic Analysis and Semantic Annotation of Text Documents.",
        "tags": []
    },
    "https://github.com/igorvlnascimento/DeepREF": {
        "extra-tags": [],
        "date": "2021-02-15",
        "title": "DeepREF",
        "summary": "Framework to train RE models",
        "tags": [
            "natural-language-processing",
            "relation-extraction",
            "python",
            "nlp",
            "relation-classification",
            "bert"
        ]
    },
    "https://github.com/Miksus/rocketry": {
        "extra-tags": [],
        "date": "2020-06-08",
        "title": "rocketry",
        "summary": "Modern scheduling library for Python",
        "tags": [
            "pydantic",
            "python",
            "framework",
            "scheduler",
            "automation",
            "scheduling",
            "automation-framework"
        ]
    },
    "https://github.com/GitJournal/GitJournal": {
        "extra-tags": [
            "mobile"
        ],
        "date": "2019-01-10",
        "title": "GitJournal",
        "summary": "Mobile first Note Taking integrated with Git",
        "tags": [
            "notes",
            "git",
            "memex",
            "note-taking",
            "dart",
            "knowledge-management",
            "markdown",
            "journal",
            "knowledge-graph",
            "notes-app",
            "google-keep"
        ]
    },
    "https://github.com/bytewax/bytewax": {
        "extra-tags": [],
        "date": "2022-02-04",
        "title": "bytewax",
        "summary": "Python Stream Processing",
        "tags": [
            "dataflow",
            "python",
            "rust",
            "html",
            "streaming-data",
            "machine-learning",
            "data-science",
            "data-engineering",
            "data-processing",
            "stream-processing"
        ]
    },
    "https://github.com/BlinkDL/RWKV-LM": {
        "extra-tags": [],
        "date": "2021-08-08",
        "title": "RWKV-LM",
        "summary": "RWKV is an RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). So it's combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, \"infinite\" ctx_len, and free sentence embedding.",
        "tags": [
            "chatgpt",
            "python",
            "transformer",
            "attention-mechanism",
            "linear-attention",
            "rwkv",
            "transformers",
            "gpt-2",
            "pytorch",
            "deep-learning",
            "rnn",
            "lstm",
            "gpt-3",
            "gpt",
            "language-model"
        ]
    },
    "https://github.com/shenweichen/DeepCTR": {
        "extra-tags": [],
        "date": "2017-10-07",
        "title": "DeepCTR",
        "summary": "Easy-to-use,Modular and Extendible package of deep-learning based CTR models .",
        "tags": [
            "deepcross",
            "xdeepfm",
            "click-through-rate",
            "esmm",
            "nfm",
            "ple",
            "recommendation",
            "ffm",
            "factorization-machines",
            "ctr",
            "dien",
            "din",
            "deepinterestnetwork",
            "mmoe",
            "python",
            "deep-learning",
            "mlr",
            "fgcnn",
            "deepfm",
            "autoint",
            "deepinterestevolutionnetwork"
        ]
    },
    "https://github.com/kuprel/min-dalle": {
        "extra-tags": [],
        "date": "2022-06-27",
        "title": "min-dalle",
        "summary": "min(DALL\u00b7E) is a fast, minimal port of DALL\u00b7E Mini to PyTorch",
        "tags": [
            "python",
            "text-to-image",
            "pytorch",
            "deep-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/online-ml/deep-river": {
        "extra-tags": [
            "deep",
            "river"
        ],
        "date": "2021-11-17",
        "title": "deep-river",
        "summary": "",
        "tags": [
            "online-deep-learning",
            "neural-network",
            "online-learning",
            "python",
            "outlier-detection",
            "incremental-learning",
            "machine-learning",
            "stream",
            "data-science",
            "pytorch",
            "deep-learning"
        ]
    },
    "https://github.com/AtomGraph/LinkedDataHub": {
        "extra-tags": [],
        "date": "2017-01-10",
        "title": "LinkedDataHub",
        "summary": "The low-code Knowledge Graph application platform. Apache license.",
        "tags": [
            "triplestore",
            "linked-data",
            "webid",
            "framework",
            "data-management",
            "owl",
            "declarative",
            "platform",
            "sparql",
            "linked-open-data",
            "semantic-web",
            "data-driven",
            "knowledge-graph",
            "ontology-driven-development",
            "openid-connect",
            "low-code",
            "rdf",
            "xslt"
        ]
    },
    "https://github.com/yandex/YaLM-100B": {
        "extra-tags": [
            "language"
        ],
        "date": "2022-06-22",
        "title": "YaLM-100B",
        "summary": "Pretrained language model with 100B parameters",
        "tags": [
            "python"
        ]
    },
    "https://github.com/chalk-diagrams/chalk": {
        "extra-tags": [],
        "date": "2022-01-04",
        "title": "chalk",
        "summary": "A declarative drawing API in Python",
        "tags": [
            "python",
            "drawing",
            "edsl",
            "cairo",
            "declarative-language"
        ]
    },
    "https://github.com/meilisearch/meilisearch-python": {
        "extra-tags": [],
        "date": "2019-12-04",
        "title": "meilisearch-python",
        "summary": "Python wrapper for the Meilisearch API",
        "tags": [
            "sdk",
            "client",
            "meilisearch",
            "python"
        ]
    },
    "https://github.com/jaredks/rumps": {
        "extra-tags": [],
        "date": "2013-07-31",
        "title": "rumps",
        "summary": "Ridiculously Uncomplicated macOS Python Statusbar apps",
        "tags": [
            "python"
        ]
    },
    "https://github.com/streamlet-dev/tributary": {
        "extra-tags": [],
        "date": "2018-09-08",
        "title": "tributary",
        "summary": "Streaming reactive and dataflow graphs in Python",
        "tags": [
            "python3",
            "python",
            "lazy-evaluation",
            "python-data-streams",
            "asynchronous",
            "websockets",
            "stream",
            "kafka",
            "reactive-data-streams",
            "data-pipeline",
            "streaming"
        ]
    },
    "https://github.com/stanfordnlp/stanza": {
        "extra-tags": [],
        "date": "2017-09-26",
        "title": "stanza",
        "summary": "Official Stanford NLP Python Library for Many Human Languages",
        "tags": [
            "natural-language-processing",
            "python",
            "named-entity-recognition",
            "corenlp",
            "machine-learning",
            "nlp",
            "universal-dependencies",
            "pytorch",
            "deep-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/cvqluu/Angular-Penalty-Softmax-Losses-Pytorch": {
        "extra-tags": [],
        "date": "2019-06-13",
        "title": "Angular-Penalty-Softmax-Losses-Pytorch",
        "summary": "Angular penalty loss functions in Pytorch (ArcFace, SphereFace, Additive Margin, CosFace) ",
        "tags": [
            "face-verification",
            "python",
            "embedding",
            "sphereface",
            "normface",
            "fmnist-dataset",
            "speaker-recognition",
            "loss-functions",
            "fashion-mnist",
            "loss-function",
            "am-softmax",
            "arcface",
            "face-recognition",
            "pytorch",
            "metric-learning"
        ]
    },
    "https://github.com/NVIDIA/TensorRT": {
        "extra-tags": [],
        "date": "2019-05-02",
        "title": "TensorRT",
        "summary": "NVIDIA\u00ae TensorRT\u2122, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applications.",
        "tags": [
            "tensorrt",
            "inference",
            "nvidia",
            "deep-learning",
            "c++"
        ]
    },
    "https://github.com/pytorch/TensorRT": {
        "extra-tags": [],
        "date": "2020-03-11",
        "title": "TensorRT",
        "summary": "PyTorch/TorchScript/FX compiler for NVIDIA GPUs using TensorRT",
        "tags": [
            "tensorrt",
            "cuda",
            "jetson",
            "machine-learning",
            "libtorch",
            "jupyter notebook",
            "nvidia",
            "deep-learning",
            "pytorch"
        ]
    },
    "https://github.com/yuanzhoulvpi2017/quick_sentence_transformers": {
        "extra-tags": [
            "onnx"
        ],
        "date": "2022-02-23",
        "title": "quick_sentence_transformers",
        "summary": "sentence-transformers to onnx",
        "tags": [
            "python",
            "transformers",
            "sentence-transformers",
            "pytorch",
            "bert"
        ]
    },
    "https://github.com/ELS-RD/transformer-deploy": {
        "extra-tags": [],
        "date": "2021-10-31",
        "title": "transformer-deploy",
        "summary": "Efficient, scalable and enterprise-grade CPU/GPU inference server for \ud83e\udd17 Hugging Face transformer models \ud83d\ude80",
        "tags": [
            "natural-language-processing",
            "python",
            "server",
            "machine-learning",
            "inference",
            "deployment",
            "deep-learning"
        ]
    },
    "https://github.com/triton-inference-server/server": {
        "extra-tags": [],
        "date": "2018-10-04",
        "title": "server",
        "summary": "The Triton Inference Server provides an optimized cloud and edge inferencing solution. ",
        "tags": [
            "python",
            "gpu",
            "machine-learning",
            "datacenter",
            "inference",
            "cloud",
            "edge",
            "deep-learning"
        ]
    },
    "https://github.com/facebookresearch/Sphere": {
        "extra-tags": [],
        "date": "2021-12-08",
        "title": "Sphere",
        "summary": "Web-scale retrieval for knowledge-intensive NLP",
        "tags": [
            "python"
        ]
    },
    "https://github.com/apple/ml-ane-transformers": {
        "extra-tags": [],
        "date": "2022-06-03",
        "title": "ml-ane-transformers",
        "summary": "Reference implementation of the Transformer architecture optimized for Apple Neural Engine (ANE)",
        "tags": [
            "python"
        ]
    },
    "https://github.com/pmbaumgartner/setfit": {
        "extra-tags": [],
        "date": "2021-12-15",
        "title": "setfit",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/joerick/pyinstrument": {
        "extra-tags": [],
        "date": "2014-03-13",
        "title": "pyinstrument",
        "summary": "\ud83d\udeb4\u00a0Call stack profiler for Python. Shows you why your code is slow!",
        "tags": [
            "async",
            "python",
            "django",
            "profile",
            "performance",
            "profiler"
        ]
    },
    "https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python": {
        "extra-tags": [],
        "date": "2022-02-07",
        "title": "Machine-Learning-for-Streaming-Data-with-Python",
        "summary": "Machine Learning for Streaming Data with Python, published by Packt",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/appsmithorg/appsmith": {
        "extra-tags": [],
        "date": "2020-06-30",
        "title": "appsmith",
        "summary": "Low code project to build admin panels, internal tools, and dashboards. Integrates with 15+ databases and any API.",
        "tags": [
            "java",
            "gui-application",
            "developer-tools",
            "gui",
            "crud",
            "workflows",
            "admin-dashboard",
            "typescript",
            "webdevelopment",
            "app-builder",
            "javascript",
            "react",
            "internal-tools",
            "low-code-framework",
            "self-hosted",
            "custom-internal",
            "hacktoberfest",
            "automation",
            "admin-panels",
            "low-code"
        ]
    },
    "https://github.com/dragonflydb/dragonfly": {
        "extra-tags": [],
        "date": "2021-12-11",
        "title": "dragonfly",
        "summary": "A modern replacement for Redis and Memcached",
        "tags": [
            "multi-threading",
            "redis",
            "memcached",
            "fibers",
            "cpp",
            "c++"
        ]
    },
    "https://github.com/ucodery/fyeah": {
        "extra-tags": [],
        "date": "2019-06-05",
        "title": "fyeah",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/ashkonf/PageRank": {
        "extra-tags": [],
        "date": "2015-05-10",
        "title": "PageRank",
        "summary": "A Python implementation of Larry's famous PageRank algorithm.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/peterbourgon/diskv": {
        "extra-tags": [
            "disk"
        ],
        "date": "2012-03-21",
        "title": "diskv",
        "summary": "A disk-backed key-value store.",
        "tags": [
            "go"
        ]
    },
    "https://github.com/nikolaydubina/go-graph-layout": {
        "extra-tags": [],
        "date": "2021-11-20",
        "title": "go-graph-layout",
        "summary": "\ud83d\udd2e Graph Layout Algorithms in Go",
        "tags": [
            "graph-visualization",
            "svg",
            "go",
            "graph"
        ]
    },
    "https://github.com/qdrant/qdrant": {
        "extra-tags": [],
        "date": "2020-05-30",
        "title": "qdrant",
        "summary": "Qdrant - Vector Search Engine and Database for the next generation of AI applications. Also available in the cloud https://cloud.qdrant.io/",
        "tags": [
            "search-engine",
            "search-engines",
            "machine-learning",
            "vector-search-engine",
            "nearest-neighbor-search",
            "knn-algorithm",
            "hnsw",
            "mlops",
            "vector-database",
            "matching",
            "neural-network",
            "approximate-nearest-neighbor-search",
            "vector-search",
            "similarity-search",
            "image-search",
            "recommender-system",
            "embeddings-similarity",
            "search",
            "rust",
            "neural-search"
        ]
    },
    "https://github.com/nsqio/nsq": {
        "extra-tags": [],
        "date": "2012-05-12",
        "title": "nsq",
        "summary": "A realtime distributed messaging platform",
        "tags": [
            "messaging",
            "go",
            "queue",
            "message-queue",
            "distributed-systems",
            "nsq"
        ]
    },
    "https://github.com/koaning/skedulord": {
        "extra-tags": [
            "logs",
            "fun"
        ],
        "date": "2019-10-16",
        "title": "skedulord",
        "summary": "captures logs and makes cron more fun",
        "tags": [
            "python"
        ]
    },
    "https://github.com/librahu/HIN-Datasets-for-Recommendation-and-Network-Embedding": {
        "extra-tags": [],
        "date": "2018-11-12",
        "title": "HIN-Datasets-for-Recommendation-and-Network-Embedding",
        "summary": "Heterogeneous Information Network Datasets for Recommendation and Network Embedding",
        "tags": []
    },
    "https://github.com/acl-org/acl-style-files": {
        "extra-tags": [],
        "date": "2021-12-16",
        "title": "acl-style-files",
        "summary": "Official style files for papers submitted to venues of the Association for Computational Linguistics",
        "tags": [
            "tex"
        ]
    },
    "https://github.com/NicolasBizzozzero/Inpainting": {
        "extra-tags": [],
        "date": "2018-04-13",
        "title": "Inpainting",
        "summary": "Image inpainting via dictionary learning and sparse representation.",
        "tags": [
            "python",
            "inpainting-methods",
            "lasso",
            "dictionary-learning",
            "inpainting",
            "machine-learning",
            "paper-implementations",
            "image-inpainting",
            "inpaint"
        ]
    },
    "https://github.com/TorchUQ/torchuq": {
        "extra-tags": [],
        "date": "2021-04-20",
        "title": "torchuq",
        "summary": "A library for uncertainty quantification based on PyTorch",
        "tags": [
            "uncertainty-quantification",
            "uncertainty",
            "jupyter notebook",
            "pytorch",
            "deep-learning"
        ]
    },
    "https://github.com/AmineZouitine/Cpad": {
        "extra-tags": [],
        "date": "2022-04-18",
        "title": "Cpad",
        "summary": "\u200b\ud83d\udcdd\u200b Do you use several commands in your terminal, one after the other? This tool allows you to combine multiple templated bash commands with the alias of your choice and many others.",
        "tags": [
            "shell",
            "productivity",
            "terminal",
            "cli",
            "cpp",
            "c++"
        ]
    },
    "https://github.com/re-data/re-data": {
        "extra-tags": [
            "data"
        ],
        "date": "2020-11-02",
        "title": "re-data",
        "summary": "re_data - fix data issues before your users & CEO would discover them \ud83d\ude0a",
        "tags": [
            "data-quality-checks",
            "open-source-tooling",
            "data-quality",
            "dataquality",
            "dbt",
            "html",
            "data-analysis",
            "dbt-packages",
            "data-testing",
            "data-reliability",
            "data-observability",
            "data-quality-monitoring",
            "data-monitoring"
        ]
    },
    "https://github.com/ddangelov/Top2Vec": {
        "extra-tags": [],
        "date": "2020-03-20",
        "title": "Top2Vec",
        "summary": "Top2Vec learns jointly embedded topic, document and word vectors.",
        "tags": [
            "document-embedding",
            "python",
            "topic-vector",
            "semantic-search",
            "text-semantic-similarity",
            "topic-modeling",
            "top2vec",
            "topic-modelling",
            "word-embeddings",
            "topic-search",
            "pre-trained-language-models",
            "sentence-transformers",
            "text-search",
            "sentence-encoder",
            "bert"
        ]
    },
    "https://github.com/rilldata/rill-developer": {
        "extra-tags": [],
        "date": "2021-12-09",
        "title": "rill-developer",
        "summary": "Rill is a tool for effortlessly transforming data sets into powerful, opinionated dashboards using SQL.  BI-as-code.",
        "tags": [
            "gcs",
            "sveltejs",
            "duckdb",
            "sveltekit",
            "bi",
            "data-visualization",
            "parquet",
            "data-analysis",
            "parquet-tools",
            "dataviz",
            "csv",
            "go",
            "sql-editor",
            "svelte",
            "business-analytics",
            "sql",
            "data",
            "golang",
            "s3",
            "parquet-viewer"
        ]
    },
    "https://github.com/snap-stanford/GreaseLM": {
        "extra-tags": [],
        "date": "2021-11-09",
        "title": "GreaseLM",
        "summary": "[ICLR 2022 spotlight]GreaseLM: Graph REASoning Enhanced Language Models for Question Answering",
        "tags": [
            "python",
            "question-answering",
            "commonsense-reasoning",
            "graph-neural-networks",
            "knowledge-graph",
            "biomedical-ques",
            "language-model"
        ]
    },
    "https://github.com/facebookresearch/SEAL": {
        "extra-tags": [],
        "date": "2022-04-12",
        "title": "SEAL",
        "summary": "Search Engines with Autoregressive Language models",
        "tags": [
            "python"
        ]
    },
    "https://github.com/replicate/cog": {
        "extra-tags": [],
        "date": "2021-02-26",
        "title": "cog",
        "summary": "Containers for machine learning",
        "tags": [
            "docker",
            "python",
            "cuda",
            "machine-learning",
            "tensorflow",
            "containers",
            "pytorch",
            "deep-learning"
        ]
    },
    "https://github.com/bashtage/arch": {
        "extra-tags": [],
        "date": "2014-08-29",
        "title": "arch",
        "summary": "ARCH models in Python",
        "tags": [
            "bootstrap",
            "volatility",
            "spa",
            "model-confidence-set",
            "arch",
            "reality-check",
            "forecasting",
            "time-series",
            "financial-econometrics",
            "df-gls",
            "unit-root",
            "phillips-perron",
            "finance",
            "variance",
            "python",
            "adf",
            "dickey-fuller",
            "multiple-comparison-procedures",
            "risk"
        ]
    },
    "https://github.com/MaterializeInc/mz-hack-day-2022": {
        "extra-tags": [],
        "date": "2022-01-25",
        "title": "mz-hack-day-2022",
        "summary": "Official repo for the Materialize + Redpanda + dbt Hack Day 2022, including a sample project to get everyone started!",
        "tags": [
            "redpanda",
            "python",
            "sql",
            "dbt",
            "data-engineering",
            "streaming"
        ]
    },
    "https://github.com/SapienzaNLP/extend": {
        "extra-tags": [],
        "date": "2022-03-22",
        "title": "extend",
        "summary": "Entity Disambiguation as text extraction (ACL 2022)",
        "tags": [
            "natural-language-processing",
            "python",
            "entity-disambiguation-models",
            "acl",
            "text-extraction",
            "acl2022",
            "nlp",
            "entity-linking",
            "pytorch",
            "entity-disambiguation"
        ]
    },
    "https://github.com/facebookresearch/CCQA": {
        "extra-tags": [],
        "date": "2021-11-17",
        "title": "CCQA",
        "summary": "CCQA A New Web-Scale Question Answering Dataset for Model Pre-Training",
        "tags": [
            "python"
        ]
    },
    "https://github.com/patrick-kidger/equinox": {
        "extra-tags": [
            "easy-to-use"
        ],
        "date": "2021-07-29",
        "title": "equinox",
        "summary": "Elegant easy-to-use neural networks in JAX. https://docs.kidger.site/equinox/",
        "tags": [
            "neural-networks",
            "jax",
            "python",
            "deep-learning"
        ]
    },
    "https://github.com/MaterializeInc/materialize": {
        "extra-tags": [],
        "date": "2019-02-22",
        "title": "materialize",
        "summary": "Materialize is a fast, distributed SQL database built on streaming internals.",
        "tags": [
            "materialized-view",
            "sql",
            "rust",
            "postgresql",
            "postgresql-dialect",
            "distributed-systems",
            "database",
            "kafka",
            "streaming",
            "stream-processing"
        ]
    },
    "https://github.com/sqlfluff/sqlfluff": {
        "extra-tags": [],
        "date": "2018-11-01",
        "title": "sqlfluff",
        "summary": "A modular SQL linter and auto-formatter with support for multiple dialects and templated code.",
        "tags": [
            "python",
            "sql",
            "hacktoberfest",
            "sql-linter",
            "pypi"
        ]
    },
    "https://github.com/awslabs/gluonts": {
        "extra-tags": [],
        "date": "2019-05-15",
        "title": "gluonts",
        "summary": "Probabilistic time series modeling in Python",
        "tags": [
            "sagemaker",
            "python",
            "forecasting",
            "neural-networks",
            "mxnet",
            "machine-learning",
            "time-series",
            "aws",
            "data-science",
            "pytorch",
            "deep-learning",
            "time-series-forecasting",
            "time-series-prediction",
            "torch",
            "timeseries",
            "artificial-intelligence"
        ]
    },
    "https://github.com/mammothb/symspellpy": {
        "extra-tags": [],
        "date": "2018-08-13",
        "title": "symspellpy",
        "summary": "Python port of SymSpell: 1 million times faster spelling correction & fuzzy search through Symmetric Delete spelling correction algorithm ",
        "tags": [
            "approximate-string-matching",
            "fuzzy-search",
            "python",
            "chinese-text-segmentation",
            "edit-distance",
            "spell-check",
            "levenshtein-distance",
            "fuzzy-matching",
            "spelling",
            "spelling-correction",
            "levenshtein",
            "spellcheck",
            "word-segmentation",
            "text-segmentation",
            "symspell",
            "chinese-word-segmentation",
            "damerau-levenshtein"
        ]
    },
    "https://github.com/epochjs/epoch": {
        "extra-tags": [],
        "date": "2013-06-27",
        "title": "epoch",
        "summary": "A general purpose, real-time visualization library.",
        "tags": [
            "html"
        ]
    },
    "https://github.com/spotify/confidence": {
        "extra-tags": [],
        "date": "2021-03-15",
        "title": "confidence",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/sendwithus/confidence": {
        "extra-tags": [
            "make"
        ],
        "date": "2014-03-18",
        "title": "confidence",
        "summary": "Confidence.js: Make sense of your A/B test results",
        "tags": [
            "javascript"
        ]
    },
    "https://github.com/tlentali/pycht": {
        "extra-tags": [],
        "date": "2017-09-02",
        "title": "pycht",
        "summary": "\u2702\ufe0f\ud83c\udfa8 Streetart by clustering",
        "tags": [
            "python",
            "stencil",
            "diy",
            "streetart",
            "clustering",
            "art",
            "creativity"
        ]
    },
    "https://github.com/NicolasBizzozzero/CemantixSolver": {
        "extra-tags": [],
        "date": "2022-04-05",
        "title": "CemantixSolver",
        "summary": "Solver for the Cemantix webgame.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/OracLabs/orac": {
        "extra-tags": [],
        "date": "2022-03-16",
        "title": "orac",
        "summary": "\ud83e\uddab MLOps for (online) machine learning",
        "tags": [
            "python"
        ]
    },
    "https://github.com/AmenRa/ranx": {
        "extra-tags": [],
        "date": "2020-06-02",
        "title": "ranx",
        "summary": "\u26a1\ufe0fA Blazing-Fast Python Library for Ranking Evaluation, Comparison, and Fusion \ud83d\udc0d",
        "tags": [
            "evaluation",
            "recommender-systems",
            "python",
            "data-fusion",
            "ranking-metrics",
            "evaluation-metrics",
            "score-fusion",
            "information-retrieval",
            "information-retrieval-evaluation",
            "information-retrieval-metrics",
            "numba",
            "comparison",
            "metasearch",
            "rank-fusion"
        ]
    },
    "https://github.com/cgrimal/cemantix-assistant": {
        "extra-tags": [
            "assistant"
        ],
        "date": "2022-03-30",
        "title": "cemantix-assistant",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/pykeen/ilpc2022": {
        "extra-tags": [
            "prediction"
        ],
        "date": "2022-02-18",
        "title": "ilpc2022",
        "summary": "\ud83c\udfc5 KG Inductive Link Prediction Challenge (ILPC) 2022",
        "tags": [
            "python",
            "benchmarking",
            "machine-learning",
            "pykeen"
        ]
    },
    "https://github.com/nyu-mll/jiant": {
        "extra-tags": [],
        "date": "2018-06-18",
        "title": "jiant",
        "summary": "jiant is an nlp toolkit",
        "tags": [
            "sentence-representation",
            "python",
            "transformers",
            "multitask-learning",
            "nlp",
            "bert",
            "transfer-learning"
        ]
    },
    "https://github.com/kleveross/ormb": {
        "extra-tags": [],
        "date": "2020-05-21",
        "title": "ormb",
        "summary": "Docker for Your ML/DL Models Based on OCI Artifacts",
        "tags": [
            "docker",
            "go",
            "docker-registry",
            "oci-artifacts",
            "oci-registry",
            "machine-learning",
            "oci",
            "opencontainers",
            "model-versioning",
            "harbor",
            "image-registry",
            "model-management"
        ]
    },
    "https://github.com/psf/black": {
        "extra-tags": [],
        "date": "2018-03-14",
        "title": "black",
        "summary": "The uncompromising Python code formatter",
        "tags": [
            "pre-commit-hook",
            "python",
            "autopep8",
            "formatter",
            "gofmt",
            "yapf",
            "codeformatter",
            "code",
            "hacktoberfest"
        ]
    },
    "https://github.com/WilliamTd/IDAO-2022-stage1": {
        "extra-tags": [],
        "date": "2022-03-16",
        "title": "IDAO-2022-stage1",
        "summary": "Solution for the first stage of the 5th International Data Analysis Olympiad : https://idao.world/ . Team \"Variance killers\"",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/LaihoE/did-it-spill": {
        "extra-tags": [
            "training",
            "set"
        ],
        "date": "2022-02-18",
        "title": "did-it-spill",
        "summary": "Check if you have training samples in your test set",
        "tags": [
            "python",
            "time-series",
            "data-science",
            "pytorch",
            "semantic-similarity",
            "deep-learning",
            "computer-vision"
        ]
    },
    "https://github.com/alshedivat/al-folio": {
        "extra-tags": [],
        "date": "2016-05-30",
        "title": "al-folio",
        "summary": "A beautiful, simple, clean, and responsive Jekyll theme for academics",
        "tags": [
            "personal-website",
            "portfolio-website",
            "html",
            "academic-website",
            "jekyll-theme",
            "academic",
            "theme",
            "jekyll"
        ]
    },
    "https://github.com/Kaleidophon/deep-significance": {
        "extra-tags": [],
        "date": "2021-02-23",
        "title": "deep-significance",
        "summary": "Enabling easy statistical significance testing for deep neural networks. ",
        "tags": [
            "machinelearning",
            "compare-scores",
            "dl",
            "ml",
            "python",
            "machine-learning",
            "significance-testing",
            "statistical-significance",
            "statistical-significance-test",
            "hypothesis-testing",
            "deep-neural-networks",
            "deep-learning",
            "deeplearning",
            "hypothesis-tests"
        ]
    },
    "https://github.com/castorini/pyserini": {
        "extra-tags": [],
        "date": "2019-11-01",
        "title": "pyserini",
        "summary": "Pyserini is a Python toolkit for reproducible information retrieval research with sparse and dense representations.",
        "tags": [
            "information-retrieval",
            "python"
        ]
    },
    "https://github.com/capreolus-ir/capreolus": {
        "extra-tags": [
            "retrieval"
        ],
        "date": "2019-12-01",
        "title": "capreolus",
        "summary": "A toolkit for end-to-end neural ad hoc retrieval",
        "tags": [
            "python",
            "information-retrieval",
            "deep-learning"
        ]
    },
    "https://github.com/MaartenGr/BERTopic": {
        "extra-tags": [
            "c"
        ],
        "date": "2020-09-22",
        "title": "BERTopic",
        "summary": "Leveraging BERT and c-TF-IDF to create easily interpretable topics. ",
        "tags": [
            "topic-models",
            "python",
            "sentence-embeddings",
            "machine-learning",
            "transformers",
            "topic",
            "topic-modeling",
            "nlp",
            "topic-modelling",
            "ldavis",
            "bert"
        ]
    },
    "https://github.com/vivien000/st-clickable-images": {
        "extra-tags": [
            "images"
        ],
        "date": "2022-01-31",
        "title": "st-clickable-images",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/vsoch/riverapi": {
        "extra-tags": [],
        "date": "2022-02-27",
        "title": "riverapi",
        "summary": "Python client for interacting with online-ml river server (under development)",
        "tags": [
            "python"
        ]
    },
    "https://github.com/morsapaes/mz-twitch-analytics": {
        "extra-tags": [],
        "date": "2021-08-23",
        "title": "mz-twitch-analytics",
        "summary": "Self-contained demo using Kafka, Materialize and Metabase to check what's streaming on Twitch. All you need is Docker and Twitch access tokens! :space_invader:",
        "tags": [
            "postgres",
            "python",
            "sql",
            "kafka",
            "streaming"
        ]
    },
    "https://github.com/vsoch/django-river-ml": {
        "extra-tags": [],
        "date": "2022-02-24",
        "title": "django-river-ml",
        "summary": "Django plugin for online machine learning with river (under-development)",
        "tags": [
            "python"
        ]
    },
    "https://github.com/xinyadu/eeqa": {
        "extra-tags": [
            "extraction"
        ],
        "date": "2020-08-25",
        "title": "eeqa",
        "summary": "Event Extraction by Answering (Almost) Natural Questions",
        "tags": [
            "python"
        ]
    },
    "https://github.com/koaning/liBERTy": {
        "extra-tags": [],
        "date": "2022-02-19",
        "title": "liBERTy",
        "summary": "A benchmark to compare BERT against sklearn.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/online-ml/river-extra": {
        "extra-tags": [
            "river"
        ],
        "date": "2020-11-07",
        "title": "river-extra",
        "summary": "Extra functionalities for river",
        "tags": [
            "python"
        ]
    },
    "https://github.com/naver/splade": {
        "extra-tags": [],
        "date": "2021-07-14",
        "title": "splade",
        "summary": "SPLADE: sparse neural search (SIGIR21, SIGIR22)",
        "tags": [
            "sparse",
            "python",
            "information-retrieval",
            "splade",
            "nlp",
            "passage-retrieval",
            "bert"
        ]
    },
    "https://github.com/PAIR-code/lit": {
        "extra-tags": [],
        "date": "2020-07-28",
        "title": "lit",
        "summary": "The Learning Interpretability Tool: Interactively analyze ML models to understand their behavior in an extensible and framework agnostic interface.",
        "tags": [
            "natural-language-processing",
            "typescript",
            "visualization",
            "machine-learning"
        ]
    },
    "https://github.com/jessevig/bertviz": {
        "extra-tags": [],
        "date": "2018-12-16",
        "title": "bertviz",
        "summary": "BertViz: Visualize Attention in NLP Models (BERT, GPT2, BART, etc.) ",
        "tags": [
            "natural-language-processing",
            "neural-network",
            "python",
            "transformer",
            "machine-learning",
            "gpt2",
            "roberta",
            "transformers",
            "nlp",
            "pytorch",
            "bert",
            "visualization"
        ]
    },
    "https://github.com/yael-vinker/CLIPasso": {
        "extra-tags": [],
        "date": "2022-02-02",
        "title": "CLIPasso",
        "summary": "",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/BishopFox/unredacter": {
        "extra-tags": [],
        "date": "2021-10-22",
        "title": "unredacter",
        "summary": "Never ever ever use pixelation as a redaction technique",
        "tags": [
            "typescript"
        ]
    },
    "https://github.com/MIND-Lab/OCTIS": {
        "extra-tags": [],
        "date": "2020-03-13",
        "title": "OCTIS",
        "summary": "OCTIS: Comparing Topic Models is Simple! A python package to optimize and evaluate topic models (accepted at EACL2021 demo track)",
        "tags": [
            "hyperparameter-tuning",
            "natural-language-processing",
            "topic-models",
            "python",
            "nlproc",
            "evaluation-metrics",
            "nlp-library",
            "non-negative-matrix-factorization",
            "latent-semantic-analysis",
            "topic-modeling",
            "nlp",
            "hyperparameter-optimization",
            "latent-dirichlet-allocation",
            "bayesian-optimization",
            "neural-topic-models",
            "hyperparameter-search"
        ]
    },
    "https://github.com/MaxHalford/orc": {
        "extra-tags": [
            "parsing",
            "ocr"
        ],
        "date": "2022-02-04",
        "title": "orc",
        "summary": "\ud83d\udc79 Parsing structured information from OCR outputs",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/dbt-labs/dbt-core": {
        "extra-tags": [],
        "date": "2016-03-10",
        "title": "dbt-core",
        "summary": "dbt enables data analysts and engineers to transform their data using the same practices that software engineers use to build applications.",
        "tags": [
            "python",
            "dbt-viewpoint",
            "analytics",
            "slack",
            "elt",
            "pypa",
            "business-intelligence",
            "data-modeling"
        ]
    },
    "https://github.com/duckdb/duckdb": {
        "extra-tags": [],
        "date": "2018-06-26",
        "title": "duckdb",
        "summary": "DuckDB is an in-process SQL OLAP Database Management System",
        "tags": [
            "analytics",
            "sql",
            "database",
            "olap",
            "embedded-database",
            "c++"
        ]
    },
    "https://github.com/HSE-LAMBDA/IDAO-2022": {
        "extra-tags": [
            "idao"
        ],
        "date": "2022-02-01",
        "title": "IDAO-2022",
        "summary": "IDAO 2022",
        "tags": [
            "python"
        ]
    },
    "https://github.com/observablehq/plot": {
        "extra-tags": [],
        "date": "2020-10-29",
        "title": "plot",
        "summary": "A concise API for exploratory data visualization",
        "tags": [
            "charts",
            "html",
            "visualization",
            "data-visualization"
        ]
    },
    "https://github.com/burnash/gspread": {
        "extra-tags": [],
        "date": "2011-12-02",
        "title": "gspread",
        "summary": "Google Sheets Python API",
        "tags": [
            "spreadsheets",
            "python",
            "spreadsheet",
            "google-sheets-api",
            "gspread",
            "google-sheets-api-v4",
            "google-sheets"
        ]
    },
    "https://github.com/vitalik/django-ninja": {
        "extra-tags": [],
        "date": "2020-05-19",
        "title": "django-ninja",
        "summary": "\ud83d\udca8  Fast, Async-ready, Openapi, type hints based framework for building APIs",
        "tags": [
            "pydantic",
            "django-ninja",
            "python",
            "swagger-ui",
            "openapi",
            "rest-api",
            "django",
            "swagger"
        ]
    },
    "https://github.com/EleutherAI/gpt-neox": {
        "extra-tags": [],
        "date": "2020-12-22",
        "title": "gpt-neox",
        "summary": "An implementation of model parallel autoregressive transformers on GPUs, based on the DeepSpeed library.",
        "tags": [
            "python",
            "transformers",
            "deepspeed-library",
            "gpt-3",
            "language-model"
        ]
    },
    "https://github.com/coqui-ai/TTS": {
        "extra-tags": [],
        "date": "2020-05-20",
        "title": "TTS",
        "summary": "\ud83d\udc38\ud83d\udcac - a deep learning toolkit for Text-to-Speech, battle-tested in research and production",
        "tags": [
            "text-to-speech",
            "tacotron",
            "python",
            "speaker-encoder",
            "melgan",
            "hifigan",
            "tts-model",
            "voice-synthesis",
            "tts",
            "vocoder",
            "deep-learning",
            "glow-tts",
            "pytorch",
            "multi-speaker-tts",
            "speech",
            "speaker-encodings",
            "speech-synthesis",
            "voice-cloning"
        ]
    },
    "https://github.com/maxbachmann/RapidFuzz": {
        "extra-tags": [],
        "date": "2020-02-29",
        "title": "RapidFuzz",
        "summary": "Rapid fuzzy string matching in Python using various string metrics",
        "tags": [
            "python",
            "string-similarity",
            "levenshtein-distance",
            "levenshtein",
            "cpp",
            "c++",
            "string-comparison",
            "string-matching"
        ]
    },
    "https://github.com/coollabsio/coolify": {
        "extra-tags": [
            "open-source"
        ],
        "date": "2021-01-25",
        "title": "coolify",
        "summary": "An open-source & self-hostable Heroku / Netlify alternative.",
        "tags": [
            "php",
            "reactjs",
            "couchdb",
            "redis",
            "static",
            "vscode",
            "minio",
            "mongodb",
            "nodejs",
            "postgresql",
            "vuejs",
            "mysql",
            "svelte",
            "docker",
            "mysql-database",
            "databases",
            "analytics",
            "nextjs",
            "self-hosting"
        ]
    },
    "https://github.com/cgrimal/wordle-assistant": {
        "extra-tags": [],
        "date": "2022-01-17",
        "title": "wordle-assistant",
        "summary": "Tools to remove all the fun from playing Wordle",
        "tags": [
            "python"
        ]
    },
    "https://github.com/allenai/ir_datasets": {
        "extra-tags": [],
        "date": "2020-11-05",
        "title": "ir_datasets",
        "summary": "Provides a common interface to many IR ranking datasets.",
        "tags": [
            "python",
            "information-retrieval",
            "ir",
            "dataset"
        ]
    },
    "https://github.com/cvangysel/pytrec_eval": {
        "extra-tags": [],
        "date": "2017-08-14",
        "title": "pytrec_eval",
        "summary": "pytrec_eval is an Information Retrieval evaluation tool for Python, based on the popular trec_eval.",
        "tags": [
            "evaluation",
            "c++",
            "information-retrieval"
        ]
    },
    "https://github.com/beir-cellar/beir": {
        "extra-tags": [],
        "date": "2021-01-18",
        "title": "beir",
        "summary": "A Heterogeneous Benchmark for Information Retrieval. Easy to use, evaluate your models across 15+ diverse IR datasets.",
        "tags": [
            "question-generation",
            "ance",
            "nlp",
            "sentence-transformers",
            "passage-retrieval",
            "dpr",
            "bert",
            "colbert",
            "python",
            "elasticsearch",
            "information-retrieval",
            "deep-learning",
            "retrieval",
            "zero-shot-retrieval",
            "benchmark",
            "retrieval-models",
            "dataset",
            "pytorch",
            "sbert",
            "use-qa"
        ]
    },
    "https://github.com/deepset-ai/rasa-haystack": {
        "extra-tags": [
            "rasa"
        ],
        "date": "2021-09-07",
        "title": "rasa-haystack",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/UKPLab/gpl": {
        "extra-tags": [],
        "date": "2021-12-14",
        "title": "gpl",
        "summary": "Powerful unsupervised domain adaptation method for dense retrieval. Requires only unlabeled corpus and yields massive improvement: \"GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval\" https://arxiv.org/abs/2112.07577",
        "tags": [
            "python",
            "domain-adaptation",
            "vector-search",
            "information-retrieval",
            "transformers",
            "nlp",
            "bert"
        ]
    },
    "https://github.com/faster-cpython/ideas": {
        "extra-tags": [],
        "date": "2021-03-02",
        "title": "ideas",
        "summary": "",
        "tags": []
    },
    "https://github.com/PrithivirajDamodaran/Gramformer": {
        "extra-tags": [],
        "date": "2021-05-26",
        "title": "Gramformer",
        "summary": "A framework for detecting, highlighting and correcting grammatical errors on natural language text. Created by Prithiviraj Damodaran. Open to pull requests and other forms of collaboration.",
        "tags": [
            "grammar-checker",
            "python",
            "grammar",
            "grammar-correction",
            "grammar-error-correction"
        ]
    },
    "https://github.com/NicolasBizzozzero/cherche": {
        "extra-tags": [
            "search"
        ],
        "date": "2022-01-21",
        "title": "cherche",
        "summary": "Neural search",
        "tags": [
            "python"
        ]
    },
    "https://github.com/mattermost/focalboard": {
        "extra-tags": [],
        "date": "2020-10-06",
        "title": "focalboard",
        "summary": "Focalboard is an open source, self-hosted alternative to Trello, Notion, and Asana.",
        "tags": [
            "project",
            "hacktoberfest",
            "typescript",
            "project-management",
            "golang",
            "collaboration",
            "asana",
            "notion",
            "trello",
            "goal-tracking",
            "kanban-board"
        ]
    },
    "https://github.com/docarray/docarray": {
        "extra-tags": [],
        "date": "2021-12-14",
        "title": "docarray",
        "summary": "\ud83e\uddec The data structure for multimodal data \u00b7 Neural Search \u00b7 Vector Search \u00b7 Document Store",
        "tags": [
            "semantic-search",
            "nearest-neighbor-search",
            "protobuf",
            "data-structures",
            "multimodal",
            "graphql",
            "python",
            "unstructured-data",
            "docarray",
            "elasticsearch",
            "vector-search",
            "cross-modal",
            "weaviate",
            "sqlite",
            "multi-modal",
            "deep-learning",
            "dataclass",
            "nested-data",
            "qdrant",
            "neural-search"
        ]
    },
    "https://github.com/argilla-io/argilla": {
        "extra-tags": [],
        "date": "2021-04-28",
        "title": "argilla",
        "summary": "\u2728 Open-source tool for data-centric NLP. Argilla helps domain experts and data teams to build better NLP datasets in less time.",
        "tags": [
            "active-learning",
            "machine-learning",
            "developer-tools",
            "mlops",
            "nlp",
            "annotation-tool",
            "spacy",
            "dataops",
            "text-annotation",
            "artificial-intelligence",
            "python",
            "text-classification",
            "weak-supervision",
            "human-in-the-loop",
            "data-science",
            "knowledge-graph",
            "text-labeling",
            "natural-language-processing",
            "weakly-supervised-learning",
            "hacktoberfest"
        ]
    },
    "https://github.com/terrier-org/pyterrier": {
        "extra-tags": [],
        "date": "2020-04-07",
        "title": "pyterrier",
        "summary": "A Python framework for performing information retrieval experiments, building on http://terrier.org/",
        "tags": [
            "python"
        ]
    },
    "https://github.com/bojone/labse": {
        "extra-tags": [],
        "date": "2020-07-07",
        "title": "labse",
        "summary": "Language-agnostic BERT Sentence Embedding (LaBSE)",
        "tags": [
            "python"
        ]
    },
    "https://github.com/lightdash/lightdash": {
        "extra-tags": [],
        "date": "2021-03-19",
        "title": "lightdash",
        "summary": "Open source BI for teams that move fast \u26a1\ufe0f",
        "tags": [
            "data-analytics",
            "dbt",
            "business-intelligence",
            "typescript",
            "data-visualization"
        ]
    },
    "https://github.com/megadose/holehe": {
        "extra-tags": [],
        "date": "2020-06-25",
        "title": "holehe",
        "summary": "holehe allows you to check if the mail is used on different sites like twitter, instagram and will retrieve information on sites with the forgotten password function.",
        "tags": [
            "osint-python",
            "twitter",
            "python",
            "information-gathering",
            "tellonym",
            "ebay",
            "instagram",
            "open-source-intelligence",
            "osint",
            "social-network",
            "email",
            "trio",
            "pypi",
            "emails",
            "osint-tools"
        ]
    },
    "https://github.com/PaddlePaddle/RocketQA": {
        "extra-tags": [],
        "date": "2021-09-07",
        "title": "RocketQA",
        "summary": "\ud83d\ude80 RocketQA, dense retrieval for information retrieval and question answering, including both Chinese and English state-of-the-art models. ",
        "tags": [
            "python",
            "question-answering",
            "information-retrieval",
            "nlp",
            "dense-retrieval"
        ]
    },
    "https://github.com/MaxHalford/clavier": {
        "extra-tags": [],
        "date": "2022-01-05",
        "title": "clavier",
        "summary": " \ud83d\udd24 Measure edit distance based on keyboard layout",
        "tags": [
            "python"
        ]
    },
    "https://github.com/yeraydiazdiaz/lunr.py": {
        "extra-tags": [],
        "date": "2018-01-28",
        "title": "lunr.py",
        "summary": "A Python implementation of Lunr.js \ud83c\udf16",
        "tags": [
            "python"
        ]
    },
    "https://github.com/dwmkerr/hacker-laws": {
        "extra-tags": [],
        "date": "2018-01-25",
        "title": "hacker-laws",
        "summary": "\ud83d\udcbb\ud83d\udcd6 Laws, Theories, Principles and Patterns that developers will find useful. #hackerlaws",
        "tags": [
            "shell",
            "computerscience",
            "coding",
            "principles",
            "laws"
        ]
    },
    "https://github.com/delestro/outputformat": {
        "extra-tags": [
            "library"
        ],
        "date": "2021-12-20",
        "title": "outputformat",
        "summary": "Python library to decorate and beautify strings",
        "tags": [
            "python"
        ]
    },
    "https://github.com/online-ml/dam": {
        "extra-tags": [],
        "date": "2021-12-15",
        "title": "dam",
        "summary": "An experimental, streaming, stateful, minimalistic ETL built on top of River",
        "tags": [
            "python"
        ]
    },
    "https://github.com/victusfate/concierge": {
        "extra-tags": [
            "time",
            "recommendation"
        ],
        "date": "2021-11-18",
        "title": "concierge",
        "summary": "real time recommendation playground",
        "tags": [
            "python"
        ]
    },
    "https://github.com/rentruewang/koila": {
        "extra-tags": [],
        "date": "2021-11-17",
        "title": "koila",
        "summary": "Prevent PyTorch's `CUDA error: out of memory` in just 1 line of code.",
        "tags": [
            "neural-network",
            "lazy-evaluation",
            "python",
            "memory-management",
            "machine-learning",
            "gradient-accumulation",
            "pytorch",
            "deep-learning",
            "out-of-memory"
        ]
    },
    "https://github.com/online-ml/river": {
        "extra-tags": [],
        "date": "2019-01-24",
        "title": "river",
        "summary": "\ud83c\udf0a Online machine learning in Python",
        "tags": [
            "concept-drift",
            "online-learning",
            "python",
            "incremental-learning",
            "machine-learning",
            "online-statistics",
            "streaming-data",
            "data-science",
            "online-machine-learning",
            "real-time-processing",
            "streaming",
            "stream-processing"
        ]
    },
    "https://github.com/jasonreisman/Timeline": {
        "extra-tags": [],
        "date": "2015-10-19",
        "title": "Timeline",
        "summary": "A tool for creating SVG timelines from simple JSON input.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/uber/orbit": {
        "extra-tags": [],
        "date": "2020-01-07",
        "title": "orbit",
        "summary": "A Python package for Bayesian forecasting with object-oriented design and probabilistic models under the hood.",
        "tags": [
            "machine-learning",
            "regression",
            "forecast",
            "forecasting",
            "time-series",
            "stan",
            "regression-models",
            "arima",
            "pyro",
            "bayesian",
            "orbit",
            "changepoint",
            "python",
            "pystan",
            "bayesian-statistics",
            "exponential-smoothing",
            "probabilistic",
            "pytorch",
            "probabilistic-programming",
            "bayesian-methods"
        ]
    },
    "https://github.com/MaxHalford/tartine": {
        "extra-tags": [
            "spreadsheets"
        ],
        "date": "2021-11-05",
        "title": "tartine",
        "summary": "\ud83c\udf5e Manipulate dynamic spreadsheets with arbitrary layouts using Python",
        "tags": [
            "python"
        ]
    },
    "https://github.com/meilisearch/meilisearch": {
        "extra-tags": [],
        "date": "2018-04-23",
        "title": "meilisearch",
        "summary": "A lightning-fast search engine that fits effortlessly into your apps, websites, and workflow.",
        "tags": [
            "full-text-search",
            "typo-tolerance",
            "easy-to-use",
            "search-engine",
            "search-as-you-type",
            "out-of-the-box",
            "api",
            "synonyms",
            "instantsearch",
            "customizable",
            "site-search",
            "app-search",
            "faceting",
            "search",
            "geosearch",
            "rust",
            "enterprise-search",
            "database",
            "rest"
        ]
    },
    "https://github.com/VHRanger/nodevectors": {
        "extra-tags": [
            "node",
            "embeddings"
        ],
        "date": "2019-07-25",
        "title": "nodevectors",
        "summary": "Fastest network node embeddings in the west",
        "tags": [
            "python"
        ]
    },
    "https://github.com/kuwala-io/kuwala": {
        "extra-tags": [],
        "date": "2021-04-08",
        "title": "kuwala",
        "summary": "Kuwala is the no-code data platform for BI analysts and engineers enabling you to build powerful analytics workflows. We are set out to bring state-of-the-art data engineering tools you love, such as Airbyte, dbt, or Great Expectations together in one intuitive interface built with React Flow. In addition we provide third-party data into data science models and products with a focus on geospatial data.  Currently, the following data connectors are available worldwide: a) High-resolution demographics data b) Point of Interests from Open Street Map c) Google Popular Times",
        "tags": [
            "elt",
            "open-source",
            "population",
            "kuwala",
            "no-code",
            "scraping",
            "jupyter",
            "postgres",
            "python",
            "spatial-analysis",
            "dbt",
            "javascript",
            "react",
            "data-science",
            "google-trends",
            "data-integration",
            "open-data",
            "data",
            "pyspark",
            "admin-boundaries",
            "react-flow"
        ]
    },
    "https://github.com/seominjoon/denspi": {
        "extra-tags": [],
        "date": "2019-01-14",
        "title": "denspi",
        "summary": "Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index (DenSPI)",
        "tags": [
            "question-answering",
            "nlp",
            "python",
            "acl2019"
        ]
    },
    "https://github.com/salesforce/Merlion": {
        "extra-tags": [],
        "date": "2021-07-28",
        "title": "Merlion",
        "summary": "Merlion: A Machine Learning Framework for Time Series Intelligence",
        "tags": [
            "python",
            "forecasting",
            "anomaly-detection",
            "machine-learning",
            "time-series",
            "automl",
            "benchmarking",
            "ensemble-learning"
        ]
    },
    "https://github.com/princeton-nlp/DensePhrases": {
        "extra-tags": [],
        "date": "2021-01-01",
        "title": "DensePhrases",
        "summary": "ACL'2021: Learning Dense Representations of Phrases at Scale; EMNLP'2021: Phrase Retrieval Learns Passage Retrieval, Too https://arxiv.org/abs/2012.12624",
        "tags": [
            "python",
            "open-domain-qa",
            "information-retrieval",
            "knowledge-base",
            "nlp",
            "slot-filling",
            "passage-retrieval"
        ]
    },
    "https://github.com/neuml/txtai": {
        "extra-tags": [],
        "date": "2020-08-09",
        "title": "txtai",
        "summary": "\ud83d\udca1 Build AI-powered semantic search applications ",
        "tags": [
            "contextual-search",
            "semantic-search",
            "machine-learning",
            "machine-learning-pipelines",
            "api",
            "nlp",
            "document-search",
            "cloud-native",
            "python",
            "audio-search",
            "similarity-search",
            "txtai",
            "vector-search",
            "machine-learning-workflows",
            "image-search",
            "video-search",
            "deep-learning",
            "search",
            "microservice",
            "neural-search"
        ]
    },
    "https://github.com/deepset-ai/haystack": {
        "extra-tags": [],
        "date": "2019-11-14",
        "title": "haystack",
        "summary": ":mag: Haystack is an open source NLP framework to interact with your data using Transformer models and LLMs (GPT-3 and alike). Haystack offers production-ready tools to quickly build ChatGPT-like question answering, semantic search, text generation, and more.",
        "tags": [
            "semantic-search",
            "machine-learning",
            "squad",
            "transfer-learning",
            "question-answering",
            "transformers",
            "nlp",
            "bert",
            "chatgpt",
            "python",
            "elasticsearch",
            "generative-ai",
            "information-retrieval",
            "ai",
            "summarization",
            "large-language-models",
            "gpt-3",
            "language-model",
            "natural-language-processing",
            "pytorch"
        ]
    },
    "https://github.com/plasticityai/magnitude": {
        "extra-tags": [],
        "date": "2018-02-24",
        "title": "magnitude",
        "summary": "A fast, efficient universal vector embedding utility package.",
        "tags": [
            "natural-language-processing",
            "word2vec",
            "python",
            "machine-learning-library",
            "vectors",
            "embeddings",
            "machine-learning",
            "gensim",
            "fasttext",
            "glove",
            "memory-efficient",
            "fast",
            "nlp",
            "word-embeddings"
        ]
    },
    "https://github.com/salesforce/CodeT5": {
        "extra-tags": [
            "code"
        ],
        "date": "2021-08-16",
        "title": "CodeT5",
        "summary": "Code for CodeT5: a new code-aware pre-trained encoder-decoder model.",
        "tags": [
            "python",
            "representation-learning",
            "code-intelligence",
            "nlp",
            "programming-language",
            "language-model"
        ]
    },
    "https://github.com/jorisschellekens/borb": {
        "extra-tags": [],
        "date": "2020-11-07",
        "title": "borb",
        "summary": "borb is a library for reading, creating and manipulating PDF files in python.",
        "tags": [
            "python3",
            "sdk",
            "python",
            "typesetting",
            "pdf-converter",
            "pdf-library",
            "pdf-generation",
            "pdf-conversion",
            "pdf",
            "library"
        ]
    },
    "https://github.com/Yale-LILY/SummerTime": {
        "extra-tags": [],
        "date": "2021-03-18",
        "title": "SummerTime",
        "summary": "An open-source text summarization toolkit for non-experts.",
        "tags": [
            "python",
            "neural-networks",
            "nlp",
            "pytorch",
            "deep-learning",
            "text-summarization"
        ]
    },
    "https://github.com/marceloprates/prettymaps": {
        "extra-tags": [],
        "date": "2021-03-05",
        "title": "prettymaps",
        "summary": "A small set of Python functions to draw pretty maps from OpenStreetMap data. Based on osmnx, matplotlib and shapely libraries.",
        "tags": [
            "jupyter-notebook",
            "python",
            "matplotlib",
            "maps",
            "jupyter notebook",
            "generative-art",
            "cartography",
            "openstreetmap"
        ]
    },
    "https://github.com/cortexproject/cortex": {
        "extra-tags": [],
        "date": "2016-09-09",
        "title": "cortex",
        "summary": "A horizontally scalable, highly available, multi-tenant, long term Prometheus.",
        "tags": [
            "cncf",
            "go",
            "kubernetes",
            "hacktoberfest",
            "prometheus",
            "monitoring"
        ]
    },
    "https://github.com/mindee/doctr": {
        "extra-tags": [],
        "date": "2021-01-08",
        "title": "doctr",
        "summary": "docTR (Document Text Recognition) - a seamless, high-performing & accessible library for OCR-related tasks powered by Deep Learning.",
        "tags": [
            "text-recognition",
            "text-detection",
            "python",
            "text-detection-recognition",
            "optical-character-recognition",
            "ocr",
            "pytorch",
            "document-recognition",
            "deep-learning",
            "tensorflow2"
        ]
    },
    "https://github.com/AppPear/ChartView": {
        "extra-tags": [],
        "date": "2019-06-12",
        "title": "ChartView",
        "summary": "ChartView made in SwiftUI",
        "tags": [
            "ios",
            "swiftui",
            "charts",
            "chart",
            "swift"
        ]
    },
    "https://github.com/roomylee/awesome-relation-extraction": {
        "extra-tags": [],
        "date": "2018-03-24",
        "title": "awesome-relation-extraction",
        "summary": "\ud83d\udcd6 A curated list of awesome resources dedicated to Relation Extraction, one of the most important tasks in Natural Language Processing (NLP).",
        "tags": [
            "natural-language-processing",
            "relation-extraction",
            "new-york-times",
            "nips",
            "naacl",
            "trends",
            "acl",
            "relation-classification",
            "machine-learning",
            "emnlp",
            "nlp",
            "state-of-the-art",
            "deep-learning",
            "awesome",
            "distant-supervision",
            "aaai",
            "paper",
            "semeval-2010"
        ]
    },
    "https://github.com/tomasonjo/trinity-ie": {
        "extra-tags": [],
        "date": "2021-02-08",
        "title": "trinity-ie",
        "summary": "Information extraction pipeline containing coreference resolution, named entity linking, and relationship extraction",
        "tags": [
            "python",
            "named-entity-recognition",
            "relationship-extraction",
            "nlp",
            "information-extraction"
        ]
    },
    "https://github.com/huggingface/neuralcoref": {
        "extra-tags": [],
        "date": "2017-07-03",
        "title": "neuralcoref",
        "summary": "\u2728Fast Coreference Resolution in spaCy with Neural Networks",
        "tags": [
            "coreference",
            "python",
            "neural-networks",
            "machine-learning",
            "spacy-extension",
            "nlp",
            "coreference-resolution",
            "pytorch",
            "spacy-pipeline",
            "spacy",
            "c"
        ]
    },
    "https://github.com/facebookresearch/BLINK": {
        "extra-tags": [],
        "date": "2019-09-25",
        "title": "BLINK",
        "summary": "Entity Linker solution",
        "tags": [
            "python"
        ]
    },
    "https://github.com/justinlovelace/robust-kg-completion": {
        "extra-tags": [],
        "date": "2021-05-20",
        "title": "robust-kg-completion",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/TaoMiner/inferwiki": {
        "extra-tags": [],
        "date": "2021-05-27",
        "title": "inferwiki",
        "summary": "",
        "tags": []
    },
    "https://github.com/jalammar/ecco": {
        "extra-tags": [],
        "date": "2020-11-07",
        "title": "ecco",
        "summary": "Explain, analyze, and visualize NLP language models. Ecco creates interactive visualizations directly in Jupyter notebooks explaining the behavior of Transformer-based language models (like GPT2, BERT, RoBERTA, T5, and T0).",
        "tags": [
            "natural-language-processing",
            "explorables",
            "language-models",
            "nlp",
            "jupyter notebook",
            "pytorch",
            "visualization"
        ]
    },
    "https://github.com/koaning/whatlies": {
        "extra-tags": [],
        "date": "2020-02-22",
        "title": "whatlies",
        "summary": "Toolkit to help understand \"what lies\" in word embeddings. Also benchmarking! ",
        "tags": [
            "embeddings",
            "python",
            "nlp",
            "visualisations"
        ]
    },
    "https://github.com/sktime/sktime": {
        "extra-tags": [],
        "date": "2018-11-06",
        "title": "sktime",
        "summary": "A unified framework for machine learning with time series",
        "tags": [
            "data-mining",
            "python",
            "forecasting",
            "machine-learning",
            "time-series",
            "time-series-analysis",
            "time-series-classification",
            "scikit-learn",
            "data-science",
            "time-series-regression"
        ]
    },
    "https://github.com/deepmind/alphafold": {
        "extra-tags": [
            "source",
            "code",
            "alphafold2"
        ],
        "date": "2021-06-17",
        "title": "alphafold",
        "summary": "Open source code for AlphaFold.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/jolibrain/manette": {
        "extra-tags": [],
        "date": "2017-08-03",
        "title": "manette",
        "summary": "Deep Reinforcement Learning with Fined Grained Action Repetition",
        "tags": [
            "python"
        ]
    },
    "https://github.com/xinguoxia/KGE": {
        "extra-tags": [],
        "date": "2019-08-03",
        "title": "KGE",
        "summary": "Some papers on Knowledge Graph Embedding(KGE)",
        "tags": [
            "knowledge-graph-completion",
            "knowledge-graph-reasoning",
            "knowledge-graph-embedding"
        ]
    },
    "https://github.com/libgoncalv/self-tuning-networks": {
        "extra-tags": [],
        "date": "2021-06-30",
        "title": "self-tuning-networks",
        "summary": "Code for Self-Tuning Networks (ICLR 2019) https://arxiv.org/abs/1903.03088",
        "tags": [
            "python"
        ]
    },
    "https://github.com/aromain/intermarche_challenge": {
        "extra-tags": [
            "forecasting"
        ],
        "date": "2021-06-22",
        "title": "intermarche_challenge",
        "summary": "[Intemarch\u00e9] Sales forecasting challenge",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/huggingface/accelerate": {
        "extra-tags": [],
        "date": "2020-10-30",
        "title": "accelerate",
        "summary": "\ud83d\ude80 A simple way to train and use PyTorch models with multi-GPU, TPU, mixed-precision",
        "tags": [
            "python"
        ]
    },
    "https://github.com/jina-ai/jina": {
        "extra-tags": [],
        "date": "2020-02-13",
        "title": "jina",
        "summary": "\ud83d\udd2e Build multimodal AI services via cloud native technologies \u00b7 Neural Search \u00b7 Generative AI \u00b7 Cloud Native",
        "tags": [
            "kubernetes",
            "semantic-search",
            "machine-learning",
            "airflow",
            "pipeline",
            "vector-search-engine",
            "grpc",
            "mlops",
            "crossmodal",
            "multimodal",
            "microservices",
            "aiops",
            "cloud-native",
            "python",
            "generative-ai",
            "fastapi",
            "creative-ai",
            "deep-learning",
            "framework",
            "neural-search",
            "workflow"
        ]
    },
    "https://github.com/serengil/chefboost": {
        "extra-tags": [],
        "date": "2019-03-06",
        "title": "chefboost",
        "summary": "A Lightweight Decision Tree Framework supporting regular algorithms: ID3, C4,5, CART, CHAID and Regression Trees; some advanced techniques: Gradient Boosting, Random Forest and Adaboost w/categorical features support for Python",
        "tags": [
            "gradient-boosting",
            "data-mining",
            "machine-learning",
            "kaggle",
            "regression-tree",
            "gbdt",
            "id3",
            "decision-trees",
            "adaboost",
            "python",
            "gbrt",
            "cart",
            "data-science",
            "categorical-features",
            "random-forest",
            "gbm",
            "gradient-boosting-machine",
            "c45-trees",
            "gradient-boosting-machines"
        ]
    },
    "https://github.com/HazyResearch/HoroPCA": {
        "extra-tags": [
            "pca"
        ],
        "date": "2021-05-27",
        "title": "HoroPCA",
        "summary": "Hyperbolic PCA via Horospherical Projections ",
        "tags": [
            "python"
        ]
    },
    "https://github.com/supsi-dacd-isaac/mbtr": {
        "extra-tags": [
            "tree"
        ],
        "date": "2020-05-17",
        "title": "mbtr",
        "summary": "Multivariate Boosted TRee",
        "tags": [
            "python"
        ]
    },
    "https://github.com/AmineZouitine/RL_Puzzle": {
        "extra-tags": [
            "agents"
        ],
        "date": "2021-05-11",
        "title": "RL_Puzzle",
        "summary": "\ud83e\udde9 Create your own puzzle, use my agents to solve it \ud83e\udd16 try them out!  \ud83e\udde9",
        "tags": [
            "reinforcement-learning-environments",
            "reinforcement-learning",
            "mlagent",
            "mlagents",
            "unity",
            "c#"
        ]
    },
    "https://github.com/zinedine-zeitnot/anomaly-detection": {
        "extra-tags": [],
        "date": "2021-04-30",
        "title": "anomaly-detection",
        "summary": "Companion code to the anomaly detection solution exposed in the following blog post: https://bit.ly/2Rcv6Zw",
        "tags": [
            "python"
        ]
    },
    "https://github.com/studio-ousia/bpr": {
        "extra-tags": [
            "retriever"
        ],
        "date": "2021-05-10",
        "title": "bpr",
        "summary": "Binary Passage Retriever (BPR) - an efficient passage retriever for open-domain question answering",
        "tags": [
            "python"
        ]
    },
    "https://github.com/pykeen/pykeen": {
        "extra-tags": [],
        "date": "2020-02-24",
        "title": "pykeen",
        "summary": "\ud83e\udd16 A Python library for learning and evaluating knowledge graph embeddings ",
        "tags": [
            "knowledge-graph-embeddings",
            "knowledge-graphs",
            "link-prediction",
            "python",
            "cuda",
            "knowledge-base-completion",
            "machine-learning",
            "torch",
            "deep-learning",
            "pykeen"
        ]
    },
    "https://github.com/pngwn/MDsveX": {
        "extra-tags": [
            "markdown",
            "svelte"
        ],
        "date": "2019-01-09",
        "title": "MDsveX",
        "summary": "A markdown preprocessor for Svelte.",
        "tags": [
            "javascript"
        ]
    },
    "https://github.com/BOUALILILila/ExactMatchMarking": {
        "extra-tags": [
            "benchmarking"
        ],
        "date": "2020-11-09",
        "title": "ExactMatchMarking",
        "summary": "",
        "tags": [
            "python",
            "bert",
            "adhoc-retrieval",
            "information-retrieval"
        ]
    },
    "https://github.com/hku-mars/ikd-Tree": {
        "extra-tags": [
            "repository",
            "tree"
        ],
        "date": "2021-02-22",
        "title": "ikd-Tree",
        "summary": "This repository provides implementation of an incremental k-d tree for robotic applications.",
        "tags": [
            "lidar-mapping",
            "c++",
            "kd-tree",
            "lidar-point-cloud"
        ]
    },
    "https://github.com/garrettj403/SciencePlots": {
        "extra-tags": [
            "matplotlib",
            "plotting"
        ],
        "date": "2018-08-13",
        "title": "SciencePlots",
        "summary": "Matplotlib styles for scientific plotting",
        "tags": [
            "latex",
            "matplotlib-styles",
            "python",
            "matplotlib-style-sheets",
            "ieee-paper",
            "scientific-papers",
            "thesis-template",
            "matplotlib-figures",
            "cjk-fonts"
        ]
    },
    "https://github.com/AmineZouitine/RL_Shooter": {
        "extra-tags": [],
        "date": "2021-04-04",
        "title": "RL_Shooter",
        "summary": "\ud83e\udd16 Creation of an RL environment with Unity, where an agent must learn to survive by moving \ud83e\uddbf and shooting\ud83d\udd2b, using ML-Agents ! ",
        "tags": [
            "agent",
            "mlagent",
            "reinforcement-learning",
            "reinforcement-learning-environments",
            "mlagents",
            "unity3d",
            "deep-reinforcement-learning",
            "ai",
            "unity",
            "c#",
            "shooter",
            "unityml",
            "ml",
            "ml-agents"
        ]
    },
    "https://github.com/bes-dev/random_face": {
        "extra-tags": [],
        "date": "2021-04-09",
        "title": "random_face",
        "summary": "A simple python library for fast image generation of people who do not exist.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/dfdazac/blp": {
        "extra-tags": [],
        "date": "2020-03-31",
        "title": "blp",
        "summary": "\"Inductive Entity Representations from Text via Link Prediction\" @ The Web Conference 2021",
        "tags": [
            "python"
        ]
    },
    "https://github.com/spotify/annoy": {
        "extra-tags": [],
        "date": "2013-04-01",
        "title": "annoy",
        "summary": "Approximate Nearest Neighbors in C++/Python optimized for memory usage and loading/saving to disk",
        "tags": [
            "locality-sensitive-hashing",
            "lua",
            "nearest-neighbor-search",
            "python",
            "approximate-nearest-neighbor-search",
            "golang",
            "c-plus-plus",
            "c++"
        ]
    },
    "https://github.com/patrick-kidger/torchtyping": {
        "extra-tags": [],
        "date": "2021-03-28",
        "title": "torchtyping",
        "summary": "Type annotations and dynamic checking for a tensor's shape, dtype, names, etc.",
        "tags": [
            "tensors",
            "python-typing",
            "python",
            "typing",
            "pytorch",
            "shape",
            "named-tensors"
        ]
    },
    "https://github.com/WilliamTd/IDAO-2021-stage1": {
        "extra-tags": [
            "idao"
        ],
        "date": "2021-04-05",
        "title": "IDAO-2021-stage1",
        "summary": "",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/tlentali/leab": {
        "extra-tags": [],
        "date": "2020-03-27",
        "title": "leab",
        "summary": "\ud83d\udcc8\ud83d\udd0d Lets Python do AB testing analysis",
        "tags": [
            "python",
            "analytics",
            "data-analysis",
            "analysis",
            "ab-testing",
            "statistics",
            "data-science",
            "abtest"
        ]
    },
    "https://github.com/hound-search/hound": {
        "extra-tags": [],
        "date": "2015-01-07",
        "title": "hound",
        "summary": "Lightning fast code searching made easy",
        "tags": [
            "hacktoberfest",
            "javascript"
        ]
    },
    "https://github.com/corollari/linusrants": {
        "extra-tags": [],
        "date": "2018-03-27",
        "title": "linusrants",
        "summary": "Dataset of Linus Torvalds' rants classified by negativity using sentiment analysis",
        "tags": [
            "linus-torvalds",
            "python",
            "linus",
            "linus-rants",
            "dataset",
            "sentiment-analysis"
        ]
    },
    "https://github.com/speechbrain/speechbrain": {
        "extra-tags": [
            "speech"
        ],
        "date": "2020-04-28",
        "title": "speechbrain",
        "summary": "A PyTorch-based Speech Toolkit",
        "tags": [
            "speech-separation",
            "speech-processing",
            "spoken-language-understanding",
            "speech-toolkit",
            "transformers",
            "voice-recognition",
            "audio-processing",
            "speech-to-text",
            "python",
            "huggingface",
            "speaker-verification",
            "deep-learning",
            "speechrecognition",
            "language-model",
            "audio",
            "speech-recognition",
            "speaker-recognition",
            "speaker-diarization",
            "pytorch",
            "speech-enhancement",
            "asr"
        ]
    },
    "https://github.com/EthanRosenthal/nannernest": {
        "extra-tags": [],
        "date": "2020-05-22",
        "title": "nannernest",
        "summary": "Optimal peanut butter and banana sandwiches",
        "tags": [
            "python",
            "sandwiches",
            "machine-learning",
            "deep-learning",
            "computer-vision"
        ]
    },
    "https://github.com/MaxHalford/bbc-weather-honolulu": {
        "extra-tags": [],
        "date": "2021-03-06",
        "title": "bbc-weather-honolulu",
        "summary": " \u2600\ufe0f Measuring the accuracy of BBC weather forecasts in Honolulu, USA",
        "tags": [
            "python"
        ]
    },
    "https://github.com/helboukkouri/character-bert": {
        "extra-tags": [
            "repository",
            "bert"
        ],
        "date": "2020-10-20",
        "title": "character-bert",
        "summary": "Main repository for \"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters\"",
        "tags": [
            "python"
        ]
    },
    "https://github.com/gto76/python-cheatsheet": {
        "extra-tags": [],
        "date": "2018-01-25",
        "title": "python-cheatsheet",
        "summary": "Comprehensive Python Cheatsheet",
        "tags": [
            "python-cheatsheet",
            "reference",
            "python",
            "cheatsheet"
        ]
    },
    "https://github.com/jrzaurin/pytorch-widedeep": {
        "extra-tags": [],
        "date": "2017-10-21",
        "title": "pytorch-widedeep",
        "summary": "A flexible package for multimodal-deep-learning to combine tabular data with text and images using Wide and Deep models in Pytorch",
        "tags": [
            "tabular-data",
            "images",
            "python",
            "pytorch-cv",
            "pytorch-transformers",
            "multimodal-deep-learning",
            "pytorch-nlp",
            "model-hub",
            "pytorch-tabular-data",
            "pytorch",
            "deep-learning",
            "text"
        ]
    },
    "https://github.com/gpleiss/temperature_scaling": {
        "extra-tags": [
            "simple"
        ],
        "date": "2017-08-03",
        "title": "temperature_scaling",
        "summary": "A simple way to calibrate your neural network.",
        "tags": [
            "calibration",
            "python",
            "deep-learning"
        ]
    },
    "https://github.com/ebhy/budgetml": {
        "extra-tags": [],
        "date": "2020-12-27",
        "title": "budgetml",
        "summary": "Deploy a ML inference service on a budget in less than 10 lines of code.",
        "tags": [
            "python",
            "machine-learning",
            "inference",
            "deployment",
            "fastapi",
            "mlops",
            "data-science",
            "api"
        ]
    },
    "https://github.com/microsoft/FLAML": {
        "extra-tags": [],
        "date": "2020-08-20",
        "title": "FLAML",
        "summary": "A fast library for AutoML and tuning. Join our Discord: https://discord.gg/Cppx2vSPVP.",
        "tags": [
            "natural-language-generation",
            "machine-learning",
            "regression",
            "jupyter notebook",
            "tabular-data",
            "tuning",
            "classification",
            "timeseries-forecasting",
            "automl",
            "python",
            "scikit-learn",
            "hyperparam",
            "data-science",
            "deep-learning",
            "hyperparameter-optimization",
            "automated-machine-learning",
            "natural-language-processing",
            "random-forest",
            "jupyter-notebook",
            "finetuning"
        ]
    },
    "https://github.com/paulmelki/paulmelki.github.io": {
        "extra-tags": [
            "github",
            "blog"
        ],
        "date": "2021-02-20",
        "title": "paulmelki.github.io",
        "summary": "My personal blog.",
        "tags": [
            "html"
        ]
    },
    "https://github.com/beurtschipper/Depix": {
        "extra-tags": [],
        "date": "2020-12-06",
        "title": "Depix",
        "summary": "Recovers passwords from pixelized screenshots",
        "tags": [
            "python"
        ]
    },
    "https://github.com/Textualize/rich": {
        "extra-tags": [],
        "date": "2019-11-10",
        "title": "rich",
        "summary": "Rich is a Python library for rich text and beautiful formatting in the terminal.",
        "tags": [
            "python3",
            "python",
            "traceback",
            "terminal",
            "emoji",
            "python-library",
            "tables",
            "markdown",
            "ansi-colors",
            "syntax-highlighting",
            "tui",
            "rich",
            "progress-bar",
            "terminal-color",
            "tracebacks-rich",
            "progress-bar-python"
        ]
    },
    "https://github.com/thunlp/ERNIE": {
        "extra-tags": [],
        "date": "2019-05-17",
        "title": "ERNIE",
        "summary": "Source code and dataset for ACL 2019 paper \"ERNIE: Enhanced Language Representation with Informative Entities\"",
        "tags": [
            "python"
        ]
    },
    "https://github.com/RasaHQ/rasa": {
        "extra-tags": [],
        "date": "2016-10-14",
        "title": "rasa",
        "summary": "\ud83d\udcac   Open source machine learning framework to automate text- and voice-based conversations: NLU, dialogue management, connect to Slack, Facebook, and more - Create chatbots and voice assistants",
        "tags": [
            "bot",
            "machine-learning",
            "nlp",
            "wit",
            "conversational-agents",
            "spacy",
            "python",
            "machine-learning-library",
            "chatbots",
            "chatbots-framework",
            "nlu",
            "bots",
            "rasa",
            "natural-language-processing",
            "conversational-ai",
            "conversational-bots",
            "botkit",
            "bot-framework",
            "chatbot",
            "conversation-driven-development",
            "mitie"
        ]
    },
    "https://github.com/RasaHQ/NLU-training-data": {
        "extra-tags": [],
        "date": "2019-10-30",
        "title": "NLU-training-data",
        "summary": "Crowd sourced training data for Rasa NLU models",
        "tags": [
            "python"
        ]
    },
    "https://github.com/hannansatopay/roughviz": {
        "extra-tags": [],
        "date": "2019-11-06",
        "title": "roughviz",
        "summary": "A Python visualization library for creating sketchy/hand-drawn styled charts.",
        "tags": [
            "jupyter-notebook",
            "python",
            "charts",
            "hacktoberfest",
            "data-science",
            "python-visualization",
            "roughviz",
            "vizualisation"
        ]
    },
    "https://github.com/neulab/InterpretEval": {
        "extra-tags": [
            "evaluation",
            "nlp"
        ],
        "date": "2020-04-21",
        "title": "InterpretEval",
        "summary": "Interpretable Evaluation for (Almost) All NLP Tasks",
        "tags": [
            "html"
        ]
    },
    "https://github.com/Certificat-sciences-des-donnees-bigdata/Module-immersion": {
        "extra-tags": [],
        "date": "2018-05-25",
        "title": "Module-immersion",
        "summary": "Retrouvez ici les contenus \u00e0 \u00e9tudier en autonomie du module immersion",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/MaxHalford/naked": {
        "extra-tags": [
            "machine",
            "learning"
        ],
        "date": "2021-02-05",
        "title": "naked",
        "summary": "The simplest way to deploy a machine learning model",
        "tags": [
            "python"
        ]
    },
    "https://github.com/louisabraham/fastnode2vec": {
        "extra-tags": [
            "fast"
        ],
        "date": "2020-04-20",
        "title": "fastnode2vec",
        "summary": "Fast and scalable node2vec implementation",
        "tags": [
            "python"
        ]
    },
    "https://github.com/huggingface/blog": {
        "extra-tags": [
            "blog"
        ],
        "date": "2020-02-14",
        "title": "blog",
        "summary": "Public repo for HF blog posts",
        "tags": [
            "hacktoberfest",
            "jupyter notebook"
        ]
    },
    "https://github.com/arogozhnikov/einops": {
        "extra-tags": [],
        "date": "2018-09-22",
        "title": "einops",
        "summary": "Deep learning operations reinvented (for pytorch, tensorflow, jax and others)",
        "tags": [
            "cupy",
            "tensor",
            "gluon",
            "python",
            "jax",
            "keras",
            "numpy",
            "chainer",
            "tensorflow",
            "pytorch",
            "deep-learning"
        ]
    },
    "https://github.com/piccolomo/plotext": {
        "extra-tags": [
            "plotting",
            "terminal"
        ],
        "date": "2019-10-20",
        "title": "plotext",
        "summary": "plotting on terminal",
        "tags": [
            "python"
        ]
    },
    "https://github.com/sonos/nlu-benchmark": {
        "extra-tags": [
            "nlu",
            "benchmark"
        ],
        "date": "2016-12-23",
        "title": "nlu-benchmark",
        "summary": "",
        "tags": []
    },
    "https://github.com/snipsco/snips-nlu": {
        "extra-tags": [],
        "date": "2017-02-08",
        "title": "snips-nlu",
        "summary": "Snips Python library to extract meaning from text",
        "tags": [
            "intent-classification",
            "python",
            "machine-learning-library",
            "named-entity-recognition",
            "intent-parser",
            "text-classification",
            "bot",
            "machine-learning",
            "ner",
            "snips",
            "chatbot",
            "nlp",
            "slot-filling",
            "nlu",
            "ml",
            "information-extraction"
        ]
    },
    "https://github.com/camelot-dev/camelot": {
        "extra-tags": [],
        "date": "2019-07-01",
        "title": "camelot",
        "summary": "A Python library to extract tabular data from PDFs",
        "tags": [
            "python"
        ]
    },
    "https://github.com/facebookresearch/GENRE": {
        "extra-tags": [
            "autoregressive",
            "retrieval"
        ],
        "date": "2020-10-05",
        "title": "GENRE",
        "summary": "Autoregressive Entity Retrieval",
        "tags": [
            "python"
        ]
    },
    "https://github.com/lucidrains/DALLE-pytorch": {
        "extra-tags": [],
        "date": "2021-01-05",
        "title": "DALLE-pytorch",
        "summary": "Implementation / replication of DALL-E, OpenAI's Text to Image Transformer, in Pytorch",
        "tags": [
            "python",
            "text-to-image",
            "attention-mechanism",
            "transformers",
            "multi-modal",
            "deep-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/explosion/spaCy": {
        "extra-tags": [],
        "date": "2014-07-03",
        "title": "spaCy",
        "summary": "\ud83d\udcab Industrial-strength Natural Language Processing (NLP) in Python",
        "tags": [
            "natural-language-processing",
            "neural-network",
            "python",
            "named-entity-recognition",
            "neural-networks",
            "text-classification",
            "machine-learning",
            "nlp-library",
            "ai",
            "nlp",
            "cython",
            "data-science",
            "entity-linking",
            "deep-learning",
            "tokenization",
            "spacy",
            "artificial-intelligence"
        ]
    },
    "https://github.com/SapienzaNLP/ewiser": {
        "extra-tags": [
            "system",
            "knowledge"
        ],
        "date": "2020-03-25",
        "title": "ewiser",
        "summary": "A Word Sense Disambiguation system integrating implicit and explicit external knowledge.",
        "tags": [
            "natural-language-processing",
            "python",
            "wsd",
            "nlp",
            "word-sense-disambiguation"
        ]
    },
    "https://github.com/allenai/kb": {
        "extra-tags": [
            "knowledge"
        ],
        "date": "2019-09-03",
        "title": "kb",
        "summary": "KnowBert -- Knowledge Enhanced Contextual Word Representations",
        "tags": [
            "python"
        ]
    },
    "https://github.com/Cylia-ABBAD/Factorization_Machines_Report": {
        "extra-tags": [],
        "date": "2021-01-04",
        "title": "Factorization_Machines_Report",
        "summary": "",
        "tags": []
    },
    "https://github.com/IliesHachemi/Text-Generation": {
        "extra-tags": [
            "text-generation"
        ],
        "date": "2021-01-05",
        "title": "Text-Generation",
        "summary": "",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/DLProjectTextGeneration/TextGeneration": {
        "extra-tags": [],
        "date": "2020-12-30",
        "title": "TextGeneration",
        "summary": "",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/loicbausor/face-recognition-and-embedding": {
        "extra-tags": [
            "face-recognition",
            "embedding"
        ],
        "date": "2021-01-05",
        "title": "face-recognition-and-embedding",
        "summary": "",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/younesszaim/Text-Generation-M2-ECO-STAT-TSE": {
        "extra-tags": [],
        "date": "2021-01-05",
        "title": "Text-Generation-M2-ECO-STAT-TSE",
        "summary": "Projet for Deep Learning 2 course about Generating cooking recipes",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/ManuteaTarati/Deep-Learning-Project": {
        "extra-tags": [
            "deep-learning",
            "project"
        ],
        "date": "2021-01-05",
        "title": "Deep-Learning-Project",
        "summary": "",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/sunnywang93/Deep-Learning-Project": {
        "extra-tags": [
            "deep-learning",
            "project"
        ],
        "date": "2021-01-03",
        "title": "Deep-Learning-Project",
        "summary": "",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/paulmelki/BERT_BM25_InformationRetrieval": {
        "extra-tags": [],
        "date": "2020-11-17",
        "title": "BERT_BM25_InformationRetrieval",
        "summary": "This project aims at creating a search engine based on BERT language model.",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/SoleneMarinier/one-shot-learning": {
        "extra-tags": [
            "learning"
        ],
        "date": "2020-12-29",
        "title": "one-shot-learning",
        "summary": "",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/Rico-le-champion/Deep-Learning-Time-series": {
        "extra-tags": [
            "deep-learning",
            "time-series"
        ],
        "date": "2021-01-04",
        "title": "Deep-Learning-Time-series",
        "summary": "",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/BaptisteH3089/ImbLearning": {
        "extra-tags": [],
        "date": "2021-01-02",
        "title": "ImbLearning",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/othmanefc/marco-polo": {
        "extra-tags": [],
        "date": "2020-11-25",
        "title": "marco-polo",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/PaulBarriere/TSE-NBSVM": {
        "extra-tags": [],
        "date": "2020-12-16",
        "title": "TSE-NBSVM",
        "summary": "The objective is to implement the NBSVM method. ",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/nershman/imbalanced-learning": {
        "extra-tags": [],
        "date": "2020-12-30",
        "title": "imbalanced-learning",
        "summary": "Comparison of different imbalanced classification methods with application to credit card fraud",
        "tags": [
            "imbalanced-learning",
            "anomaly-detection",
            "jupyter notebook"
        ]
    },
    "https://github.com/dorianbrown/rank_bm25": {
        "extra-tags": [],
        "date": "2019-01-20",
        "title": "rank_bm25",
        "summary": "A Collection of BM25 Algorithms in Python",
        "tags": [
            "python",
            "bm25",
            "algorithm",
            "information-retrieval",
            "ranking"
        ]
    },
    "https://github.com/deeppavlov/DeepPavlov": {
        "extra-tags": [],
        "date": "2017-11-17",
        "title": "DeepPavlov",
        "summary": "An open source library for deep learning end-to-end dialog systems and chatbots.",
        "tags": [
            "entity-extraction",
            "intent-classification",
            "named-entity-recognition",
            "bot",
            "dialogue-systems",
            "machine-learning",
            "question-answering",
            "nlp-machine-learning",
            "tensorflow",
            "nlp",
            "artificial-intelligence",
            "intent-detection",
            "python",
            "ai",
            "deep-learning",
            "dialogue-manager",
            "dialogue-agents",
            "chatbot",
            "slot-filling",
            "deep-neural-networks",
            "chitchat"
        ]
    },
    "https://github.com/dperezrada/keywords2vec": {
        "extra-tags": [],
        "date": "2018-11-14",
        "title": "keywords2vec",
        "summary": "",
        "tags": [
            "multi-language",
            "text-mining",
            "nlp",
            "keywords-extraction",
            "jupyter notebook",
            "phrase-extraction"
        ]
    },
    "https://github.com/microsoft/nlp-recipes": {
        "extra-tags": [],
        "date": "2019-04-05",
        "title": "nlp-recipes",
        "summary": "Natural Language Processing Best Practices & Examples",
        "tags": [
            "mlflow",
            "natural-language-processing",
            "python",
            "text-classification",
            "pretrained-models",
            "sota",
            "machine-learning",
            "azure-ml",
            "natural-language-inference",
            "transfomer",
            "nlp",
            "best-practices",
            "natural-language-understanding",
            "nli",
            "deep-learning",
            "nlu",
            "natural-language",
            "text"
        ]
    },
    "https://github.com/MaxHalford/idao-2020-qualifier": {
        "extra-tags": [],
        "date": "2020-01-03",
        "title": "idao-2020-qualifier",
        "summary": "Solution of team \"Data O Plomo\" to the qualification phase of the 2020 edition of the International Data Analysis Olympiad (IDAO) ",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/MaxHalford/idao-2020-final": {
        "extra-tags": [],
        "date": "2020-11-18",
        "title": "idao-2020-final",
        "summary": "Solution of team \"Data O Plomo\" to the final phase of the 2020 edition of the International Data Analysis Olympiad (IDAO) ",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/deepmind/jraph": {
        "extra-tags": [],
        "date": "2020-11-23",
        "title": "jraph",
        "summary": "A Graph Neural Network Library in Jax",
        "tags": [
            "python",
            "jax",
            "machine-learning",
            "graph-neural-networks",
            "deep-learning"
        ]
    },
    "https://github.com/MaxHalford/sorobn": {
        "extra-tags": [
            "bayesian"
        ],
        "date": "2020-05-05",
        "title": "sorobn",
        "summary": "\ud83e\uddee Bayesian networks in Python",
        "tags": [
            "bayesian-network",
            "python"
        ]
    },
    "https://github.com/lvdmaaten/bhtsne": {
        "extra-tags": [],
        "date": "2015-02-11",
        "title": "bhtsne",
        "summary": "Barnes-Hut t-SNE",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/w4k2/stream-learn": {
        "extra-tags": [],
        "date": "2018-02-07",
        "title": "stream-learn",
        "summary": "The stream-learn is an open-source Python library for difficult data stream analysis.",
        "tags": [
            "software",
            "python",
            "data-streams",
            "machine-learning"
        ]
    },
    "https://github.com/ragulpr/wtte-rnn": {
        "extra-tags": [],
        "date": "2017-01-26",
        "title": "wtte-rnn",
        "summary": "WTTE-RNN a framework for churn and time to event prediction",
        "tags": [
            "churn-prediction",
            "failure-rate",
            "neural-network",
            "python",
            "keras",
            "tensorflow",
            "rnn",
            "machine-learning-algorithms",
            "weibull"
        ]
    },
    "https://github.com/smastelini/lsh-knn": {
        "extra-tags": [
            "pytorch-knn"
        ],
        "date": "2020-11-13",
        "title": "lsh-knn",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/junegunn/fzf": {
        "extra-tags": [
            "command-line"
        ],
        "date": "2013-10-23",
        "title": "fzf",
        "summary": ":cherry_blossom: A command-line fuzzy finder",
        "tags": [
            "neovim",
            "vim",
            "go",
            "tmux",
            "zsh",
            "cli",
            "unix",
            "fish",
            "bash",
            "fzf"
        ]
    },
    "https://github.com/huggingface/datasets": {
        "extra-tags": [],
        "date": "2020-03-26",
        "title": "datasets",
        "summary": "\ud83e\udd17 The largest hub of ready-to-use datasets for ML models with fast, easy-to-use and efficient data manipulation tools",
        "tags": [
            "evaluation",
            "natural-language-processing",
            "datasets",
            "python",
            "metrics",
            "pandas",
            "machine-learning",
            "numpy",
            "hacktoberfest",
            "tensorflow",
            "nlp",
            "pytorch",
            "deep-learning",
            "speech",
            "computer-vision"
        ]
    },
    "https://github.com/patil-suraj/question_generation": {
        "extra-tags": [
            "transformers"
        ],
        "date": "2020-07-03",
        "title": "question_generation",
        "summary": "Neural question generation using transformers",
        "tags": [
            "natural-language-processing",
            "transformer",
            "t5",
            "natural-language-generation",
            "nlp",
            "question-generation",
            "jupyter notebook",
            "deep-learning",
            "nlg"
        ]
    },
    "https://github.com/MBrouns/timeseers": {
        "extra-tags": [
            "time"
        ],
        "date": "2020-01-23",
        "title": "timeseers",
        "summary": "Time should be taken seer-iously",
        "tags": [
            "python"
        ]
    },
    "https://github.com/NVIDIA/NeMo": {
        "extra-tags": [
            "ai"
        ],
        "date": "2019-08-05",
        "title": "NeMo",
        "summary": "NeMo: a toolkit for conversational AI",
        "tags": [
            "text-to-speech",
            "speech-to-text",
            "neural-network",
            "python",
            "text-normalization",
            "speech-recognition",
            "speaker-recognition",
            "nlp-machine-learning",
            "machine-translation",
            "nmt",
            "nlp",
            "speaker-diarization",
            "tts",
            "deep-learning",
            "asr",
            "speech-synthesis",
            "language-model"
        ]
    },
    "https://github.com/facebookresearch/fairseq": {
        "extra-tags": [],
        "date": "2017-08-29",
        "title": "fairseq",
        "summary": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python.",
        "tags": [
            "python",
            "pytorch",
            "artificial-intelligence"
        ]
    },
    "https://github.com/facebookresearch/ParlAI": {
        "extra-tags": [],
        "date": "2017-04-24",
        "title": "ParlAI",
        "summary": "A framework for training and evaluating AI models on a variety of openly available dialogue datasets.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/fpservant/semanlink": {
        "extra-tags": [],
        "date": "2013-10-06",
        "title": "semanlink",
        "summary": "Semanlink is a personal information management system based on RDF. It lets you add tags, as well as other RDF metadata, to files, bookmarks and text notes. Providing a simple way to organize the tags in\ta graph, it allows you to incrementally define the vocabulary you use when annotating documents with metadata.",
        "tags": [
            "java"
        ]
    },
    "https://github.com/DeepGraphLearning/graphvite": {
        "extra-tags": [],
        "date": "2019-07-16",
        "title": "graphvite",
        "summary": "GraphVite: A General and High-performance Graph Embedding System ",
        "tags": [
            "network-embedding",
            "gpu",
            "cuda",
            "machine-learning",
            "representation-learning",
            "data-visualization",
            "knowledge-graph",
            "c++"
        ]
    },
    "https://github.com/awslabs/dgl-ke": {
        "extra-tags": [],
        "date": "2020-03-03",
        "title": "dgl-ke",
        "summary": "High performance, easy-to-use, and scalable package for learning large-scale knowledge graph embeddings.",
        "tags": [
            "python",
            "knowledge-graphs-embeddings",
            "dgl",
            "machine-learning",
            "graph-learning",
            "knowledge-graph"
        ]
    },
    "https://github.com/torchkge-team/torchkge": {
        "extra-tags": [],
        "date": "2019-03-29",
        "title": "torchkge",
        "summary": "TorchKGE: Knowledge Graph embedding in Python and PyTorch.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/Certificat-sciences-des-donnees-bigdata/Module-sensibilisation": {
        "extra-tags": [],
        "date": "2018-05-25",
        "title": "Module-sensibilisation",
        "summary": "Retrouvez ici les contenus \u00e0 \u00e9tudier en autonomie du module sensibilisation.",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/MaxHalford/chime": {
        "extra-tags": [],
        "date": "2020-10-04",
        "title": "chime",
        "summary": "\ud83c\udfb5 Python sound notifications made easy",
        "tags": [
            "audio",
            "ipython",
            "cross-platform",
            "notifications",
            "python",
            "command-line",
            "streamlit",
            "sound",
            "wav"
        ]
    },
    "https://github.com/lancifollia/tinygbt": {
        "extra-tags": [
            "gradient"
        ],
        "date": "2018-07-14",
        "title": "tinygbt",
        "summary": "A Tiny, Pure Python implementation of Gradient Boosted Trees.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/samuelbroscheit/entity_knowledge_in_bert": {
        "extra-tags": [],
        "date": "2020-05-25",
        "title": "entity_knowledge_in_bert",
        "summary": "This repository contains the code for the CONLL 2019 paper \"Investigating Entity Knowledge in BERT with Simple Neural End-To-End Entity Linking\". The code is provided as a documentation for the paper and also for follow-up research.",
        "tags": [
            "nlp",
            "python",
            "machine-learning",
            "entity-linking"
        ]
    },
    "https://github.com/pfliu-nlp/Named-Entity-Recognition-NER-Papers": {
        "extra-tags": [],
        "date": "2020-01-06",
        "title": "Named-Entity-Recognition-NER-Papers",
        "summary": "An elaborate and exhaustive paper list for Named Entity Recognition (NER)",
        "tags": [
            "ner-datasets",
            "paper-list",
            "named-entity-recognition",
            "ner"
        ]
    },
    "https://github.com/microsoft/playwright-python": {
        "extra-tags": [],
        "date": "2020-07-01",
        "title": "playwright-python",
        "summary": "Python version of the Playwright testing and automation library.",
        "tags": [
            "python",
            "webkit",
            "chromium",
            "playwright",
            "firefox"
        ]
    },
    "https://github.com/MehdiZouitine/gym_ma_toy": {
        "extra-tags": [],
        "date": "2020-09-25",
        "title": "gym_ma_toy",
        "summary": ":video_game: Toy environment set for multi-agent reinforcement learning and more",
        "tags": [
            "python"
        ]
    },
    "https://github.com/scikit-multiflow/scikit-multiflow": {
        "extra-tags": [],
        "date": "2017-11-14",
        "title": "scikit-multiflow",
        "summary": "A machine learning package for streaming data in Python. The other ancestor of River.",
        "tags": [
            "scikit",
            "moa",
            "python",
            "streaming-data",
            "machine-learning",
            "scikit-learn",
            "meka",
            "stream"
        ]
    },
    "https://github.com/scikit-multilearn/scikit-multilearn": {
        "extra-tags": [],
        "date": "2014-04-30",
        "title": "scikit-multilearn",
        "summary": "A scikit-learn based module for multi-label et. al. classification",
        "tags": [
            "scikit",
            "python",
            "label-prediction",
            "machine-learning",
            "classification",
            "scikit-learn",
            "clustering",
            "scikit-multilearn",
            "multi-label",
            "partitioning"
        ]
    },
    "https://github.com/MaxHalford/spotgeo-challenge": {
        "extra-tags": [],
        "date": "2020-06-08",
        "title": "spotgeo-challenge",
        "summary": "\ud83d\udef0\ufe0f My solution to the Kelvins spotGEO challenge ",
        "tags": [
            "spotgeo",
            "python",
            "competitive-data-science"
        ]
    },
    "https://github.com/MaxHalford/ziboinboin.com": {
        "extra-tags": [],
        "date": "2017-05-24",
        "title": "ziboinboin.com",
        "summary": "\ud83c\udf42 Old Ziboinboin website",
        "tags": [
            "html"
        ]
    },
    "https://github.com/blue-yonder/tsfresh": {
        "extra-tags": [],
        "date": "2016-10-26",
        "title": "tsfresh",
        "summary": "Automatic extraction of relevant features from time series:",
        "tags": [
            "feature-extraction",
            "jupyter notebook",
            "data-science",
            "time-series"
        ]
    },
    "https://github.com/WojciechMula/pyahocorasick": {
        "extra-tags": [],
        "date": "2013-05-30",
        "title": "pyahocorasick",
        "summary": "Python module (C extension and plain python) implementing Aho-Corasick algorithm",
        "tags": [
            "aho-corasick",
            "automaton",
            "string-manipulation",
            "trie",
            "c"
        ]
    },
    "https://github.com/huggingface/awesome-papers": {
        "extra-tags": [],
        "date": "2020-03-11",
        "title": "awesome-papers",
        "summary": "Papers & presentation materials from Hugging Face's internal science day",
        "tags": []
    },
    "https://github.com/pyg-team/pytorch_geometric": {
        "extra-tags": [],
        "date": "2017-10-06",
        "title": "pytorch_geometric",
        "summary": "Graph Neural Network Library for PyTorch",
        "tags": [
            "graph-convolutional-networks",
            "python",
            "geometric-deep-learning",
            "graph-neural-networks",
            "pytorch",
            "deep-learning"
        ]
    },
    "https://github.com/rusty1s/pytorch_cluster": {
        "extra-tags": [],
        "date": "2018-01-12",
        "title": "pytorch_cluster",
        "summary": "PyTorch Extension Library of Optimized Graph Cluster Algorithms",
        "tags": [
            "cluster-algorithms",
            "geometric-deep-learning",
            "graph-neural-networks",
            "pytorch",
            "c++"
        ]
    },
    "https://github.com/tensorflow/neural-structured-learning": {
        "extra-tags": [],
        "date": "2019-08-27",
        "title": "neural-structured-learning",
        "summary": "Training neural models with structured signals.",
        "tags": [
            "python",
            "adversarial-learning",
            "neural-networks",
            "keras",
            "tensorflow",
            "structured-signals",
            "graph-learning",
            "regularization"
        ]
    },
    "https://github.com/MaxHalford/pytorch-resample": {
        "extra-tags": [],
        "date": "2020-08-11",
        "title": "pytorch-resample",
        "summary": "\ud83c\udfb2 Iterable dataset resampling in PyTorch",
        "tags": [
            "python",
            "oversampling",
            "undersampling",
            "resampling",
            "pytorch",
            "imbalanced-learning"
        ]
    },
    "https://github.com/hakluke/how-to-exit-vim": {
        "extra-tags": [
            "simple"
        ],
        "date": "2019-09-25",
        "title": "how-to-exit-vim",
        "summary": "Below are some simple methods for exiting vim.",
        "tags": [
            "vim"
        ]
    },
    "https://github.com/VieVie31/bonapity": {
        "extra-tags": [],
        "date": "2019-07-28",
        "title": "bonapity",
        "summary": "Get a simple HTTP (REST) API with only this simple decorator : @bonapity !",
        "tags": [
            "python3",
            "cool-stuff",
            "python",
            "rest-api",
            "simple-api",
            "api"
        ]
    },
    "https://github.com/Accenture/AmpliGraph": {
        "extra-tags": [],
        "date": "2019-01-09",
        "title": "AmpliGraph",
        "summary": "Python library for Representation Learning on Knowledge Graphs https://docs.ampligraph.org",
        "tags": [
            "knowledge-graph-embeddings",
            "python",
            "graph-embeddings",
            "machine-learning",
            "representation-learning",
            "relational-learning",
            "graph-representation-learning",
            "knowledge-graph"
        ]
    },
    "https://github.com/fpservant/semanlink-kdmkb": {
        "extra-tags": [
            "data"
        ],
        "date": "2020-07-25",
        "title": "semanlink-kdmkb",
        "summary": "Export of semanlink data to kdmkb",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/villmow/datasets_knowledge_embedding": {
        "extra-tags": [],
        "date": "2018-01-25",
        "title": "datasets_knowledge_embedding",
        "summary": "Datasets for Knowledge Graph Completion with textual information about the entities",
        "tags": [
            "wn18rr",
            "knowledge-graphs-embeddings",
            "wn18",
            "knowledge-graph-completion",
            "wordnet",
            "dataset",
            "text",
            "knowledge-graph",
            "knowledgebase",
            "kgc"
        ]
    },
    "https://github.com/facebookresearch/faiss": {
        "extra-tags": [],
        "date": "2017-02-07",
        "title": "faiss",
        "summary": "A library for efficient similarity search and clustering of dense vectors.",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/feast-dev/feast": {
        "extra-tags": [
            "machine",
            "learning"
        ],
        "date": "2018-12-10",
        "title": "feast",
        "summary": "Feature Store for Machine Learning",
        "tags": [
            "python",
            "data-quality",
            "features",
            "big-data",
            "machine-learning",
            "mlops",
            "data-engineering",
            "data-science",
            "ml",
            "feature-store"
        ]
    },
    "https://github.com/huggingface/transformers": {
        "extra-tags": [],
        "date": "2018-10-29",
        "title": "transformers",
        "summary": "\ud83e\udd17 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.",
        "tags": [
            "pytorch-transformers",
            "machine-learning",
            "nlp-library",
            "seq2seq",
            "model-hub",
            "flax",
            "tensorflow",
            "nlp",
            "bert",
            "python",
            "transformer",
            "language-models",
            "pretrained-models",
            "deep-learning",
            "language-model",
            "natural-language-processing",
            "speech-recognition",
            "jax",
            "hacktoberfest",
            "pytorch"
        ]
    },
    "https://github.com/onnx/sklearn-onnx": {
        "extra-tags": [],
        "date": "2018-12-18",
        "title": "sklearn-onnx",
        "summary": "Convert scikit-learn models and pipelines to ONNX",
        "tags": [
            "scikit-learn",
            "onnx",
            "python"
        ]
    },
    "https://github.com/gbolmier/funk-svd": {
        "extra-tags": [],
        "date": "2019-04-03",
        "title": "funk-svd",
        "summary": ":zap: A python fast implementation of the famous SVD algorithm popularized by Simon Funk during Netflix Prize",
        "tags": [
            "recommendation-algorithm",
            "python",
            "numba"
        ]
    },
    "https://github.com/PaddlePaddle/Research": {
        "extra-tags": [],
        "date": "2020-02-13",
        "title": "Research",
        "summary": "novel deep learning research works with PaddlePaddle",
        "tags": [
            "data-mining",
            "python",
            "spatial-temporal",
            "nlp",
            "deep-learning",
            "knowledge-graph",
            "computer-vision"
        ]
    },
    "https://github.com/uma-pi1/kge": {
        "extra-tags": [],
        "date": "2019-02-22",
        "title": "kge",
        "summary": "LibKGE - A knowledge graph embedding library for reproducible research",
        "tags": [
            "python"
        ]
    },
    "https://github.com/MaxHalford/vose": {
        "extra-tags": [],
        "date": "2020-04-27",
        "title": "vose",
        "summary": "Cython implementation of Vose's Alias method",
        "tags": [
            "cython"
        ]
    },
    "https://github.com/MaxHalford/jan": {
        "extra-tags": [],
        "date": "2020-04-25",
        "title": "jan",
        "summary": "\ud83d\udca4 Just Another Neural network",
        "tags": [
            "python"
        ]
    },
    "https://github.com/online-ml/awesome-online-machine-learning": {
        "extra-tags": [],
        "date": "2019-08-27",
        "title": "awesome-online-machine-learning",
        "summary": ":bookmark_tabs: Online machine learning resources",
        "tags": [
            "awesome",
            "awesome-list",
            "machine-learning",
            "online-machine-learning"
        ]
    },
    "https://github.com/uqfoundation/dill": {
        "extra-tags": [],
        "date": "2013-06-28",
        "title": "dill",
        "summary": "serialize all of python",
        "tags": [
            "python"
        ]
    },
    "https://github.com/microsoft/codetour": {
        "extra-tags": [
            "code",
            "editor"
        ],
        "date": "2020-03-07",
        "title": "codetour",
        "summary": "VS Code extension that allows you to record and play back guided tours of codebases, directly within the editor.",
        "tags": [
            "vscode-extension",
            "knowledge-sharing",
            "typescript",
            "code-navigation",
            "onboarding",
            "vscode"
        ]
    },
    "https://github.com/ibalazevic/multirelational-poincare": {
        "extra-tags": [
            "graph",
            "embeddings"
        ],
        "date": "2019-05-20",
        "title": "multirelational-poincare",
        "summary": "Multi-relational Poincar\u00e9 Graph Embeddings",
        "tags": [
            "python"
        ]
    },
    "https://github.com/online-ml/chantilly": {
        "extra-tags": [],
        "date": "2019-10-03",
        "title": "chantilly",
        "summary": "\ud83c\udf66 Deployment tool for online machine learning models",
        "tags": [
            "python"
        ]
    },
    "https://github.com/grafana/grafana": {
        "extra-tags": [],
        "date": "2013-12-11",
        "title": "grafana",
        "summary": "The open and composable observability and data visualization platform. Visualize metrics, logs, and traces from multiple sources like Prometheus, Loki, Elasticsearch, InfluxDB, Postgres and many more. ",
        "tags": [
            "postgres",
            "influxdb",
            "go",
            "analytics",
            "dashboard",
            "monitoring",
            "alerting",
            "elasticsearch",
            "metrics",
            "business-intelligence",
            "hacktoberfest",
            "typescript",
            "prometheus",
            "data-visualization",
            "grafana",
            "mysql"
        ]
    },
    "https://github.com/jzck/kernel-zig": {
        "extra-tags": [],
        "date": "2019-05-10",
        "title": "kernel-zig",
        "summary": ":floppy_disk: hobby x86 kernel zig",
        "tags": [
            "kernel",
            "zig",
            "x86"
        ]
    },
    "https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding": {
        "extra-tags": [],
        "date": "2019-01-23",
        "title": "KnowledgeGraphEmbedding",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/facebookresearch/SentEval": {
        "extra-tags": [],
        "date": "2017-05-18",
        "title": "SentEval",
        "summary": "A python tool for evaluating the quality of sentence embeddings.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/vinsis/math-and-ml-notes": {
        "extra-tags": [],
        "date": "2019-09-11",
        "title": "math-and-ml-notes",
        "summary": "Books, papers and links to latest research in ML/AI",
        "tags": [
            "neural-networks",
            "papers",
            "jupyter notebook",
            "deep-learning"
        ]
    },
    "https://github.com/nmrksic/counter-fitting": {
        "extra-tags": [
            "vectors"
        ],
        "date": "2016-02-19",
        "title": "counter-fitting",
        "summary": "Counter-fitting Word Vectors to Linguistic Constraints",
        "tags": [
            "python"
        ]
    },
    "https://github.com/lucidrains/reformer-pytorch": {
        "extra-tags": [
            "transformer"
        ],
        "date": "2020-01-09",
        "title": "reformer-pytorch",
        "summary": "Reformer, the efficient Transformer, in Pytorch",
        "tags": [
            "python",
            "attention-mechanism",
            "machine-learning",
            "transformers",
            "pytorch",
            "artificial-intelligence"
        ]
    },
    "https://github.com/google/trax": {
        "extra-tags": [],
        "date": "2019-10-05",
        "title": "trax",
        "summary": "Trax \u2014 Deep Learning with Clear Code and Speed",
        "tags": [
            "python",
            "reinforcement-learning",
            "transformer",
            "jax",
            "machine-learning",
            "numpy",
            "deep-reinforcement-learning",
            "deep-learning"
        ]
    },
    "https://github.com/thunlp/KRLPapers": {
        "extra-tags": [],
        "date": "2018-02-03",
        "title": "KRLPapers",
        "summary": "Must-read papers on knowledge representation learning (KRL) / knowledge embedding (KE)",
        "tags": [
            "knowledge-embedding",
            "paper-list",
            "tex"
        ]
    },
    "https://github.com/thunlp/OpenKE": {
        "extra-tags": [],
        "date": "2017-10-08",
        "title": "OpenKE",
        "summary": "An Open-Source Package for Knowledge Embedding (KE)",
        "tags": [
            "knowledge-embedding",
            "python"
        ]
    },
    "https://github.com/philipperemy/n-beats": {
        "extra-tags": [],
        "date": "2019-07-24",
        "title": "n-beats",
        "summary": "Keras/Pytorch implementation of N-BEATS: Neural basis expansion analysis for interpretable time series forecasting.",
        "tags": [
            "python",
            "neural-networks",
            "pytorch",
            "deep-learning",
            "series-forecasting"
        ]
    },
    "https://github.com/eriklindernoren/Keras-GAN": {
        "extra-tags": [],
        "date": "2017-07-11",
        "title": "Keras-GAN",
        "summary": "Keras implementations of Generative Adversarial Networks.",
        "tags": [
            "python",
            "neural-networks",
            "keras",
            "deep-learning",
            "gan",
            "generative-adversarial-networks"
        ]
    },
    "https://github.com/wikipedia2vec/wikipedia2vec": {
        "extra-tags": [],
        "date": "2015-10-26",
        "title": "wikipedia2vec",
        "summary": "A tool for learning vector representations of words and entities from Wikipedia",
        "tags": [
            "natural-language-processing",
            "python",
            "text-classification",
            "embeddings",
            "nlp",
            "wikipedia"
        ]
    },
    "https://github.com/kudkudak/word-embeddings-benchmarks": {
        "extra-tags": [],
        "date": "2015-11-21",
        "title": "word-embeddings-benchmarks",
        "summary": "Package for evaluating word embeddings",
        "tags": [
            "python"
        ]
    },
    "https://github.com/mfaruqui/retrofitting": {
        "extra-tags": [
            "vectors"
        ],
        "date": "2014-11-15",
        "title": "retrofitting",
        "summary": "Retrofitting Word Vectors to Semantic Lexicons",
        "tags": [
            "python"
        ]
    },
    "https://github.com/SeldonIO/alibi-detect": {
        "extra-tags": [],
        "date": "2019-10-07",
        "title": "alibi-detect",
        "summary": "Algorithms for outlier, adversarial and drift detection",
        "tags": [
            "concept-drift",
            "images",
            "python",
            "drift-detection",
            "data-drift",
            "time-series",
            "adversarial",
            "outlier",
            "anomaly",
            "tabular-data",
            "semi-supervised-learning",
            "unsupervised-learning",
            "text",
            "detection"
        ]
    },
    "https://github.com/theeluwin/pytorch-sgns": {
        "extra-tags": [],
        "date": "2017-10-10",
        "title": "pytorch-sgns",
        "summary": "Skipgram Negative Sampling in PyTorch",
        "tags": [
            "word2vec",
            "skipgram",
            "python",
            "pytorch"
        ]
    },
    "https://github.com/MaxHalford/data-science-tutorials": {
        "extra-tags": [
            "data-science"
        ],
        "date": "2018-10-21",
        "title": "data-science-tutorials",
        "summary": "",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/XAI-ANITI/ethik": {
        "extra-tags": [],
        "date": "2019-04-01",
        "title": "ethik",
        "summary": ":mag_right: A toolbox for fair and explainable machine learning",
        "tags": [
            "python"
        ]
    },
    "https://github.com/vaexio/vaex": {
        "extra-tags": [],
        "date": "2014-09-27",
        "title": "vaex",
        "summary": "Out-of-Core hybrid Apache Arrow/NumPy DataFrame for Python, ML, visualization and exploration of big tabular data at a billion rows per second \ud83d\ude80",
        "tags": [
            "tabular-data",
            "python",
            "pyarrow",
            "machine-learning",
            "hdf5",
            "memory-mapped-file",
            "data-science",
            "machinelearning",
            "visualization",
            "bigdata",
            "dataframe"
        ]
    },
    "https://github.com/charliermarsh/semantic": {
        "extra-tags": [],
        "date": "2014-01-24",
        "title": "semantic",
        "summary": "A Python library for extracting semantic information from text, such as dates and numbers.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/AdilZouitine/outfit": {
        "extra-tags": [],
        "date": "2019-06-28",
        "title": "outfit",
        "summary": ":dress: Tidy up your machine learning experiments",
        "tags": [
            "experiments",
            "jupyter notebook",
            "machine-learning"
        ]
    },
    "https://github.com/french-ai/ressources": {
        "extra-tags": [
            "ai"
        ],
        "date": "2019-05-04",
        "title": "ressources",
        "summary": "Usefull french and english ressources to learn AI",
        "tags": []
    },
    "https://github.com/kzhai/InfVocLDA": {
        "extra-tags": [
            "online",
            "inference"
        ],
        "date": "2013-02-01",
        "title": "InfVocLDA",
        "summary": "Online Latent Dirichlet Allocation with Infinite Vocabulary using Variational Inference",
        "tags": [
            "python"
        ]
    },
    "https://github.com/KaggleSolutions/open-solution-home-credit": {
        "extra-tags": [
            "risk"
        ],
        "date": "2018-08-25",
        "title": "open-solution-home-credit",
        "summary": "Open solution to the Home Credit Default Risk challenge :house_with_garden:",
        "tags": [
            "python"
        ]
    },
    "https://github.com/online-ml/lol-match-duration": {
        "extra-tags": [
            "forecasting"
        ],
        "date": "2019-02-25",
        "title": "lol-match-duration",
        "summary": ":video_game: League of Legends match duration forecasting",
        "tags": [
            "python"
        ]
    },
    "https://github.com/titu1994/LSTM-FCN": {
        "extra-tags": [],
        "date": "2017-08-26",
        "title": "LSTM-FCN",
        "summary": "Codebase for the paper LSTM Fully Convolutional Networks for Time Series Classification",
        "tags": [
            "python",
            "keras",
            "cnn",
            "deep-learning",
            "lstm",
            "timeseries"
        ]
    },
    "https://github.com/ericpts/resume": {
        "extra-tags": [
            "resume"
        ],
        "date": "2018-06-08",
        "title": "resume",
        "summary": "My resume.",
        "tags": [
            "tex"
        ]
    },
    "https://github.com/jmcarpenter2/swifter": {
        "extra-tags": [],
        "date": "2018-04-07",
        "title": "swifter",
        "summary": "A package which efficiently applies any function to a pandas dataframe or series in the fastest available manner",
        "tags": [
            "modin",
            "python",
            "pandas-dataframe",
            "parallel-computing",
            "pandas",
            "dask",
            "parallelization"
        ]
    },
    "https://github.com/MaxHalford/starboost": {
        "extra-tags": [
            "gradient"
        ],
        "date": "2018-11-28",
        "title": "starboost",
        "summary": ":star::rocket: Gradient boosting on steroids",
        "tags": [
            "gradient-boosting",
            "scikit-learn",
            "python",
            "machine-learning"
        ]
    },
    "https://github.com/dmlc/treelite": {
        "extra-tags": [
            "compiler",
            "tree"
        ],
        "date": "2017-06-28",
        "title": "treelite",
        "summary": "model compiler for decision tree ensembles",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/NicolasHug/Surprise": {
        "extra-tags": [],
        "date": "2016-10-23",
        "title": "Surprise",
        "summary": "A Python scikit for building and analyzing recommender systems",
        "tags": [
            "matrix",
            "recommendation",
            "python",
            "svd",
            "machine-learning",
            "factorization",
            "systems",
            "recommender"
        ]
    },
    "https://github.com/MaxHalford/tuna": {
        "extra-tags": [],
        "date": "2018-03-22",
        "title": "tuna",
        "summary": ":fish: A streaming ETL for fish",
        "tags": [
            "feature-extraction",
            "go",
            "large-dataset",
            "machine-learning",
            "golang",
            "online-algorithms",
            "stream",
            "stream-processing",
            "etl"
        ]
    },
    "https://github.com/AdilZouitine/pyFeel": {
        "extra-tags": [],
        "date": "2018-04-20",
        "title": "pyFeel",
        "summary": "Python package for emotion analysis in French ",
        "tags": [
            "data-mining",
            "python",
            "emotion-analysis",
            "data-analysis",
            "nlp-library",
            "nlp",
            "data-science",
            "opinion-mining",
            "emotion"
        ]
    },
    "https://github.com/vi3k6i5/flashtext": {
        "extra-tags": [
            "flashtext"
        ],
        "date": "2017-08-15",
        "title": "flashtext",
        "summary": "Extract Keywords from sentence or Replace keywords in sentences.",
        "tags": [
            "data-extraction",
            "word2vec",
            "python",
            "search-in-text",
            "keyword-extraction",
            "nlp"
        ]
    },
    "https://github.com/MaxHalford/kaggle-DSG18-qualifier": {
        "extra-tags": [],
        "date": "2018-07-03",
        "title": "kaggle-DSG18-qualifier",
        "summary": "",
        "tags": [
            "binary-classification",
            "jupyter notebook",
            "lightgbm",
            "kaggle"
        ]
    },
    "https://github.com/MaxHalford/eaopt": {
        "extra-tags": [],
        "date": "2016-01-31",
        "title": "eaopt",
        "summary": ":four_leaf_clover: Evolutionary optimization library for Go (genetic algorithm, partical swarm optimization, differential evolution)",
        "tags": [
            "speciation",
            "go",
            "particle-swarm-optimization",
            "parallel",
            "metaheuristics",
            "machine-learning",
            "genetic-algorithm",
            "differential-evolution",
            "optimization",
            "evolutionary-algorithms",
            "evolutionary-computation"
        ]
    },
    "https://github.com/BorgwardtLab/sampling-outlier-detection": {
        "extra-tags": [
            "outlier-detection"
        ],
        "date": "2013-12-02",
        "title": "sampling-outlier-detection",
        "summary": "Rapid computation of distance-based outlierness scores via sampling",
        "tags": [
            "c"
        ]
    },
    "https://github.com/hyperdashio/hyperdash-sdk-py": {
        "extra-tags": [],
        "date": "2017-06-10",
        "title": "hyperdash-sdk-py",
        "summary": "Official Python SDK for Hyperdash",
        "tags": [
            "ipython",
            "jupyter-notebook",
            "sdk",
            "python",
            "keras",
            "machine-learning",
            "ai",
            "pytorch"
        ]
    },
    "https://github.com/MaxHalford/xgp": {
        "extra-tags": [
            "library"
        ],
        "date": "2017-07-20",
        "title": "xgp",
        "summary": ":crystal_ball: Symbolic regression library",
        "tags": [
            "go",
            "machine-learning",
            "classification",
            "symbolic-regression",
            "regression",
            "evolutionary-algorithms",
            "genetic-programming"
        ]
    },
    "https://github.com/MaxHalford/kaggle-recruit-restaurant": {
        "extra-tags": [],
        "date": "2018-02-07",
        "title": "kaggle-recruit-restaurant",
        "summary": ":trophy: Kaggle 8th place solution",
        "tags": [
            "kaggle-recruit-restaurant",
            "kaggle",
            "jupyter notebook",
            "lightgbm",
            "timeseries"
        ]
    },
    "https://github.com/MaxHalford/xam": {
        "extra-tags": [],
        "date": "2017-01-24",
        "title": "xam",
        "summary": ":dart: Personal data science and machine learning toolbox",
        "tags": [
            "python",
            "preprocessing",
            "stacking",
            "machine-learning",
            "data-science"
        ]
    },
    "https://github.com/x64dbg/x64dbg": {
        "extra-tags": [],
        "date": "2015-04-11",
        "title": "x64dbg",
        "summary": "An open-source user mode debugger for Windows. Optimized for reverse engineering and malware analysis.",
        "tags": [
            "ctf",
            "disassembler",
            "oscp",
            "cpp",
            "debugging",
            "x86-64",
            "debugger",
            "x86",
            "hacking",
            "windows",
            "x64",
            "malware-analysis",
            "reverse-engineering",
            "dynamic-analysis",
            "security-tools",
            "security",
            "binary-analysis",
            "c++",
            "exploit-development"
        ]
    },
    "https://github.com/salesforce/online_conformal": {
        "extra-tags": [
            "online",
            "prediction"
        ],
        "date": "2023-01-18",
        "title": "online_conformal",
        "summary": "Methods for online conformal prediction.",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/dream-faster/fold": {
        "extra-tags": [],
        "date": "2022-12-16",
        "title": "fold",
        "summary": "Nowcasting (single step ahead prediction on time series) on a rolling/expanding window basis. ",
        "tags": [
            "python",
            "financial-machine-learning",
            "nowcasting",
            "time-series-classification",
            "time-series",
            "machine-learning",
            "time-series-forecasting"
        ]
    },
    "https://github.com/shikijs/shiki": {
        "extra-tags": [
            "beautiful",
            "syntax"
        ],
        "date": "2018-10-29",
        "title": "shiki",
        "summary": "A beautiful Syntax Highlighter.",
        "tags": [
            "typescript"
        ]
    },
    "https://github.com/z3z1ma/dbt-osmosis": {
        "extra-tags": [],
        "date": "2021-09-25",
        "title": "dbt-osmosis",
        "summary": "Provides a dbt server, streamlit workbench, automated YAML management, and git-integrated dbt model output diff tools",
        "tags": [
            "testing",
            "python",
            "modelling",
            "dbt",
            "cli",
            "sql",
            "data",
            "documentation",
            "editor"
        ]
    },
    "https://github.com/qweeze/uring_file": {
        "extra-tags": [
            "asynchronous"
        ],
        "date": "2020-11-01",
        "title": "uring_file",
        "summary": "Asynchronous file I/O with io_uring and asyncio",
        "tags": [
            "liburing",
            "asyncio",
            "python"
        ]
    },
    "https://github.com/Kanaries/pygwalker": {
        "extra-tags": [],
        "date": "2023-02-16",
        "title": "pygwalker",
        "summary": "PyGWalker: Turn your pandas dataframe into a Tableau-style User Interface for visual analysis",
        "tags": [
            "data-analysis",
            "tableau-alternative",
            "jupyter notebook",
            "tableau",
            "pandas",
            "visualization",
            "dataframe",
            "data-exploration"
        ]
    },
    "https://github.com/risingwavelabs/risingwave": {
        "extra-tags": [],
        "date": "2022-01-28",
        "title": "risingwave",
        "summary": "RisingWave: A Distributed SQL Database for Stream Processing",
        "tags": [
            "cloud-native",
            "sql",
            "rust",
            "postgresql",
            "database",
            "serverless",
            "distributed-database",
            "stream-processing"
        ]
    },
    "https://github.com/deephaven/deephaven-core": {
        "extra-tags": [],
        "date": "2021-01-19",
        "title": "deephaven-core",
        "summary": "Deephaven Community Core",
        "tags": [
            "deephaven",
            "java"
        ]
    },
    "https://github.com/n8n-io/n8n": {
        "extra-tags": [],
        "date": "2019-06-22",
        "title": "n8n",
        "summary": "Free and source-available fair-code licensed workflow automation tool. Easily automate tasks across different services.",
        "tags": [
            "node",
            "iaas",
            "cli",
            "automated",
            "workflow-automation",
            "n8n",
            "typescript",
            "ipaas",
            "low-code-development-platform",
            "apis",
            "low-code-platform",
            "integrations",
            "self-hosted",
            "docker",
            "data-flow",
            "development",
            "automation",
            "workflow",
            "low-code",
            "integration-framework"
        ]
    },
    "https://github.com/spencermountain/compromise": {
        "extra-tags": [
            "natural-language",
            "processing"
        ],
        "date": "2011-07-05",
        "title": "compromise",
        "summary": "modest natural-language processing",
        "tags": [
            "part-of-speech",
            "javascript",
            "nlp",
            "named-entity-recognition"
        ]
    },
    "https://github.com/sfu-db/connector-x": {
        "extra-tags": [],
        "date": "2021-01-13",
        "title": "connector-x",
        "summary": "Fastest library to load data from DB to DataFrames in Rust and Python",
        "tags": [
            "python",
            "sql",
            "rust",
            "database",
            "dataframe"
        ]
    },
    "https://github.com/zineland/zine": {
        "extra-tags": [],
        "date": "2022-02-25",
        "title": "zine",
        "summary": "Zine - a simple and opinionated tool to build your own magazine.",
        "tags": [
            "ssg",
            "magazine",
            "rust",
            "rust-lang",
            "static-site-generator",
            "zine"
        ]
    },
    "https://github.com/uwdata/arquero": {
        "extra-tags": [],
        "date": "2020-09-01",
        "title": "arquero",
        "summary": "Query processing and transformation of array-backed data tables.",
        "tags": [
            "transform",
            "javascript",
            "table",
            "data",
            "arrays",
            "database",
            "dataframe",
            "query"
        ]
    },
    "https://github.com/ploomber/jupysql": {
        "extra-tags": [],
        "date": "2022-07-26",
        "title": "jupysql",
        "summary": "Better SQL in Jupyter. \ud83d\udcca",
        "tags": [
            "postgres",
            "spark-sql",
            "python",
            "tsql",
            "sql",
            "bigquery",
            "presto",
            "clickhouse",
            "duckdb",
            "jupyter",
            "redshift",
            "sqlite",
            "data-science",
            "data-engineering",
            "hive",
            "trino",
            "snowflake",
            "mysql"
        ]
    },
    "https://github.com/sraoss/pg_ivm": {
        "extra-tags": [
            "postgresql"
        ],
        "date": "2022-03-24",
        "title": "pg_ivm",
        "summary": "IVM (Incremental View Maintenance) implementation as a PostgreSQL extension",
        "tags": [
            "c"
        ]
    },
    "https://github.com/plouc/nivo": {
        "extra-tags": [],
        "date": "2016-04-16",
        "title": "nivo",
        "summary": "nivo provides a rich set of dataviz components, built on top of the awesome d3 and React libraries",
        "tags": [
            "dataviz",
            "d3js",
            "isomorphic",
            "react",
            "charts",
            "components",
            "svg",
            "typescript",
            "canvas"
        ]
    },
    "https://github.com/apache/echarts": {
        "extra-tags": [],
        "date": "2013-04-03",
        "title": "echarts",
        "summary": "Apache ECharts is a powerful, interactive charting and data visualization library for browser",
        "tags": [
            "charting-library",
            "echarts",
            "data-viz",
            "apache",
            "charts",
            "svg",
            "typescript",
            "data-visualization",
            "visualization",
            "canvas"
        ]
    },
    "https://github.com/benbjohnson/litestream": {
        "extra-tags": [],
        "date": "2020-10-06",
        "title": "litestream",
        "summary": "Streaming replication for SQLite.",
        "tags": [
            "sqlite",
            "s3",
            "go",
            "replication"
        ]
    },
    "https://github.com/ruuda/kilsbergen": {
        "extra-tags": [],
        "date": "2019-03-11",
        "title": "kilsbergen",
        "summary": "A clean MkDocs theme",
        "tags": [
            "mkdocs",
            "html",
            "theme"
        ]
    },
    "https://github.com/libffcv/ffcv": {
        "extra-tags": [],
        "date": "2021-10-13",
        "title": "ffcv",
        "summary": "FFCV: Fast Forward Computer Vision (and other ML workloads!)",
        "tags": [
            "pytorch",
            "python",
            "data-science",
            "machine-learning"
        ]
    },
    "https://github.com/widgetti/reacton": {
        "extra-tags": [],
        "date": "2022-02-09",
        "title": "reacton",
        "summary": "A pure Python port of\u00a0React for ipywidgets",
        "tags": [
            "python",
            "react",
            "user-interface",
            "jupyter",
            "ipywidgets"
        ]
    },
    "https://github.com/pikepdf/pikepdf": {
        "extra-tags": [],
        "date": "2017-09-14",
        "title": "pikepdf",
        "summary": "A Python library for reading and writing PDF, powered by qpdf",
        "tags": [
            "python",
            "pikepdf",
            "pdf-manipulation",
            "pdf-generation",
            "pypdf2",
            "qpdf",
            "existing-pdfs",
            "pdf"
        ]
    },
    "https://github.com/normconf/awesome-normconf": {
        "extra-tags": [],
        "date": "2022-12-01",
        "title": "awesome-normconf",
        "summary": "List of resources coming out of Normconf Slack",
        "tags": []
    },
    "https://github.com/triggerdotdev/jsonhero-web": {
        "extra-tags": [],
        "date": "2022-03-01",
        "title": "jsonhero-web",
        "summary": "JSON Hero is an open-source, beautiful JSON explorer for the web that lets you browse, search and navigate your JSON files at speed. \ud83d\ude80. Built with \ud83d\udc9c by the Trigger.dev team.",
        "tags": [
            "devtools",
            "viewer",
            "react",
            "json-viewer",
            "hacktoberfest",
            "tools",
            "typescript",
            "developer-tools",
            "json"
        ]
    },
    "https://github.com/tortoise/tortoise-orm": {
        "extra-tags": [],
        "date": "2018-03-29",
        "title": "tortoise-orm",
        "summary": "Familiar asyncio ORM for python, built with relations in mind",
        "tags": [
            "async",
            "python3",
            "python",
            "asyncio",
            "postgresql",
            "sqlite",
            "mysql",
            "orm"
        ]
    },
    "https://github.com/MaterializeInc/materialize-dbt-utils": {
        "extra-tags": [],
        "date": "2021-01-12",
        "title": "materialize-dbt-utils",
        "summary": "Utility functions for dbt projects running on Materialize",
        "tags": [
            "makefile"
        ]
    },
    "https://github.com/nat/natbot": {
        "extra-tags": [
            "browser",
            "gpt-3"
        ],
        "date": "2022-09-29",
        "title": "natbot",
        "summary": "Drive a browser with GPT-3",
        "tags": [
            "python"
        ]
    },
    "https://github.com/memphisdev/memphis": {
        "extra-tags": [],
        "date": "2022-02-01",
        "title": "memphis",
        "summary": "Next-Generation Real-Time Data Processing Platform",
        "tags": [
            "message-bus",
            "data-stream-processing",
            "go",
            "kubernetes",
            "schema-registry",
            "messaging-queue",
            "data",
            "golang",
            "data-engineering",
            "message-queue",
            "microservices",
            "data-pipeline",
            "enrichment",
            "message-broker",
            "data-streaming"
        ]
    },
    "https://github.com/DataDome/sliceline": {
        "extra-tags": [],
        "date": "2022-06-29",
        "title": "sliceline",
        "summary": "\u2702\ufe0f Fast slice finding for Machine Learning model debugging.",
        "tags": [
            "contrast-set-mining",
            "ml-debug",
            "python"
        ]
    },
    "https://github.com/geohot/tinygrad": {
        "extra-tags": [
            "pytorch"
        ],
        "date": "2020-10-18",
        "title": "tinygrad",
        "summary": "You like pytorch? You like micrograd? You love tinygrad! \u2764\ufe0f ",
        "tags": [
            "python"
        ]
    },
    "https://github.com/ChartsCSS/charts.css": {
        "extra-tags": [],
        "date": "2020-09-26",
        "title": "charts.css",
        "summary": "Open source CSS framework for data visualization.",
        "tags": [
            "html",
            "css",
            "css-framework",
            "charts",
            "chart",
            "data-visualization",
            "scss",
            "ui-components",
            "visualization"
        ]
    },
    "https://github.com/outbrain/fwumious_wabbit": {
        "extra-tags": [],
        "date": "2020-09-14",
        "title": "fwumious_wabbit",
        "summary": "Fwumious Wabbit, fast on-line machine learning toolkit written in Rust",
        "tags": [
            "online-learning",
            "factorization-machines",
            "rust",
            "incremental-learning",
            "logistic-regression",
            "machine-learning"
        ]
    },
    "https://github.com/datamade/parserator": {
        "extra-tags": [
            "probabilistic"
        ],
        "date": "2014-10-16",
        "title": "parserator",
        "summary": ":bookmark: A toolkit for making domain-specific probabilistic parsers",
        "tags": [
            "nlp-parsing",
            "python",
            "probabilistic-parser",
            "crf"
        ]
    },
    "https://github.com/idyll-lang/fidyll": {
        "extra-tags": [],
        "date": "2021-06-03",
        "title": "fidyll",
        "summary": "Research project for cross-platform Idyll projects",
        "tags": [
            "javascript"
        ]
    },
    "https://github.com/cloud-carbon-footprint/cloud-carbon-footprint": {
        "extra-tags": [],
        "date": "2020-11-17",
        "title": "cloud-carbon-footprint",
        "summary": "Cloud Carbon Footprint is a tool to estimate energy use (kilowatt-hours) and carbon emissions (metric tons CO2e) from public cloud usage",
        "tags": [
            "carbon-emissions",
            "carbon-footprint",
            "climate",
            "sustainability",
            "hacktoberfest",
            "typescript",
            "cloud",
            "thoughtworks"
        ]
    },
    "https://github.com/phiresky/sql.js-httpvfs": {
        "extra-tags": [],
        "date": "2021-05-02",
        "title": "sql.js-httpvfs",
        "summary": "Hosting read-only SQLite databases on static file hosters like Github Pages",
        "tags": [
            "typescript"
        ]
    },
    "https://github.com/splitgraph/seafowl": {
        "extra-tags": [],
        "date": "2022-07-04",
        "title": "seafowl",
        "summary": "Analytical database for data-driven Web applications",
        "tags": [
            "sql",
            "rust",
            "edge",
            "database",
            "http",
            "serverless",
            "api",
            "visualization"
        ]
    },
    "https://github.com/karpathy/makemore": {
        "extra-tags": [
            "autoregressive",
            "language"
        ],
        "date": "2022-06-09",
        "title": "makemore",
        "summary": "An autoregressive character-level language model for making more things",
        "tags": [
            "python"
        ]
    },
    "https://github.com/transitive-bullshit/nextjs-notion-starter-kit": {
        "extra-tags": [],
        "date": "2021-01-15",
        "title": "nextjs-notion-starter-kit",
        "summary": "Deploy your own Notion-powered website in minutes with Next.js and Vercel.",
        "tags": [
            "blog",
            "portfolio",
            "nextjs",
            "react",
            "react-notion-x",
            "typescript",
            "notion"
        ]
    },
    "https://github.com/mlco2/codecarbon": {
        "extra-tags": [
            "environment"
        ],
        "date": "2020-05-12",
        "title": "codecarbon",
        "summary": "Track emissions from Compute and recommend ways to reduce their impact on the environment.",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/huawei-noah/streamDM": {
        "extra-tags": [],
        "date": "2015-06-08",
        "title": "streamDM",
        "summary": "Stream Data Mining Library for Spark Streaming",
        "tags": [
            "scala"
        ]
    },
    "https://github.com/jubatus/jubatus": {
        "extra-tags": [],
        "date": "2011-10-25",
        "title": "jubatus",
        "summary": "Framework and Library for Distributed Online Machine Learning",
        "tags": [
            "machine-learning",
            "distributed",
            "c-plus-plus",
            "ml",
            "c++"
        ]
    },
    "https://github.com/turbot/steampipe": {
        "extra-tags": [],
        "date": "2021-01-17",
        "title": "steampipe",
        "summary": "Use SQL to instantly query your cloud services (AWS, Azure, GCP and more). Open source CLI. No DB required. ",
        "tags": [
            "fdw",
            "kubernetes",
            "cloud",
            "aws",
            "cspm",
            "cwpp",
            "azure",
            "postgresql",
            "devops",
            "go",
            "devsecops",
            "postgresql-fdw",
            "cnapp",
            "gcp",
            "sql",
            "hacktoberfest",
            "steampipe",
            "golang",
            "security",
            "cis",
            "terraform"
        ]
    },
    "https://github.com/addthis/stream-lib": {
        "extra-tags": [
            "stream",
            "estimator"
        ],
        "date": "2011-03-15",
        "title": "stream-lib",
        "summary": "Stream summarizer and cardinality estimator.",
        "tags": [
            "java"
        ]
    },
    "https://github.com/Waikato/moa": {
        "extra-tags": [],
        "date": "2014-05-02",
        "title": "moa",
        "summary": "MOA is an open source framework for Big Data stream mining. It includes a collection of machine learning algorithms (classification, regression, clustering, outlier detection, concept drift detection and recommender systems) and tools for evaluation.",
        "tags": [
            "moa",
            "java",
            "streaming-algorithms",
            "machine-learning",
            "data-stream-mining",
            "clustering",
            "machine-learning-algorithms"
        ]
    },
    "https://github.com/fossunited/joy": {
        "extra-tags": [],
        "date": "2021-06-04",
        "title": "joy",
        "summary": "Joy is a tiny creative coding library in Python.",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/feathr-ai/feathr": {
        "extra-tags": [
            "performance"
        ],
        "date": "2022-02-18",
        "title": "feathr",
        "summary": "Feathr \u2013 An Enterprise-Grade, High Performance Feature Store",
        "tags": [
            "scala",
            "feature-metadata",
            "data-quality",
            "feature-marketplace",
            "machine-learning",
            "feature-engineering",
            "feature-management",
            "azure",
            "feature-platform",
            "mlops",
            "data-engineering",
            "data-science",
            "feature-store",
            "feature-governance",
            "apache-spark",
            "artificial-intelligence"
        ]
    },
    "https://github.com/dankeyy/incdec.py": {
        "extra-tags": [],
        "date": "2022-09-16",
        "title": "incdec.py",
        "summary": "for all your ++ -- needs",
        "tags": [
            "python"
        ]
    },
    "https://github.com/proplot-dev/proplot": {
        "extra-tags": [],
        "date": "2017-12-06",
        "title": "proplot",
        "summary": "\ud83c\udfa8 A succinct matplotlib wrapper for making beautiful, publication-quality graphics",
        "tags": [
            "plotting",
            "matplotlib",
            "python",
            "data-visualization"
        ]
    },
    "https://github.com/napari/napari": {
        "extra-tags": [],
        "date": "2018-08-13",
        "title": "napari",
        "summary": "napari: a fast, interactive, multi-dimensional image viewer for python",
        "tags": [
            "napari",
            "python",
            "visualization",
            "numpy"
        ]
    },
    "https://github.com/taosdata/TDengine": {
        "extra-tags": [],
        "date": "2019-07-11",
        "title": "TDengine",
        "summary": "TDengine is an open source, high-performance, cloud native time-series database optimized for Internet of Things (IoT), Connected Cars, Industrial IoT and DevOps.",
        "tags": [
            "cloud-native",
            "time-series-database",
            "monitoring",
            "sql",
            "industrial-iot",
            "metrics",
            "tdengine",
            "time-series",
            "scalability",
            "iot",
            "connected-vehicles",
            "financial-analysis",
            "distributed",
            "database",
            "tsdb",
            "cluster",
            "bigdata",
            "c"
        ]
    },
    "https://github.com/ibestvina/datasloth": {
        "extra-tags": [],
        "date": "2022-08-29",
        "title": "datasloth",
        "summary": "Natural language Pandas queries and data generation powered by GPT-3",
        "tags": [
            "pandas",
            "gpt-3",
            "python"
        ]
    },
    "https://github.com/velascoluis/serverless-duckdb": {
        "extra-tags": [],
        "date": "2022-08-30",
        "title": "serverless-duckdb",
        "summary": "A serverless duckDB deployment at GCP",
        "tags": [
            "python"
        ]
    },
    "https://github.com/inveniosoftware/dictdiffer": {
        "extra-tags": [
            "diff"
        ],
        "date": "2013-05-25",
        "title": "dictdiffer",
        "summary": "Dictdiffer is a module that helps you to diff and patch dictionaries. ",
        "tags": [
            "python"
        ]
    },
    "https://github.com/gregrahn/tpcds-kit": {
        "extra-tags": [
            "ds"
        ],
        "date": "2012-11-02",
        "title": "tpcds-kit",
        "summary": "TPC-DS benchmark kit with some modifications/fixes",
        "tags": [
            "benchmark",
            "c",
            "database",
            "sql"
        ]
    },
    "https://github.com/IBM/sail": {
        "extra-tags": [],
        "date": "2021-09-21",
        "title": "sail",
        "summary": "Library for streaming data and incremental learning algorithms.",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/google/jsonnet": {
        "extra-tags": [],
        "date": "2014-08-01",
        "title": "jsonnet",
        "summary": "Jsonnet - The data templating language",
        "tags": [
            "config",
            "configuration",
            "functional",
            "jsonnet",
            "json"
        ]
    },
    "https://github.com/neogeny/TatSu": {
        "extra-tags": [],
        "date": "2017-05-02",
        "title": "TatSu",
        "summary": "\u7adc TatSu generates Python parsers from grammars in a variation of EBNF",
        "tags": [
            "python3",
            "python",
            "walker",
            "parser",
            "ast",
            "parser-generator",
            "grammar",
            "ebnf",
            "parser-library",
            "python2"
        ]
    },
    "https://github.com/quil/quil": {
        "extra-tags": [
            "source",
            "code"
        ],
        "date": "2012-03-05",
        "title": "quil",
        "summary": "Main repo. Quil source code.",
        "tags": [
            "clojure"
        ]
    },
    "https://github.com/EthanRosenthal/aispy": {
        "extra-tags": [],
        "date": "2022-05-03",
        "title": "aispy",
        "summary": "ML monitoring with materialize.com",
        "tags": []
    },
    "https://github.com/BenPortner/brightway_recipes": {
        "extra-tags": [],
        "date": "2019-11-18",
        "title": "brightway_recipes",
        "summary": "Calculation recipes for brightway2. Instructions to becoming an LCA gourmet.",
        "tags": [
            "jupyter-notebook",
            "brightway",
            "ecoinvent",
            "lca",
            "tutorial",
            "getting-started",
            "jupyter notebook"
        ]
    },
    "https://github.com/JaidedAI/EasyOCR": {
        "extra-tags": [],
        "date": "2020-03-14",
        "title": "EasyOCR",
        "summary": "Ready-to-use OCR with 80+ supported languages and all popular writing scripts including Latin, Chinese, Arabic, Devanagari, Cyrillic and etc.",
        "tags": [
            "easyocr",
            "data-mining",
            "python",
            "information-retrieval",
            "machine-learning",
            "ocr",
            "optical-character-recognition",
            "image-processing",
            "scene-text",
            "cnn",
            "scene-text-recognition",
            "pytorch",
            "deep-learning",
            "lstm",
            "crnn"
        ]
    },
    "https://github.com/ljvmiranda921/prodigy-pdf-custom-recipe": {
        "extra-tags": [],
        "date": "2022-05-02",
        "title": "prodigy-pdf-custom-recipe",
        "summary": "Custom recipe and utilities for document processing",
        "tags": [
            "python"
        ]
    },
    "https://github.com/jaidevd/numerizer": {
        "extra-tags": [
            "language"
        ],
        "date": "2019-12-02",
        "title": "numerizer",
        "summary": "A Python module to convert natural language numerics into ints and floats.",
        "tags": [
            "python",
            "regular-expressions",
            "spacy-extension",
            "nlp",
            "information-extraction",
            "spacy"
        ]
    },
    "https://github.com/TeamHG-Memex/sklearn-crfsuite": {
        "extra-tags": [],
        "date": "2015-11-26",
        "title": "sklearn-crfsuite",
        "summary": "scikit-learn inspired API for CRFsuite",
        "tags": [
            "python"
        ]
    },
    "https://github.com/ContinualAI/avalanche": {
        "extra-tags": [],
        "date": "2020-03-05",
        "title": "avalanche",
        "summary": "Avalanche: an End-to-End Library for Continual Learning based on PyTorch.",
        "tags": [
            "evaluation",
            "lifelong-learning",
            "python",
            "continualai",
            "metrics",
            "strategies",
            "benchmarks",
            "framework",
            "training",
            "pytorch",
            "deep-learning",
            "continual-learning",
            "library"
        ]
    },
    "https://github.com/Belval/TextRecognitionDataGenerator": {
        "extra-tags": [],
        "date": "2017-07-01",
        "title": "TextRecognitionDataGenerator",
        "summary": "A synthetic data generator for text recognition",
        "tags": [
            "synthetic",
            "text-recognition",
            "python",
            "training-set-generator",
            "ocr",
            "dataset",
            "data",
            "fake",
            "text"
        ]
    },
    "https://github.com/NaturalNode/natural": {
        "extra-tags": [
            "language",
            "node"
        ],
        "date": "2011-05-07",
        "title": "natural",
        "summary": "general natural language facilities for node",
        "tags": [
            "javascript"
        ]
    },
    "https://github.com/microprediction/timemachines": {
        "extra-tags": [],
        "date": "2021-01-04",
        "title": "timemachines",
        "summary": "Predict time-series with one line of code. ",
        "tags": [
            "predictive-modeling",
            "timeseries-analysis",
            "predictions",
            "python",
            "prediction-algorithm",
            "time-series",
            "time-series-analysis",
            "timeseries-forecasting",
            "timeseries-data",
            "prediction",
            "timeseries"
        ]
    },
    "https://github.com/romkatv/powerlevel10k": {
        "extra-tags": [
            "theme"
        ],
        "date": "2019-02-24",
        "title": "powerlevel10k",
        "summary": "A Zsh theme",
        "tags": [
            "zsh",
            "shell"
        ]
    },
    "https://github.com/rose-pine/rose-pine-theme": {
        "extra-tags": [
            "theme"
        ],
        "date": "2020-03-21",
        "title": "rose-pine-theme",
        "summary": "All natural pine, faux fur and a bit of soho vibes for the classy minimalist",
        "tags": [
            "aesthetic",
            "palette",
            "ui-theme",
            "colorscheme",
            "syntax-theme"
        ]
    },
    "https://github.com/fal-ai/fal": {
        "extra-tags": [],
        "date": "2021-01-24",
        "title": "fal",
        "summary": "do more with dbt. fal helps you run Python alongside dbt, so you can send Slack alerts, detect anomalies and build machine learning models.",
        "tags": [
            "python",
            "analytics",
            "dbt",
            "machine-learning",
            "data-modeling",
            "machinelearning",
            "pandas"
        ]
    },
    "https://github.com/simonw/google-calendar-to-sqlite": {
        "extra-tags": [],
        "date": "2022-05-21",
        "title": "google-calendar-to-sqlite",
        "summary": "Create a SQLite database containing your data from Google Calendar",
        "tags": [
            "python"
        ]
    },
    "https://github.com/magicbookproject/magicbook": {
        "extra-tags": [
            "book",
            "project"
        ],
        "date": "2016-03-07",
        "title": "magicbook",
        "summary": "The magic book project returns!",
        "tags": [
            "javascript"
        ]
    },
    "https://github.com/postgresml/postgresml": {
        "extra-tags": [],
        "date": "2022-04-11",
        "title": "postgresml",
        "summary": "PostgresML is an end-to-end machine learning system. It enables you to train models and make online predictions using only SQL, without your data ever leaving your favorite database.",
        "tags": [
            "python",
            "rust",
            "machine-learning",
            "mlops",
            "postgresql",
            "ml"
        ]
    },
    "https://github.com/lelit/pglast": {
        "extra-tags": [],
        "date": "2017-08-07",
        "title": "pglast",
        "summary": "PostgreSQL Languages AST and statements prettifier: master branch covers PG10, v2 branch covers PG12, v3 covers PG13, v4 covers PG14, v5 covers PG15",
        "tags": [
            "postgresql",
            "sql-formatter",
            "python",
            "python-3"
        ]
    },
    "https://github.com/colinhacks/zod": {
        "extra-tags": [],
        "date": "2020-03-07",
        "title": "zod",
        "summary": "TypeScript-first schema validation with static type inference",
        "tags": [
            "runtime-validation",
            "schema-validation",
            "type-inference",
            "typescript",
            "static-types"
        ]
    },
    "https://github.com/batchcorp/plumber": {
        "extra-tags": [],
        "date": "2020-07-28",
        "title": "plumber",
        "summary": "A swiss army knife CLI tool for interacting with Kafka, RabbitMQ and other messaging systems.",
        "tags": [
            "message-bus",
            "protobuf",
            "go",
            "event-driven",
            "hacktoberfest",
            "rabbitmq",
            "golang",
            "event-bus",
            "message-queue",
            "kafka"
        ]
    },
    "https://github.com/J535D165/recordlinkage": {
        "extra-tags": [
            "detection"
        ],
        "date": "2015-10-18",
        "title": "recordlinkage",
        "summary": "A powerful and modular toolkit for record linkage and duplicate detection in Python",
        "tags": [
            "privacy",
            "string-distance",
            "python",
            "similarity",
            "machine-learning",
            "utrecht-university",
            "dedupe",
            "entity-resolution",
            "data-matching",
            "record-linkage",
            "deduplication",
            "python-library"
        ]
    },
    "https://github.com/protontypes/open-sustainable-technology": {
        "extra-tags": [],
        "date": "2020-09-18",
        "title": "open-sustainable-technology",
        "summary": "A curated list of open technology projects to sustain a stable climate, energy supply, biodiversity and natural resources. ",
        "tags": [
            "battery",
            "renewable-energy",
            "sustainable-development-goals",
            "climate-science",
            "wind-turbine",
            "jupyter notebook",
            "carbon-emissions",
            "clean-energy",
            "climate-data",
            "energy-data",
            "energy-consumption",
            "energy",
            "awesome",
            "awesome-list",
            "ocean",
            "carbon-footprint",
            "climate",
            "climate-change",
            "sustainability",
            "geoscience",
            "photovoltaic"
        ]
    },
    "https://github.com/copier-org/copier": {
        "extra-tags": [],
        "date": "2011-11-01",
        "title": "copier",
        "summary": "Library and command-line utility for rendering projects templates.",
        "tags": [
            "cookiecutter",
            "python",
            "hacktoberfest",
            "project-template",
            "copier-template",
            "scaffolding"
        ]
    },
    "https://github.com/supabase/supa_audit": {
        "extra-tags": [
            "generic",
            "table"
        ],
        "date": "2022-02-09",
        "title": "supa_audit",
        "summary": "Generic Table Auditing",
        "tags": [
            "plpgsql"
        ]
    },
    "https://github.com/Calamari-OCR/calamari": {
        "extra-tags": [],
        "date": "2018-03-20",
        "title": "calamari",
        "summary": "Line based ATR Engine based on OCRopy",
        "tags": [
            "python"
        ]
    },
    "https://github.com/raphaelsty/cherche-api": {
        "extra-tags": [],
        "date": "2022-02-18",
        "title": "cherche-api",
        "summary": "Deploy Cherche using FastAPI and Docker",
        "tags": [
            "docker",
            "python",
            "bm25",
            "question-answering",
            "fastapi",
            "tfidf",
            "summarization",
            "neural-search"
        ]
    },
    "https://github.com/nithinmurali/pygsheets": {
        "extra-tags": [],
        "date": "2016-06-06",
        "title": "pygsheets",
        "summary": "Google Sheets Python API v4",
        "tags": [
            "python",
            "spreadsheet",
            "google-sheets-api",
            "google-sheets-api-v4",
            "google-sheets",
            "python-lib",
            "google-sheets-library"
        ]
    },
    "https://github.com/KappaML/kappaml-core": {
        "extra-tags": [],
        "date": "2022-01-10",
        "title": "kappaml-core",
        "summary": "Online automated machine learning algorithms from KappaML",
        "tags": [
            "online",
            "kappaml",
            "automl",
            "python"
        ]
    },
    "https://github.com/lark-parser/lark": {
        "extra-tags": [],
        "date": "2017-02-04",
        "title": "lark",
        "summary": "Lark is a parsing toolkit for Python, built with a focus on ergonomics, performance and modularity.",
        "tags": [
            "lalr",
            "python",
            "cyk",
            "parser",
            "grammar",
            "earley",
            "parser-library",
            "parsing-library",
            "lark",
            "parse",
            "tree",
            "parsing-engine"
        ]
    },
    "https://github.com/mrabarnett/mrab-regex": {
        "extra-tags": [
            "regex"
        ],
        "date": "2020-11-02",
        "title": "mrab-regex",
        "summary": "",
        "tags": [
            "c"
        ]
    },
    "https://github.com/jamespwilliams/ebnf-shipping-forecast": {
        "extra-tags": [],
        "date": "2022-02-06",
        "title": "ebnf-shipping-forecast",
        "summary": "EBNF specification of the BBC's shipping forecast",
        "tags": [
            "ebnf",
            "forecast",
            "shipping"
        ]
    },
    "https://github.com/dcajasn/Riskfolio-Lib": {
        "extra-tags": [],
        "date": "2020-03-02",
        "title": "Riskfolio-Lib",
        "summary": "Portfolio Optimization and Quantitative Strategic Asset Allocation in Python",
        "tags": [
            "convex-optimization",
            "efficient-frontier",
            "asset-allocation",
            "risk-factors",
            "risk-parity",
            "sharpe-ratio",
            "investment-analysis",
            "risk-contribution",
            "quantitative-finance",
            "drawdown-model",
            "duration-matching",
            "finance",
            "principal-components-regression",
            "portfolio-optimization",
            "cvar-optimization",
            "trading",
            "portfolio-management",
            "investment",
            "cvxpy",
            "c++",
            "stepwise-regression"
        ]
    },
    "https://github.com/open-policy-agent/opa": {
        "extra-tags": [],
        "date": "2015-12-28",
        "title": "opa",
        "summary": "An open source, general-purpose policy engine.",
        "tags": [
            "cloud-native",
            "go",
            "compliance",
            "lolcat",
            "declarative",
            "open-policy-agent",
            "policy",
            "json",
            "doge",
            "authorization",
            "opa"
        ]
    },
    "https://github.com/ymirsky/Kitsune-py": {
        "extra-tags": [],
        "date": "2018-07-01",
        "title": "Kitsune-py",
        "summary": "A network intrusion detection system based on incremental statistics (AfterImage) and an ensemble of autoencoders (KitNET)",
        "tags": [
            "python"
        ]
    },
    "https://github.com/lmcinnes/pynndescent": {
        "extra-tags": [],
        "date": "2018-02-07",
        "title": "pynndescent",
        "summary": "A Python nearest neighbor descent for approximate nearest neighbors",
        "tags": [
            "approximate-nearest-neighbor-search",
            "nearest-neighbor-search",
            "python",
            "knn-graphs"
        ]
    },
    "https://github.com/raphaelsty/cherche": {
        "extra-tags": [],
        "date": "2021-12-04",
        "title": "cherche",
        "summary": "\ud83d\udcd1 Neural Search",
        "tags": [
            "natural-language-processing",
            "python",
            "search",
            "bm25",
            "neural-networks",
            "question-answering",
            "reader",
            "information-retrieval",
            "machine-learning",
            "semantic-search",
            "vector-search",
            "nlp",
            "neural-search",
            "retrieval",
            "flashtext",
            "searching"
        ]
    },
    "https://github.com/GokuMohandas/Made-With-ML": {
        "extra-tags": [],
        "date": "2018-11-05",
        "title": "Made-With-ML",
        "summary": "Learn how to responsibly develop, deploy and maintain production machine learning applications.",
        "tags": [
            "natural-language-processing",
            "python",
            "machine-learning",
            "mlops",
            "jupyter notebook",
            "data-science",
            "data-engineering",
            "deep-learning",
            "pytorch"
        ]
    },
    "https://github.com/dbt-labs/dbt-utils": {
        "extra-tags": [],
        "date": "2017-07-16",
        "title": "dbt-utils",
        "summary": "Utility functions for dbt projects.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/LukeSmithxyz/based.cooking": {
        "extra-tags": [
            "simple"
        ],
        "date": "2021-03-10",
        "title": "based.cooking",
        "summary": "A simple culinary website.",
        "tags": [
            "css"
        ]
    },
    "https://github.com/linkedin/lambda-learner": {
        "extra-tags": [],
        "date": "2020-10-29",
        "title": "lambda-learner",
        "summary": "Lambda Learner is a library for iterative incremental training of a class of supervised machine learning models.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/grantjenks/python-diskcache": {
        "extra-tags": [],
        "date": "2016-02-03",
        "title": "python-diskcache",
        "summary": "Python disk-backed cache (Django-compatible). Faster than Redis and Memcached. Pure-Python.",
        "tags": [
            "python",
            "key-value-store",
            "persistence",
            "filesystem",
            "cache"
        ]
    },
    "https://github.com/nschloe/deadlink": {
        "extra-tags": [
            "code",
            "documentation"
        ],
        "date": "2021-04-20",
        "title": "deadlink",
        "summary": ":skull: Checks and fixes URLs in code and documentation.",
        "tags": [
            "python",
            "url",
            "tool",
            "command-line"
        ]
    },
    "https://github.com/cair/deep-rts": {
        "extra-tags": [],
        "date": "2017-03-05",
        "title": "deep-rts",
        "summary": "A Real-Time-Strategy game for Deep Learning research",
        "tags": [
            "python",
            "reinforcement-learning",
            "game",
            "neural-networks",
            "machine-learning",
            "deep-reinforcement-learning",
            "ai",
            "tree-search",
            "deep-learning",
            "cpp",
            "c++",
            "per-arne",
            "artificial-intelligence"
        ]
    },
    "https://github.com/tldraw/tldraw": {
        "extra-tags": [],
        "date": "2021-05-09",
        "title": "tldraw",
        "summary": "A tiny little drawing app.",
        "tags": [
            "drawing",
            "sketch",
            "code",
            "svg",
            "typescript",
            "fun",
            "whiteboard"
        ]
    },
    "https://github.com/gagyibenedek/ReDoS-checker": {
        "extra-tags": [
            "regex"
        ],
        "date": "2018-05-18",
        "title": "ReDoS-checker",
        "summary": "Check your regex for ReDoS vulnerability.",
        "tags": [
            "javascript"
        ]
    },
    "https://github.com/eserie/wax-ml": {
        "extra-tags": [],
        "date": "2021-05-24",
        "title": "wax-ml",
        "summary": "A Python library for machine-learning and feedback loops on streaming data",
        "tags": [
            "xarray",
            "python",
            "reinforcement-learning",
            "jax",
            "machine-learning",
            "time-series",
            "pandas",
            "data-streaming"
        ]
    },
    "https://github.com/bevacqua/dragula": {
        "extra-tags": [
            "simple"
        ],
        "date": "2015-04-13",
        "title": "dragula",
        "summary": ":ok_hand: Drag and drop so simple it hurts",
        "tags": [
            "dragging",
            "javascript",
            "drag-and-drop",
            "vanilla",
            "drag-drop",
            "front-end",
            "component"
        ]
    },
    "https://github.com/axa-group/Parsr": {
        "extra-tags": [],
        "date": "2019-08-05",
        "title": "Parsr",
        "summary": "Transforms PDF, Documents and Images into Enriched Structured Data",
        "tags": [
            "document",
            "images",
            "python",
            "javascript",
            "parsr",
            "extraction",
            "ocr",
            "data",
            "hacktoberfest",
            "typescript",
            "nlp",
            "pdf"
        ]
    },
    "https://github.com/cloudpipe/cloudpickle": {
        "extra-tags": [],
        "date": "2015-04-13",
        "title": "cloudpickle",
        "summary": "Extended pickling support for Python objects",
        "tags": [
            "python"
        ]
    },
    "https://github.com/koaning/pytest-duration-insights": {
        "extra-tags": [],
        "date": "2021-02-27",
        "title": "pytest-duration-insights",
        "summary": "A mini dashboard to help find slow tests in pytest.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/arthurflor23/spelling-correction": {
        "extra-tags": [],
        "date": "2019-08-24",
        "title": "spelling-correction",
        "summary": "Spelling correction using TensorFlow 2.x",
        "tags": [
            "python"
        ]
    },
    "https://github.com/bentoml/BentoML": {
        "extra-tags": [],
        "date": "2019-04-02",
        "title": "BentoML",
        "summary": "Unified Model Serving Framework \ud83c\udf71",
        "tags": [
            "kubernetes",
            "machine-learning",
            "ml-infrastructure",
            "ml-platform",
            "aws-sagemaker",
            "inference-server",
            "azure-ml",
            "mlops",
            "tensorflow",
            "python",
            "model-deployment",
            "prediction-service",
            "ai",
            "bentoml",
            "deep-learning",
            "model-serving",
            "aws-lambda",
            "machine-learning-operations",
            "pytorch",
            "ml",
            "model-management"
        ]
    },
    "https://github.com/fairlearn/fairlearn": {
        "extra-tags": [],
        "date": "2018-05-15",
        "title": "fairlearn",
        "summary": "A Python package to assess and improve fairness of machine learning models.",
        "tags": [
            "artificial-intelligence",
            "unfairness-mitigation",
            "python",
            "responsible-ai",
            "fairness-ai",
            "fairness-ml",
            "machine-learning",
            "fairness-assessment",
            "ai",
            "harms",
            "group-fairness",
            "ai-systems",
            "fairness"
        ]
    },
    "https://github.com/benthosdev/benthos": {
        "extra-tags": [
            "stream",
            "processing"
        ],
        "date": "2016-03-22",
        "title": "benthos",
        "summary": "Fancy stream processing made operationally mundane",
        "tags": [
            "message-bus",
            "go",
            "amqp",
            "logs",
            "nats",
            "streaming-data",
            "rabbitmq",
            "event-sourcing",
            "cqrs",
            "data-engineering",
            "golang",
            "data-ops",
            "kafka",
            "message-queue",
            "stream-processor",
            "stream-processing",
            "etl"
        ]
    },
    "https://github.com/wbkd/react-flow": {
        "extra-tags": [],
        "date": "2019-07-15",
        "title": "react-flow",
        "summary": "Highly customizable library for building an interactive node-based UI, workflow editor, flow chart or static diagram ",
        "tags": [
            "typescript-library",
            "node-based-ui",
            "react",
            "flowchart",
            "graph",
            "typescript",
            "react-library",
            "workflow"
        ]
    },
    "https://github.com/woodgern/confusables": {
        "extra-tags": [],
        "date": "2019-02-10",
        "title": "confusables",
        "summary": "A python package providing functionality for matching words using different characters but appearing to be a similar/the same word.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/imba/imba": {
        "extra-tags": [
            "language"
        ],
        "date": "2014-06-14",
        "title": "imba",
        "summary": "\ud83d\udc24 The friendly full-stack language",
        "tags": [
            "ui",
            "imba",
            "javascript",
            "framework",
            "frontend",
            "declarative",
            "dom",
            "programming-language"
        ]
    },
    "https://github.com/go-task/task": {
        "extra-tags": [],
        "date": "2017-02-27",
        "title": "task",
        "summary": "A task runner / simpler Make alternative written in Go",
        "tags": [
            "task",
            "go",
            "devops",
            "taskfile",
            "make",
            "task-runner",
            "makefile",
            "build-tool"
        ]
    },
    "https://github.com/casact/chainladder-python": {
        "extra-tags": [],
        "date": "2017-06-14",
        "title": "chainladder-python",
        "summary": "Actuarial reserving in Python",
        "tags": [
            "actuarial",
            "python",
            "chainladder",
            "reserving",
            "scikit-learn",
            "pandas",
            "estimators",
            "actuary"
        ]
    },
    "https://github.com/qurator-spk/dinglehopper": {
        "extra-tags": [],
        "date": "2019-08-14",
        "title": "dinglehopper",
        "summary": "An OCR evaluation tool",
        "tags": [
            "alto",
            "python",
            "page-xml",
            "qurator",
            "ocr",
            "page",
            "ocr-evaluation",
            "alto-xml",
            "ocr-d"
        ]
    },
    "https://github.com/meteofrance/meteonet": {
        "extra-tags": [
            "toolbox",
            "documentation"
        ],
        "date": "2020-01-28",
        "title": "meteonet",
        "summary": "MeteoNet's toolbox and documentation",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/robhagemans/hoard-of-bitfonts": {
        "extra-tags": [],
        "date": "2019-04-13",
        "title": "hoard-of-bitfonts",
        "summary": "turns out I like bitmap fonts",
        "tags": [
            "ascii-art",
            "pascal",
            "bitmap-fonts",
            "fonts",
            "bitmap-font",
            "font",
            "retrocomputing",
            "retro",
            "8bit",
            "retrogaming"
        ]
    },
    "https://github.com/jamesturk/jellyfish": {
        "extra-tags": [],
        "date": "2010-07-09",
        "title": "jellyfish",
        "summary": "\ud83e\udebc a python library for doing approximate and phonetic matching of strings.",
        "tags": [
            "fuzzy-search",
            "python",
            "jaro-winkler",
            "levenshtein",
            "hacktoberfest",
            "metaphone",
            "soundex",
            "hamming"
        ]
    },
    "https://github.com/concrete-utopia/utopia": {
        "extra-tags": [],
        "date": "2020-05-27",
        "title": "utopia",
        "summary": "Design \u2764\ufe0f Code",
        "tags": [
            "utopia",
            "now",
            "typescript",
            "future"
        ]
    },
    "https://github.com/shon/httpagentparser": {
        "extra-tags": [],
        "date": "2011-03-11",
        "title": "httpagentparser",
        "summary": "Python HTTP Agent Parser",
        "tags": [
            "python"
        ]
    },
    "https://github.com/sql-formatter-org/sql-formatter": {
        "extra-tags": [],
        "date": "2016-09-12",
        "title": "sql-formatter",
        "summary": "A whitespace formatter for different query languages",
        "tags": [
            "n1ql",
            "sql",
            "formatter",
            "javascript",
            "typescript"
        ]
    },
    "https://github.com/dgasmith/opt_einsum": {
        "extra-tags": [],
        "date": "2014-12-12",
        "title": "opt_einsum",
        "summary": "\u26a1\ufe0fOptimizing einsum functions in NumPy, Tensorflow, Dask, and more with contraction order optimization.",
        "tags": [
            "tensor",
            "python",
            "tensor-contraction",
            "contraction",
            "einsum",
            "gpu-acceleration",
            "performance"
        ]
    },
    "https://github.com/JohannesBuchner/imagehash": {
        "extra-tags": [],
        "date": "2013-03-02",
        "title": "imagehash",
        "summary": "A Python Perceptual Image Hashing Module",
        "tags": [
            "python",
            "image-hashing",
            "image-hashing-algorithms"
        ]
    },
    "https://github.com/Samir55/Image2Lines": {
        "extra-tags": [],
        "date": "2017-10-22",
        "title": "Image2Lines",
        "summary": "A tool for handwritten text (straight and skewed) line segmentation based on a statistical approach.",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/tvst/st-annotated-text": {
        "extra-tags": [],
        "date": "2020-08-02",
        "title": "st-annotated-text",
        "summary": "A simple component to display annotated text in Streamlit apps.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/Layout-Parser/layout-parser": {
        "extra-tags": [],
        "date": "2020-06-10",
        "title": "layout-parser",
        "summary": "A Unified Toolkit for Deep Learning Based Document Image Analysis",
        "tags": [
            "python",
            "document-layout-analysis",
            "layout-detection",
            "ocr",
            "layout-analysis",
            "detectron2",
            "deep-learning",
            "layout-parser",
            "computer-vision",
            "document-image-processing",
            "object-detection"
        ]
    },
    "https://github.com/cleanlab/cleanlab": {
        "extra-tags": [],
        "date": "2018-05-11",
        "title": "cleanlab",
        "summary": "The standard data-centric AI package for data quality and machine learning with messy, real-world data and labels.",
        "tags": [
            "entity-recognition",
            "active-learning",
            "machine-learning",
            "out-of-distribution-detection",
            "noisy-labels",
            "image-tagging",
            "classification",
            "annotations",
            "python",
            "data-quality",
            "outlier-detection",
            "data-labeling",
            "weak-supervision",
            "data-centric-ai",
            "data-science",
            "exploratory-data-analysis",
            "label-errors",
            "data-cleaning",
            "data-validation",
            "robust-machine-learning",
            "crowdsourcing"
        ]
    },
    "https://github.com/abersheeran/cool": {
        "extra-tags": [],
        "date": "2021-01-31",
        "title": "cool",
        "summary": "Make Python code cooler. Less is more.",
        "tags": [
            "pipeline",
            "redirect",
            "python"
        ]
    },
    "https://github.com/sharkdp/hyperfine": {
        "extra-tags": [],
        "date": "2018-01-13",
        "title": "hyperfine",
        "summary": "A command-line benchmarking tool",
        "tags": [
            "command-line",
            "benchmark",
            "cli",
            "rust",
            "terminal",
            "hacktoberfest",
            "tool"
        ]
    },
    "https://github.com/pemistahl/grex": {
        "extra-tags": [],
        "date": "2019-10-05",
        "title": "grex",
        "summary": "A command-line tool and Rust library for generating regular expressions from user-provided test cases",
        "tags": [
            "rust-cli",
            "rust-crate",
            "terminal",
            "cli",
            "rust",
            "regular-expressions",
            "regex",
            "command-line-tool",
            "regular-expression",
            "regex-pattern",
            "rust-library",
            "tool",
            "regexp"
        ]
    },
    "https://github.com/nalgeon/sqlean": {
        "extra-tags": [
            "set"
        ],
        "date": "2021-02-28",
        "title": "sqlean",
        "summary": "The ultimate set of SQLite extensions",
        "tags": [
            "sqlite-extension",
            "sqlite",
            "c"
        ]
    },
    "https://github.com/exiftool/exiftool": {
        "extra-tags": [
            "reader"
        ],
        "date": "2018-05-09",
        "title": "exiftool",
        "summary": "ExifTool meta information reader/writer",
        "tags": [
            "metadata",
            "iptc",
            "xmp",
            "perl",
            "cli",
            "exif",
            "api",
            "image-metadata"
        ]
    },
    "https://github.com/lidatong/dataclasses-json": {
        "extra-tags": [],
        "date": "2018-04-21",
        "title": "dataclasses-json",
        "summary": "Easily serialize Data Classes to and from JSON",
        "tags": [
            "json",
            "dataclasses",
            "python"
        ]
    },
    "https://github.com/jaraco/keyring": {
        "extra-tags": [],
        "date": "2015-02-24",
        "title": "keyring",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/textstat/textstat": {
        "extra-tags": [],
        "date": "2014-06-18",
        "title": "textstat",
        "summary": ":memo: python package to calculate readability statistics of a text object - paragraphs, sentences, articles.",
        "tags": [
            "smog",
            "python",
            "readability",
            "flesch-reading-ease",
            "textstat",
            "flesch-kincaid-grade"
        ]
    },
    "https://github.com/WICG/floc": {
        "extra-tags": [],
        "date": "2019-08-22",
        "title": "floc",
        "summary": "FLoC",
        "tags": [
            "makefile"
        ]
    },
    "https://github.com/gitpython-developers/GitPython": {
        "extra-tags": [],
        "date": "2010-11-30",
        "title": "GitPython",
        "summary": "GitPython is a python library used to interact with Git repositories.",
        "tags": [
            "python",
            "git-plumbing",
            "git-porcelain",
            "python-library"
        ]
    },
    "https://github.com/ohler55/ojg": {
        "extra-tags": [],
        "date": "2020-04-12",
        "title": "ojg",
        "summary": "Optimized JSON for Go",
        "tags": [
            "go",
            "parser",
            "jsonpath",
            "golang",
            "fast",
            "json"
        ]
    },
    "https://github.com/ManimCommunity/manim": {
        "extra-tags": [],
        "date": "2020-05-19",
        "title": "manim",
        "summary": "A community-maintained Python framework for creating mathematical animations. ",
        "tags": [
            "math",
            "python",
            "animations",
            "hacktoberfest",
            "manim"
        ]
    },
    "https://github.com/benfred/py-spy": {
        "extra-tags": [],
        "date": "2018-08-01",
        "title": "py-spy",
        "summary": "Sampling profiler for Python programs",
        "tags": [
            "performance-analysis",
            "python",
            "rust",
            "profiling",
            "profiler"
        ]
    },
    "https://github.com/nostalgic-css/NES.css": {
        "extra-tags": [],
        "date": "2018-09-24",
        "title": "NES.css",
        "summary": "NES-style CSS Framework | \u30d5\u30a1\u30df\u30b3\u30f3\u98a8CSS\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af",
        "tags": [
            "css",
            "css-framework",
            "nes",
            "scss",
            "8bit"
        ]
    },
    "https://github.com/jeffreystarr/dateinfer": {
        "extra-tags": [],
        "date": "2014-01-12",
        "title": "dateinfer",
        "summary": "Python library to infer date format from examples",
        "tags": [
            "python"
        ]
    },
    "https://github.com/TimelyDataflow/differential-dataflow": {
        "extra-tags": [
            "dataflow"
        ],
        "date": "2015-05-05",
        "title": "differential-dataflow",
        "summary": "An implementation of differential dataflow using timely dataflow on Rust.",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/agermanidis/pigeon": {
        "extra-tags": [],
        "date": "2017-09-05",
        "title": "pigeon",
        "summary": "\ud83d\udc26 Quickly annotate data from the comfort of your Jupyter notebook",
        "tags": [
            "python"
        ]
    },
    "https://github.com/lmatteis/torrent-net": {
        "extra-tags": [],
        "date": "2017-04-14",
        "title": "torrent-net",
        "summary": "Distributed search engines using BitTorrent and SQLite",
        "tags": [
            "c"
        ]
    },
    "https://github.com/excalidraw/excalidraw": {
        "extra-tags": [],
        "date": "2020-01-02",
        "title": "excalidraw",
        "summary": "Virtual whiteboard for sketching hand-drawn like diagrams",
        "tags": [
            "productivity",
            "diagrams",
            "drawing",
            "hacktoberfest",
            "typescript",
            "collaboration",
            "whiteboard"
        ]
    },
    "https://github.com/zeek/zeek": {
        "extra-tags": [],
        "date": "2012-07-06",
        "title": "zeek",
        "summary": "Zeek is a powerful network analysis framework that is much different from the typical IDS you may know.",
        "tags": [
            "nsm",
            "zeek",
            "dfir",
            "network-monitoring",
            "bro",
            "security",
            "c++",
            "pcap"
        ]
    },
    "https://github.com/dolthub/dolt": {
        "extra-tags": [
            "data"
        ],
        "date": "2019-07-24",
        "title": "dolt",
        "summary": "Dolt \u2013 Git for Data",
        "tags": [
            "data-versioning",
            "git-for-databases",
            "git-database",
            "go",
            "command-line",
            "decentralized-database",
            "sql",
            "git",
            "git-sql",
            "version-controlled-database",
            "git-for-data",
            "golang",
            "database",
            "data-version-control",
            "database-version-control",
            "database-versioning",
            "mysql",
            "immutable-database"
        ]
    },
    "https://github.com/pdfminer/pdfminer.six": {
        "extra-tags": [],
        "date": "2014-08-29",
        "title": "pdfminer.six",
        "summary": "Community maintained fork of pdfminer - we fathom PDF",
        "tags": [
            "pdf",
            "python",
            "parser"
        ]
    },
    "https://github.com/facebook/duckling": {
        "extra-tags": [
            "testing",
            "language"
        ],
        "date": "2017-03-02",
        "title": "duckling",
        "summary": "Language, engine, and tooling for expressing, testing, and evaluating composable language rules on input strings.",
        "tags": [
            "haskell"
        ]
    },
    "https://github.com/nuno-faria/tiler": {
        "extra-tags": [
            "build",
            "images"
        ],
        "date": "2019-09-07",
        "title": "tiler",
        "summary": "\ud83d\udc77 Build images with images",
        "tags": [
            "image-builder",
            "cross-stitch",
            "python",
            "opencv",
            "image-processing",
            "minecraft",
            "mosaic-images",
            "tiling",
            "lego"
        ]
    },
    "https://github.com/drivendataorg/deon": {
        "extra-tags": [],
        "date": "2018-08-08",
        "title": "deon",
        "summary": "A command line tool to easily add an ethics checklist to your data science projects.",
        "tags": [
            "python",
            "machine-learning",
            "data-ethics",
            "data-science",
            "ethics"
        ]
    },
    "https://github.com/zhm-real/PathPlanning": {
        "extra-tags": [
            "algorithms",
            "animations"
        ],
        "date": "2020-06-16",
        "title": "PathPlanning",
        "summary": "Common used path planning algorithms with animations.",
        "tags": [
            "rrt",
            "lifelong-planning-astar",
            "anytime-dstar",
            "informed-rrt-star",
            "realtime-adaptive-astar",
            "batch-informed-trees",
            "fast-marching-trees",
            "path-planning",
            "dstar",
            "python",
            "rrt-star-smart",
            "rrt-connect",
            "rrt-star",
            "extended-rrt",
            "dynamic-rrt",
            "astar",
            "anytime-repairing-astar",
            "learning-realtime-astar",
            "dstar-lite"
        ]
    },
    "https://github.com/tidymodels/infer": {
        "extra-tags": [],
        "date": "2017-06-05",
        "title": "infer",
        "summary": "An R package for tidyverse-friendly statistical inference",
        "tags": [
            "r"
        ]
    },
    "https://github.com/hgrecco/pint-pandas": {
        "extra-tags": [
            "pandas"
        ],
        "date": "2018-12-24",
        "title": "pint-pandas",
        "summary": "Pandas support for pint",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/thegeeklab/hugo-geekdoc": {
        "extra-tags": [],
        "date": "2019-12-31",
        "title": "hugo-geekdoc",
        "summary": "Hugo theme made for documentation",
        "tags": [
            "html",
            "hugo-theme",
            "hugo",
            "theme",
            "documentation"
        ]
    },
    "https://github.com/soypat/go-presentx": {
        "extra-tags": [],
        "date": "2020-11-07",
        "title": "go-presentx",
        "summary": "golang's present tool but with code syntax highlighting",
        "tags": [
            "presentation",
            "go-present",
            "css",
            "syntax-highlighting",
            "golang",
            "interactive"
        ]
    },
    "https://github.com/raphaelvallat/pingouin": {
        "extra-tags": [],
        "date": "2018-04-01",
        "title": "pingouin",
        "summary": "Statistical package in Python based on Pandas",
        "tags": [
            "circular-statistics",
            "python",
            "ttest",
            "statistical-methods",
            "statistical-tests",
            "correlations",
            "multiple-comparisons",
            "anova",
            "statistics",
            "bayesian-statistics",
            "pandas",
            "cohens-d",
            "effect-size"
        ]
    },
    "https://github.com/nteract/papermill": {
        "extra-tags": [],
        "date": "2017-07-06",
        "title": "papermill",
        "summary": "\ud83d\udcda Parameterize, execute, and analyze notebooks",
        "tags": [
            "scala",
            "python",
            "julia",
            "notebooks",
            "notebook-generator",
            "r",
            "jupyter",
            "notebook",
            "pipeline",
            "publishing",
            "nteract"
        ]
    },
    "https://github.com/deepcharles/ruptures": {
        "extra-tags": [
            "detection"
        ],
        "date": "2018-01-20",
        "title": "ruptures",
        "summary": "ruptures: change point detection in Python",
        "tags": [
            "changepoint",
            "signal-processing",
            "python",
            "scientific-computing",
            "science",
            "change-point-detection"
        ]
    },
    "https://github.com/Gurobi/modeling-examples": {
        "extra-tags": [
            "examples"
        ],
        "date": "2019-02-05",
        "title": "modeling-examples",
        "summary": "Gurobi modeling examples",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/ramonhagenaars/nptyping": {
        "extra-tags": [],
        "date": "2019-02-05",
        "title": "nptyping",
        "summary": "\ud83d\udca1 Type hints for Numpy and Pandas",
        "tags": [
            "python3",
            "python",
            "numpy",
            "typehints",
            "pandas"
        ]
    },
    "https://github.com/rapidsai/cuml": {
        "extra-tags": [],
        "date": "2018-10-11",
        "title": "cuml",
        "summary": "cuML - RAPIDS Machine Learning Library",
        "tags": [
            "gpu",
            "cuda",
            "machine-learning",
            "nvidia",
            "machine-learning-algorithms",
            "c++"
        ]
    },
    "https://github.com/janestreet/incremental": {
        "extra-tags": [
            "library",
            "incremental-learning"
        ],
        "date": "2015-07-06",
        "title": "incremental",
        "summary": "A library for incremental computations",
        "tags": [
            "ocaml"
        ]
    },
    "https://github.com/raphaelsty/mkb": {
        "extra-tags": [],
        "date": "2020-04-06",
        "title": "mkb",
        "summary": "Knowledge Base Embedding By Cooperative Knowledge Distillation",
        "tags": [
            "mkb",
            "knowledge-graph-embeddings",
            "python",
            "distillation",
            "wn18",
            "embeddings",
            "machine-learning",
            "graph",
            "triplets",
            "knowledge",
            "pytorch",
            "knowledge-graph",
            "graph-embedding"
        ]
    },
    "https://github.com/parrt/tensor-sensor": {
        "extra-tags": [],
        "date": "2020-08-28",
        "title": "tensor-sensor",
        "summary": "The goal of this library is to generate more helpful exception messages for matrix algebra expressions for numpy, pytorch, jax, tensorflow, keras, fastai. ",
        "tags": [
            "debugging",
            "matrix",
            "python",
            "jax",
            "numpy",
            "tensorflow",
            "jupyter notebook",
            "pytorch",
            "deep-learning",
            "vector",
            "tracing"
        ]
    },
    "https://github.com/drivendataorg/cloudpathlib": {
        "extra-tags": [],
        "date": "2020-07-27",
        "title": "cloudpathlib",
        "summary": "Python pathlib-style classes for cloud storage services such as Amazon S3, Azure Blob Storage, and Google Cloud Storage.",
        "tags": [
            "azure-blob",
            "python",
            "google-cloud-storage",
            "s3",
            "pathlib",
            "cloud-storage"
        ]
    },
    "https://github.com/rq/rq": {
        "extra-tags": [],
        "date": "2011-11-14",
        "title": "rq",
        "summary": "Simple job queues for Python",
        "tags": [
            "async",
            "task",
            "python",
            "workers",
            "task-queue",
            "delayed-jobs",
            "redis",
            "background-jobs",
            "job-queue",
            "delayed-tasks",
            "rq"
        ]
    },
    "https://github.com/SauceCat/PDPbox": {
        "extra-tags": [],
        "date": "2017-06-26",
        "title": "PDPbox",
        "summary": "python partial dependence plot toolbox",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/google/python-fire": {
        "extra-tags": [
            "library"
        ],
        "date": "2017-02-21",
        "title": "python-fire",
        "summary": "Python Fire is a library for automatically generating command line interfaces (CLIs) from absolutely any Python object.",
        "tags": [
            "cli",
            "python"
        ]
    },
    "https://github.com/autogluon/autogluon": {
        "extra-tags": [],
        "date": "2019-07-29",
        "title": "autogluon",
        "summary": "AutoGluon: AutoML for Image, Text, Time Series, and Tabular Data",
        "tags": [
            "machine-learning",
            "tabular-data",
            "transfer-learning",
            "gluon",
            "forecasting",
            "time-series",
            "automl",
            "object-detection",
            "image-classification",
            "python",
            "scikit-learn",
            "autogluon",
            "data-science",
            "deep-learning",
            "hyperparameter-optimization",
            "ensemble-learning",
            "structured-data",
            "automated-machine-learning",
            "natural-language-processing",
            "pytorch",
            "computer-vision"
        ]
    },
    "https://github.com/snorkel-team/snorkel": {
        "extra-tags": [],
        "date": "2016-02-26",
        "title": "snorkel",
        "summary": "A system for quickly generating training data with weak supervision",
        "tags": [
            "training-data",
            "snorkel",
            "python",
            "data-augmentation",
            "weak-supervision",
            "machine-learning",
            "ai",
            "data-slicing",
            "data-science",
            "labeling"
        ]
    },
    "https://github.com/boyter/scc": {
        "extra-tags": [],
        "date": "2018-03-01",
        "title": "scc",
        "summary": "Sloc, Cloc and Code: scc is a very fast accurate code counter with complexity calculations and COCOMO estimates written in pure Go",
        "tags": [
            "windows",
            "go",
            "sloc",
            "cli",
            "tokei",
            "linux",
            "code",
            "cloc",
            "golang",
            "macos",
            "statistics",
            "sloccount",
            "complexity"
        ]
    },
    "https://github.com/hudson-and-thames/mlfinlab": {
        "extra-tags": [],
        "date": "2019-02-13",
        "title": "mlfinlab",
        "summary": "MlFinLab helps portfolio managers and traders who want to leverage the power of machine learning by providing reproducible, interpretable, and easy to use tools. ",
        "tags": [
            "portfolio-optimization",
            "python",
            "financial-machine-learning",
            "portfolio-management",
            "investing",
            "quantitative-finance",
            "research",
            "machine-learning",
            "trading",
            "algorithmic-trading",
            "finance"
        ]
    },
    "https://github.com/Knio/dominate": {
        "extra-tags": [],
        "date": "2009-05-14",
        "title": "dominate",
        "summary": "Dominate is a Python library for creating and manipulating HTML documents using an elegant DOM API.  It allows you to write HTML pages in pure Python very concisely, which eliminate the need to learn another template language, and to take advantage of the more powerful features of Python.",
        "tags": [
            "python",
            "html",
            "html-document",
            "html-element",
            "python-library"
        ]
    },
    "https://github.com/umami-software/umami": {
        "extra-tags": [],
        "date": "2020-07-17",
        "title": "umami",
        "summary": "Umami is a simple, fast, privacy-focused alternative to Google Analytics.",
        "tags": [
            "analytics",
            "javascript",
            "charts",
            "statistics",
            "web-analytics",
            "google-analytics"
        ]
    },
    "https://github.com/daft-dev/daft": {
        "extra-tags": [],
        "date": "2012-09-21",
        "title": "daft",
        "summary": "Render probabilistic graphical models using matplotlib",
        "tags": [
            "python"
        ]
    },
    "https://github.com/viebel/klipse": {
        "extra-tags": [],
        "date": "2015-11-19",
        "title": "klipse",
        "summary": "Klipse is a JavaScript plugin for embedding interactive code snippets in tech blogs.",
        "tags": [
            "clojurescript",
            "reactjs",
            "codemirror-editor",
            "scheme",
            "evaluation",
            "lua",
            "brainfuck",
            "html",
            "prolog",
            "ruby",
            "python",
            "javascript",
            "react",
            "common-lisp",
            "clojure",
            "reasonml",
            "code-evaluation",
            "klipse-plugin",
            "interactive-snippets",
            "ocaml"
        ]
    },
    "https://github.com/andreaskipf/learnedcardinalities": {
        "extra-tags": [],
        "date": "2018-12-14",
        "title": "learnedcardinalities",
        "summary": "Code and workloads from the Learned Cardinalities paper (https://arxiv.org/abs/1809.00677)",
        "tags": [
            "python"
        ]
    },
    "https://github.com/PavelDoGreat/WebGL-Fluid-Simulation": {
        "extra-tags": [],
        "date": "2017-08-22",
        "title": "WebGL-Fluid-Simulation",
        "summary": "Play with fluids in your browser (works even on mobile)",
        "tags": [
            "webgl",
            "gpu",
            "simulation",
            "javascript",
            "navier-stokes",
            "fluid"
        ]
    },
    "https://github.com/schollz/croc": {
        "extra-tags": [
            "package"
        ],
        "date": "2017-10-17",
        "title": "croc",
        "summary": "Easily and securely send things from one computer to another :crocodile: :package:",
        "tags": [
            "tcp",
            "go",
            "file-sharing",
            "data-transfer",
            "pake",
            "peer-to-peer",
            "golang",
            "transfer"
        ]
    },
    "https://github.com/sql-machine-learning/sqlflow": {
        "extra-tags": [],
        "date": "2018-10-04",
        "title": "sqlflow",
        "summary": "Brings SQL and AI together.",
        "tags": [
            "databases",
            "go",
            "sql-syntax",
            "sqlflow",
            "transpiler",
            "machine-learning",
            "ai",
            "deep-learning"
        ]
    },
    "https://github.com/airspeed-velocity/asv": {
        "extra-tags": [],
        "date": "2013-11-07",
        "title": "asv",
        "summary": "Airspeed Velocity: A simple Python benchmarking tool with web-based reporting",
        "tags": [
            "benchmark",
            "airspeed-velocity",
            "python"
        ]
    },
    "https://github.com/alirezamika/autoscraper": {
        "extra-tags": [],
        "date": "2020-08-31",
        "title": "autoscraper",
        "summary": "A Smart, Automatic, Fast and Lightweight Web Scraper for Python",
        "tags": [
            "python",
            "scraper",
            "web-scraping",
            "webscraping",
            "scraping",
            "machine-learning",
            "ai",
            "automation",
            "crawler",
            "webautomation",
            "scrape",
            "artificial-intelligence"
        ]
    },
    "https://github.com/Continvvm/continuum": {
        "extra-tags": [],
        "date": "2020-04-11",
        "title": "continuum",
        "summary": "A clean and simple data loading library for Continual Learning",
        "tags": [
            "online-learning",
            "lifelong-learning",
            "python",
            "incremental-learning",
            "dataset",
            "pytorch",
            "dataloader",
            "continual-learning"
        ]
    },
    "https://github.com/eradman/entr": {
        "extra-tags": [
            "files"
        ],
        "date": "2018-03-27",
        "title": "entr",
        "summary": "Run arbitrary commands when files change",
        "tags": [
            "test-automation",
            "inotify",
            "kqueue",
            "c"
        ]
    },
    "https://github.com/connorferster/handcalcs": {
        "extra-tags": [],
        "date": "2020-02-19",
        "title": "handcalcs",
        "summary": "Python library for converting Python calculations into rendered latex.",
        "tags": [
            "css"
        ]
    },
    "https://github.com/jimporter/mike": {
        "extra-tags": [],
        "date": "2017-09-24",
        "title": "mike",
        "summary": "Manage multiple versions of your MkDocs-powered documentation via Git",
        "tags": [
            "documentation-tool",
            "python",
            "github-pages",
            "git",
            "mkdocs"
        ]
    },
    "https://github.com/karpathy/minGPT": {
        "extra-tags": [],
        "date": "2020-08-17",
        "title": "minGPT",
        "summary": "A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training",
        "tags": [
            "python"
        ]
    },
    "https://github.com/Dhghomon/easy_rust": {
        "extra-tags": [
            "rust"
        ],
        "date": "2020-07-11",
        "title": "easy_rust",
        "summary": "Rust explained using easy English",
        "tags": [
            "shell"
        ]
    },
    "https://github.com/webdataset/webdataset": {
        "extra-tags": [],
        "date": "2019-08-07",
        "title": "webdataset",
        "summary": "A high-performance Python-based I/O system for large (and small) deep learning problems, with strong support for PyTorch.",
        "tags": [
            "python",
            "webdataset",
            "pytorch",
            "deep-learning",
            "data-augmentation",
            "webdataset-format"
        ]
    },
    "https://github.com/statsd/statsd": {
        "extra-tags": [],
        "date": "2010-12-30",
        "title": "statsd",
        "summary": "Daemon for easy but powerful stats aggregation",
        "tags": [
            "metrics",
            "javascript",
            "nodejs",
            "graphite",
            "statsd"
        ]
    },
    "https://github.com/joshday/OnlineStats.jl": {
        "extra-tags": [],
        "date": "2015-02-04",
        "title": "OnlineStats.jl",
        "summary": "\u26a1 Single-pass algorithms for statistics",
        "tags": [
            "julia",
            "streaming-data",
            "big-data",
            "stochastic-approximation",
            "online-algorithms",
            "statistics",
            "julia-language",
            "julialang",
            "onlinestats"
        ]
    },
    "https://github.com/TDAmeritrade/stumpy": {
        "extra-tags": [],
        "date": "2019-05-03",
        "title": "stumpy",
        "summary": "STUMPY is a powerful and scalable Python library for modern time series analysis",
        "tags": [
            "time-series-segmentation",
            "motif-discovery",
            "python",
            "pattern-matching",
            "anomaly-detection",
            "time-series-analysis",
            "data-science",
            "numba",
            "matrix-profile",
            "dask",
            "time-series-data-mining",
            "pydata"
        ]
    },
    "https://github.com/h2oai/datatable": {
        "extra-tags": [],
        "date": "2017-03-03",
        "title": "datatable",
        "summary": "A Python package for manipulating 2-dimensional tabular data structures",
        "tags": [
            "python",
            "data-structure",
            "data-analysis",
            "ftrl",
            "performance",
            "c++"
        ]
    },
    "https://github.com/josephreisinger/vowpal_porpoise": {
        "extra-tags": [],
        "date": "2012-04-15",
        "title": "vowpal_porpoise",
        "summary": "lightweight python wrapper for vowpal wabbit",
        "tags": [
            "python"
        ]
    },
    "https://github.com/360Controller/360Controller": {
        "extra-tags": [],
        "date": "2013-09-27",
        "title": "360Controller",
        "summary": "TattieBogle Xbox 360 Driver (with improvements)",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/tholman/elevator.js": {
        "extra-tags": [
            "top"
        ],
        "date": "2015-04-12",
        "title": "elevator.js",
        "summary": "Finally, a \"back to top\" button that behaves like a real elevator. ",
        "tags": [
            "javascript"
        ]
    },
    "https://github.com/allisonhorst/palmerpenguins": {
        "extra-tags": [],
        "date": "2020-06-05",
        "title": "palmerpenguins",
        "summary": "A great intro dataset for data exploration & visualization (alternative to iris).",
        "tags": [
            "r"
        ]
    },
    "https://github.com/jiffyclub/snakeviz": {
        "extra-tags": [],
        "date": "2012-06-26",
        "title": "snakeviz",
        "summary": "An in-browser Python profile viewer",
        "tags": [
            "python"
        ]
    },
    "https://github.com/JustGlowing/minisom": {
        "extra-tags": [
            "maps"
        ],
        "date": "2013-07-03",
        "title": "minisom",
        "summary": ":red_circle: MiniSom is a minimalistic implementation of the Self Organizing Maps",
        "tags": [
            "python",
            "outlier-detection",
            "neural-networks",
            "self-organizing-map",
            "som",
            "machine-learning",
            "clustering",
            "dimensionality-reduction",
            "vector-quantization",
            "manifold-learning",
            "unsupervised-learning"
        ]
    },
    "https://github.com/maxhumber/gif": {
        "extra-tags": [],
        "date": "2020-01-30",
        "title": "gif",
        "summary": "The matplotlib Animation Extension",
        "tags": [
            "python",
            "matplotlib",
            "pillow",
            "pil",
            "gif",
            "gif-animation"
        ]
    },
    "https://github.com/robinhood/faust": {
        "extra-tags": [],
        "date": "2017-03-08",
        "title": "faust",
        "summary": "Python Stream Processing",
        "tags": [
            "python",
            "kafka-streams",
            "asyncio",
            "distributed-systems",
            "kafka",
            "stream-processing"
        ]
    },
    "https://github.com/verdict-project/verdict": {
        "extra-tags": [],
        "date": "2014-11-21",
        "title": "verdict",
        "summary": "Interactive-Speed Analytics: 200x Faster, 200x Fewer Cluster Resources, Approximate Query Processing",
        "tags": [
            "java"
        ]
    },
    "https://github.com/hcho3/minimal-cython-cpp-example": {
        "extra-tags": [],
        "date": "2020-04-30",
        "title": "minimal-cython-cpp-example",
        "summary": "Minimal template for using C++ code from Python via Cython",
        "tags": [
            "python"
        ]
    },
    "https://github.com/onelearn/onelearn": {
        "extra-tags": [],
        "date": "2020-03-05",
        "title": "onelearn",
        "summary": "Online machine learning methods",
        "tags": [
            "random-forest",
            "python",
            "online-learning-algorithms",
            "machine-learning",
            "classification",
            "regression"
        ]
    },
    "https://github.com/karpathy/micrograd": {
        "extra-tags": [],
        "date": "2020-04-13",
        "title": "micrograd",
        "summary": "A tiny scalar-valued autograd engine and a neural net library on top of it with PyTorch-like API",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/twanvl/hearthstone-battlegrounds-simulator": {
        "extra-tags": [],
        "date": "2019-11-17",
        "title": "hearthstone-battlegrounds-simulator",
        "summary": "A simulator for battles in the Hearthstone Battlegrounds",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/python-streamz/streamz": {
        "extra-tags": [],
        "date": "2017-04-04",
        "title": "streamz",
        "summary": "Real-time stream processing for python",
        "tags": [
            "real-time",
            "async",
            "python",
            "streaming-data"
        ]
    },
    "https://github.com/herbie-fp/herbie": {
        "extra-tags": [],
        "date": "2013-10-18",
        "title": "herbie",
        "summary": "Optimize floating-point expressions for accuracy",
        "tags": [
            "numerical-methods",
            "html",
            "synthesis",
            "developer-tools",
            "floating-point",
            "racket",
            "herbie"
        ]
    },
    "https://github.com/GBDT-PL/GBDT-PL": {
        "extra-tags": [
            "gbdt",
            "gradient"
        ],
        "date": "2018-02-09",
        "title": "GBDT-PL",
        "summary": "Gradient Boosting With Piece-Wise Linear Trees",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/sedthh/pyxelate": {
        "extra-tags": [],
        "date": "2020-01-10",
        "title": "pyxelate",
        "summary": "Python class that generates pixel art from images",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/ucbrise/clipper": {
        "extra-tags": [],
        "date": "2016-10-27",
        "title": "clipper",
        "summary": "A low-latency prediction-serving system",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/jlfwong/speedscope": {
        "extra-tags": [],
        "date": "2017-11-22",
        "title": "speedscope",
        "summary": "\ud83d\udd2c A fast, interactive web-based viewer for performance profiles.",
        "tags": [
            "speedscope",
            "performance-tools",
            "performance-profiling",
            "flamegraphs",
            "webgl",
            "performance-visualization",
            "profile",
            "typescript",
            "flamegraph"
        ]
    },
    "https://github.com/dalibo/pev2": {
        "extra-tags": [],
        "date": "2019-08-05",
        "title": "pev2",
        "summary": "Postgres Explain Visualizer 2",
        "tags": [
            "postgresql",
            "explain",
            "typescript"
        ]
    },
    "https://github.com/ctgk/PRML": {
        "extra-tags": [],
        "date": "2017-02-05",
        "title": "PRML",
        "summary": "PRML algorithms implemented in Python",
        "tags": [
            "python",
            "jupyter",
            "notebook",
            "jupyter notebook",
            "prml"
        ]
    },
    "https://github.com/processing/p5.js": {
        "extra-tags": [],
        "date": "2013-02-26",
        "title": "p5.js",
        "summary": "p5.js is a client-side JS platform that empowers artists, designers, students, and anyone to learn to code and express themselves creatively on the web. It is based on the core principles of Processing. http://twitter.com/p5xjs \u2014",
        "tags": [
            "education",
            "design",
            "creative-coding",
            "html",
            "graphics",
            "javascript",
            "learning",
            "processing",
            "sound",
            "p5js",
            "art"
        ]
    },
    "https://github.com/pythonprofilers/memory_profiler": {
        "extra-tags": [],
        "date": "2011-10-14",
        "title": "memory_profiler",
        "summary": "Monitor Memory usage of Python code",
        "tags": [
            "python"
        ]
    },
    "https://github.com/mml-book/mml-book.github.io": {
        "extra-tags": [],
        "date": "2017-12-08",
        "title": "mml-book.github.io",
        "summary": "Companion webpage to the book \"Mathematics For Machine Learning\"",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/timescale/timescaledb": {
        "extra-tags": [],
        "date": "2017-03-07",
        "title": "timescaledb",
        "summary": "An open-source time-series SQL database optimized for fast ingest and complex queries.  Packaged as a PostgreSQL extension.",
        "tags": [
            "postgres",
            "timescaledb",
            "time-series-database",
            "analytics",
            "sql",
            "time-series",
            "postgresql",
            "financial-analysis",
            "tsdb",
            "database",
            "iot",
            "c"
        ]
    },
    "https://github.com/ClickHouse/ClickHouse": {
        "extra-tags": [],
        "date": "2016-06-02",
        "title": "ClickHouse",
        "summary": "ClickHouse\u00ae is a free analytics DBMS for big data",
        "tags": [
            "dbms",
            "mpp",
            "analytics",
            "sql",
            "clickhouse",
            "big-data",
            "hacktoberfest",
            "olap",
            "distributed-database",
            "c++"
        ]
    },
    "https://github.com/pthom/northwind_psql": {
        "extra-tags": [
            "database",
            "postgres"
        ],
        "date": "2015-04-12",
        "title": "northwind_psql",
        "summary": "Northwind sample database for postgres",
        "tags": []
    },
    "https://github.com/huginn/huginn": {
        "extra-tags": [
            "agents"
        ],
        "date": "2013-03-10",
        "title": "huginn",
        "summary": "Create agents that monitor and act on your behalf.  Your agents are standing by!",
        "tags": [
            "notifications",
            "twitter",
            "agent",
            "feedgenerator",
            "scraper",
            "webscraping",
            "rss",
            "twitter-streaming",
            "automation",
            "ruby",
            "monitoring",
            "huginn",
            "feed"
        ]
    },
    "https://github.com/fivethirtyeight/data": {
        "extra-tags": [],
        "date": "2014-03-17",
        "title": "data",
        "summary": "Data and code behind the articles and graphics at FiveThirtyEight",
        "tags": [
            "jupyter notebook",
            "data"
        ]
    },
    "https://github.com/HypothesisWorks/hypothesis": {
        "extra-tags": [
            "library"
        ],
        "date": "2013-03-10",
        "title": "hypothesis",
        "summary": "Hypothesis is a powerful, flexible, and easy to use library for property-based testing.",
        "tags": [
            "testing",
            "property-based-testing",
            "python",
            "fuzzing"
        ]
    },
    "https://github.com/pomber/code-surfer": {
        "extra-tags": [],
        "date": "2018-08-19",
        "title": "code-surfer",
        "summary": "Rad code slides <\ud83c\udfc4/>",
        "tags": [
            "deck",
            "keynote",
            "slides",
            "mdx",
            "presentation",
            "syntax",
            "javascript",
            "react",
            "markdown",
            "code",
            "syntax-highlighting",
            "mdx-deck"
        ]
    },
    "https://github.com/imageio/imageio": {
        "extra-tags": [],
        "date": "2013-05-04",
        "title": "imageio",
        "summary": "Python library for reading and writing image data",
        "tags": [
            "dicom",
            "python",
            "animated-gif",
            "video",
            "scientific-formats",
            "imageio",
            "webcam-capture"
        ]
    },
    "https://github.com/lmcinnes/umap": {
        "extra-tags": [],
        "date": "2017-07-02",
        "title": "umap",
        "summary": "Uniform Manifold Approximation and Projection",
        "tags": [
            "python",
            "machine-learning",
            "umap",
            "dimensionality-reduction",
            "topological-data-analysis",
            "visualization"
        ]
    },
    "https://github.com/nicolargo/glances": {
        "extra-tags": [],
        "date": "2011-12-04",
        "title": "glances",
        "summary": "Glances an Eye on your system. A top/htop alternative for GNU/Linux, BSD, Mac OS and Windows operating systems.",
        "tags": [
            "system",
            "python",
            "terminal",
            "restful-api",
            "web",
            "restful",
            "monitoring",
            "multi-platform"
        ]
    },
    "https://github.com/alkaline-ml/pmdarima": {
        "extra-tags": [],
        "date": "2017-03-30",
        "title": "pmdarima",
        "summary": "A statistical library designed to fill the void in Python's time series analysis capabilities, including the equivalent of R's auto.arima function.",
        "tags": [
            "forecasting-models",
            "python",
            "econometrics",
            "forecasting",
            "sarimax",
            "machine-learning",
            "time-series",
            "arima",
            "pmdarima"
        ]
    },
    "https://github.com/eyaltrabelsi/pandas-log": {
        "extra-tags": [],
        "date": "2019-09-18",
        "title": "pandas-log",
        "summary": "The goal of pandas-log is to provide feedback about basic pandas operations. It provides simple wrapper functions for the most common functions that add additional logs",
        "tags": [
            "python"
        ]
    },
    "https://github.com/PostgREST/postgrest": {
        "extra-tags": [],
        "date": "2014-06-13",
        "title": "postgrest",
        "summary": "REST API for any Postgres database",
        "tags": [
            "postgres",
            "automatic-api",
            "sql",
            "postgrest",
            "pg",
            "rest",
            "server",
            "postgresql",
            "database",
            "haskell",
            "http",
            "api",
            "pgsql"
        ]
    },
    "https://github.com/mgartner/pg_flame": {
        "extra-tags": [],
        "date": "2019-10-16",
        "title": "pg_flame",
        "summary": "A flamegraph generator for Postgres EXPLAIN ANALYZE output.",
        "tags": [
            "postgres",
            "go",
            "performance-visualization",
            "postgresql-tool",
            "postgresql",
            "flamegraph",
            "database",
            "performance"
        ]
    },
    "https://github.com/pypa/cibuildwheel": {
        "extra-tags": [],
        "date": "2017-03-19",
        "title": "cibuildwheel",
        "summary": "\ud83c\udfa1 Build Python wheels for all the platforms on CI with minimal configuration. ",
        "tags": [
            "ci",
            "python",
            "python-wheels",
            "build-automation",
            "appveyor",
            "circleci",
            "azure-pipelines",
            "travis-ci",
            "pypi",
            "wheel",
            "github-actions"
        ]
    },
    "https://github.com/stanfordmlgroup/ngboost": {
        "extra-tags": [],
        "date": "2018-06-21",
        "title": "ngboost",
        "summary": "Natural Gradient Boosting for Probabilistic Prediction",
        "tags": [
            "gradient-boosting",
            "uncertainty-estimation",
            "python",
            "ngboost",
            "machine-learning",
            "natural-gradients"
        ]
    },
    "https://github.com/iterative/dvc": {
        "extra-tags": [],
        "date": "2017-03-04",
        "title": "dvc",
        "summary": "\ud83e\udd89Data Version Control | Git for Data & Models | ML Experiments Management",
        "tags": [
            "python",
            "reproducibility",
            "machine-learning",
            "ai",
            "hacktoberfest",
            "developer-tools",
            "collaboration",
            "data-science",
            "data-version-control",
            "git"
        ]
    },
    "https://github.com/PAIR-code/facets": {
        "extra-tags": [],
        "date": "2017-07-07",
        "title": "facets",
        "summary": "Visualizations for machine learning datasets",
        "tags": [
            "jupyter notebook",
            "data-visualization",
            "machine-learning"
        ]
    },
    "https://github.com/idealo/imagededup": {
        "extra-tags": [
            "images"
        ],
        "date": "2019-04-05",
        "title": "imagededup",
        "summary": "\ud83d\ude0e Finding duplicate images made easy!",
        "tags": [
            "neural-network",
            "python",
            "image-deduplication",
            "e-commerce",
            "pytorch",
            "hashing",
            "computer-vision",
            "idealo",
            "image-preprocessing"
        ]
    },
    "https://github.com/streamlit/streamlit": {
        "extra-tags": [],
        "date": "2019-08-24",
        "title": "streamlit",
        "summary": "Streamlit \u2014 The fastest way to build data apps in Python",
        "tags": [
            "python",
            "machine-learning",
            "data-analysis",
            "streamlit",
            "developer-tools",
            "data-visualization",
            "data-science",
            "deep-learning"
        ]
    },
    "https://github.com/albumentations-team/albumentations": {
        "extra-tags": [],
        "date": "2018-06-06",
        "title": "albumentations",
        "summary": "Fast image augmentation library and an easy-to-use wrapper around other libraries. Documentation:  https://albumentations.ai/docs/ Paper about the library: https://www.mdpi.com/2078-2489/11/2/125",
        "tags": [
            "image-augmentation",
            "image-classification",
            "fast-augmentations",
            "python",
            "image-segmentation",
            "machine-learning",
            "image-processing",
            "augmentation",
            "segmentation",
            "deep-learning",
            "object-detection",
            "detection"
        ]
    },
    "https://github.com/ydataai/ydata-profiling": {
        "extra-tags": [],
        "date": "2016-01-09",
        "title": "ydata-profiling",
        "summary": "Create HTML profiling reports from pandas DataFrame objects",
        "tags": [
            "pandas-dataframe",
            "machine-learning",
            "eda",
            "exploration",
            "data-analysis",
            "jupyter",
            "pandas-profiling",
            "html-report",
            "data-exploration",
            "python",
            "data-quality",
            "data-science",
            "exploratory-data-analysis",
            "deep-learning",
            "big-data-analytics",
            "jupyter-notebook",
            "data-profiling",
            "hacktoberfest",
            "statistics",
            "pandas"
        ]
    },
    "https://github.com/cjlin1/liblinear": {
        "extra-tags": [
            "library",
            "classification"
        ],
        "date": "2014-05-19",
        "title": "liblinear",
        "summary": "LIBLINEAR -- A Library for Large Linear Classification",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/ypeleg/HungaBunga": {
        "extra-tags": [],
        "date": "2019-08-20",
        "title": "HungaBunga",
        "summary": "HungaBunga: Brute-Force all sklearn models with all parameters using .fit .predict!",
        "tags": [
            "python",
            "fit",
            "brute",
            "kaggle",
            "learning",
            "scikit-learn",
            "predict",
            "sklearn",
            "automl",
            "machine",
            "force"
        ]
    },
    "https://github.com/timqian/chart.xkcd": {
        "extra-tags": [],
        "date": "2019-08-05",
        "title": "chart.xkcd",
        "summary": "xkcd styled chart lib",
        "tags": [
            "hand-drawn",
            "javascript",
            "graph",
            "xkcd",
            "svg",
            "chart",
            "svg-sprite",
            "html5",
            "html5-charts"
        ]
    },
    "https://github.com/nschloe/tikzplotlib": {
        "extra-tags": [],
        "date": "2010-01-14",
        "title": "tikzplotlib",
        "summary": ":bar_chart: Save matplotlib figures as TikZ/PGFplots for smooth integration into LaTeX.",
        "tags": [
            "latex",
            "python",
            "tikz",
            "matplotlib",
            "pgfplots"
        ]
    },
    "https://github.com/coleifer/huey": {
        "extra-tags": [],
        "date": "2011-11-03",
        "title": "huey",
        "summary": "a little task queue for python",
        "tags": [
            "python",
            "task-queue",
            "dank",
            "queue",
            "redis"
        ]
    },
    "https://github.com/Mcompetitions/M4-methods": {
        "extra-tags": [],
        "date": "2017-12-21",
        "title": "M4-methods",
        "summary": "Data, Benchmarks, and methods submitted to the M4 forecasting competition",
        "tags": [
            "r"
        ]
    },
    "https://github.com/spatialaudio/nbsphinx": {
        "extra-tags": [],
        "date": "2015-11-17",
        "title": "nbsphinx",
        "summary": ":ledger: Sphinx source parser for Jupyter notebooks",
        "tags": [
            "sphinx-doc",
            "jupyter-notebook",
            "python",
            "sphinx-extension"
        ]
    },
    "https://github.com/computationalmodelling/nbval": {
        "extra-tags": [
            "jupyter",
            "notebooks"
        ],
        "date": "2015-04-09",
        "title": "nbval",
        "summary": "A py.test plugin to validate Jupyter notebooks",
        "tags": [
            "testing",
            "jupyter-notebook",
            "python",
            "pytest-plugin",
            "pytest",
            "ipython-notebook"
        ]
    },
    "https://github.com/more-itertools/more-itertools": {
        "extra-tags": [],
        "date": "2012-04-26",
        "title": "more-itertools",
        "summary": "More routines for operating on iterables, beyond itertools",
        "tags": [
            "python"
        ]
    },
    "https://github.com/rasbt/deeplearning-models": {
        "extra-tags": [],
        "date": "2019-06-05",
        "title": "deeplearning-models",
        "summary": "A collection of various deep learning architectures, models, and tips",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/thomasahle/sunfish": {
        "extra-tags": [],
        "date": "2014-02-10",
        "title": "sunfish",
        "summary": "Sunfish: a Python Chess Engine in 111 lines of code",
        "tags": [
            "python",
            "chess-engine",
            "chess-ai",
            "ai",
            "sunfish",
            "chess"
        ]
    },
    "https://github.com/librosa/librosa": {
        "extra-tags": [],
        "date": "2012-10-20",
        "title": "librosa",
        "summary": "Python library for audio and music analysis",
        "tags": [
            "audio",
            "music",
            "dsp",
            "python",
            "librosa",
            "closember",
            "scipy"
        ]
    },
    "https://github.com/koaning/scikit-lego": {
        "extra-tags": [],
        "date": "2019-01-21",
        "title": "scikit-lego",
        "summary": "Extra blocks for scikit-learn pipelines.",
        "tags": [
            "scikit-learn",
            "common-sense",
            "python",
            "machine-learning"
        ]
    },
    "https://github.com/axelbellec/Jupytoc": {
        "extra-tags": [],
        "date": "2016-11-22",
        "title": "Jupytoc",
        "summary": ":pushpin: A commmand-line interface to add or update TOC to Jupyter Notebooks",
        "tags": [
            "python"
        ]
    },
    "https://github.com/carbon-app/carbon": {
        "extra-tags": [],
        "date": "2017-06-16",
        "title": "carbon",
        "summary": ":black_heart: Create and share beautiful images of your source code",
        "tags": [
            "beautiful",
            "education",
            "sharing",
            "presentation",
            "javascript",
            "tweet",
            "github-gist",
            "snippets",
            "carbon"
        ]
    },
    "https://github.com/boostorg/histogram": {
        "extra-tags": [],
        "date": "2016-04-06",
        "title": "histogram",
        "summary": "Fast multi-dimensional generalized histogram with convenient interface for C++14",
        "tags": [
            "convenient-interface",
            "data-analysis",
            "histogram",
            "boost",
            "convenient",
            "statistics",
            "c-plus-plus",
            "c-plus-plus-14",
            "boost-libraries",
            "c++",
            "header-only"
        ]
    },
    "https://github.com/DistrictDataLabs/yellowbrick": {
        "extra-tags": [],
        "date": "2016-05-18",
        "title": "yellowbrick",
        "summary": "Visual analysis and diagnostic tools to facilitate machine learning model selection.",
        "tags": [
            "anaconda",
            "python",
            "matplotlib",
            "machine-learning",
            "scikit-learn",
            "estimator",
            "visual-analysis",
            "visualizer",
            "model-selection",
            "visualization"
        ]
    },
    "https://github.com/TeamHG-Memex/eli5": {
        "extra-tags": [],
        "date": "2016-09-15",
        "title": "eli5",
        "summary": "A library for debugging/inspecting machine learning classifiers and explaining their predictions",
        "tags": [
            "python",
            "inspection",
            "machine-learning",
            "scikit-learn",
            "nlp",
            "xgboost",
            "data-science",
            "jupyter notebook",
            "crfsuite",
            "explanation",
            "lightgbm"
        ]
    },
    "https://github.com/YahooArchive/samoa": {
        "extra-tags": [],
        "date": "2013-10-11",
        "title": "samoa",
        "summary": "SAMOA (Scalable Advanced Massive Online Analysis) is an open-source platform for mining big data streams.",
        "tags": [
            "java"
        ]
    },
    "https://github.com/sublee/trueskill": {
        "extra-tags": [],
        "date": "2012-01-10",
        "title": "trueskill",
        "summary": "An implementation of the TrueSkill rating system for Python",
        "tags": [
            "trueskill",
            "rating-system",
            "python"
        ]
    },
    "https://github.com/sdv-dev/Copulas": {
        "extra-tags": [],
        "date": "2017-11-13",
        "title": "Copulas",
        "summary": "A library to model multivariate data using copulas.",
        "tags": [
            "generative-model",
            "python",
            "data-generation",
            "copulas",
            "generative-ai",
            "machine-learning",
            "synthetic-data",
            "synthetic-data-generation",
            "tabular-data"
        ]
    },
    "https://github.com/kynan/nbstripout": {
        "extra-tags": [],
        "date": "2015-09-12",
        "title": "nbstripout",
        "summary": "strip output from Jupyter and IPython notebooks",
        "tags": [
            "ipython",
            "jupyter-notebook",
            "python",
            "hooks",
            "filter",
            "hacktoberfest",
            "jupyter",
            "git",
            "ipython-notebook"
        ]
    },
    "https://github.com/openvenues/libpostal": {
        "extra-tags": [],
        "date": "2015-03-03",
        "title": "libpostal",
        "summary": "A C library for parsing/normalizing street addresses around the world. Powered by statistical NLP and open geo data.",
        "tags": [
            "natural-language-processing",
            "international",
            "address-parser",
            "deduping",
            "machine-learning",
            "address",
            "nlp",
            "record-linkage",
            "deduplication",
            "c"
        ]
    },
    "https://github.com/workalendar/workalendar": {
        "extra-tags": [],
        "date": "2013-11-20",
        "title": "workalendar",
        "summary": "Worldwide holidays and workdays computational toolkit.",
        "tags": [
            "peopledoc-opensource",
            "python",
            "calendars",
            "localization",
            "calendar"
        ]
    },
    "https://github.com/alteryx/featuretools": {
        "extra-tags": [],
        "date": "2017-09-08",
        "title": "featuretools",
        "summary": "An open source python library for automated feature engineering",
        "tags": [
            "python",
            "machine-learning",
            "feature-engineering",
            "scikit-learn",
            "automl",
            "data-science",
            "automated-feature-engineering",
            "automated-machine-learning"
        ]
    },
    "https://github.com/HIPS/autograd": {
        "extra-tags": [],
        "date": "2014-11-24",
        "title": "autograd",
        "summary": "Efficiently computes derivatives of numpy code.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/MontFerret/ferret": {
        "extra-tags": [],
        "date": "2018-08-23",
        "title": "ferret",
        "summary": "Declarative web scraping",
        "tags": [
            "chrome",
            "dsl",
            "data-mining",
            "go",
            "query-language",
            "scraper",
            "scraping-websites",
            "library",
            "cli",
            "scraping",
            "hacktoberfest",
            "crawling",
            "golang",
            "crawler",
            "cdp",
            "tool",
            "hacktoberfest2021"
        ]
    },
    "https://github.com/parrt/dtreeviz": {
        "extra-tags": [],
        "date": "2018-08-13",
        "title": "dtreeviz",
        "summary": "A python library for decision tree visualization and model interpretation.",
        "tags": [
            "random-forest",
            "python",
            "machine-learning",
            "scikit-learn",
            "jupyter notebook",
            "xgboost",
            "data-science",
            "decision-trees",
            "model-interpretation",
            "visualization"
        ]
    },
    "https://github.com/openfaas/faas": {
        "extra-tags": [],
        "date": "2016-12-22",
        "title": "faas",
        "summary": "OpenFaaS - Serverless Functions Made Simple",
        "tags": [
            "docker",
            "paas",
            "go",
            "faas",
            "kubernetes",
            "k8s",
            "nodejs",
            "serverless-functions",
            "golang",
            "prometheus",
            "lambda",
            "serverless",
            "gitops",
            "functions",
            "functions-as-a-service"
        ]
    },
    "https://github.com/saulpw/visidata": {
        "extra-tags": [],
        "date": "2016-10-27",
        "title": "visidata",
        "summary": "A terminal spreadsheet multitool for discovering and arranging data",
        "tags": [
            "csv",
            "opendata",
            "python",
            "reconciliation",
            "devops-tools",
            "cli",
            "spreadsheet",
            "unix-toolkit",
            "datawrangling",
            "tui",
            "datajournalism",
            "hdf5",
            "sqlite",
            "tsv",
            "tabular-data",
            "json",
            "pandas",
            "eda"
        ]
    },
    "https://github.com/fcampelo/EC-Bestiary": {
        "extra-tags": [
            "algorithms"
        ],
        "date": "2016-03-26",
        "title": "EC-Bestiary",
        "summary": "A bestiary of evolutionary, swarm and other metaphor-based algorithms",
        "tags": [
            "tex"
        ]
    },
    "https://github.com/eriklindernoren/ML-From-Scratch": {
        "extra-tags": [],
        "date": "2017-02-05",
        "title": "ML-From-Scratch",
        "summary": "Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from linear regression to deep learning.",
        "tags": [
            "data-mining",
            "python",
            "machine-learning-from-scratch",
            "machine-learning",
            "genetic-algorithm",
            "deep-reinforcement-learning",
            "data-science",
            "deep-learning"
        ]
    },
    "https://github.com/asottile/setuptools-golang": {
        "extra-tags": [],
        "date": "2016-03-06",
        "title": "setuptools-golang",
        "summary": "A setuptools extension for building cpython extensions written in golang.",
        "tags": [
            "setuptools",
            "golang",
            "python"
        ]
    },
    "https://github.com/rough-stuff/rough": {
        "extra-tags": [
            "hand-drawn"
        ],
        "date": "2016-12-13",
        "title": "rough",
        "summary": "Create graphics with a hand-drawn, sketchy, appearance",
        "tags": [
            "svg-path",
            "draw",
            "html",
            "graphics",
            "svg",
            "html5-canvas",
            "canvas"
        ]
    },
    "https://github.com/gregrahn/join-order-benchmark": {
        "extra-tags": [],
        "date": "2016-08-31",
        "title": "join-order-benchmark",
        "summary": "Join Order Benchmark (JOB)",
        "tags": [
            "benchmark",
            "database",
            "sql"
        ]
    },
    "https://github.com/norvig/pytudes": {
        "extra-tags": [],
        "date": "2017-03-01",
        "title": "pytudes",
        "summary": "Python programs, usually short, of considerable difficulty, to perfect particular skills.",
        "tags": [
            "python",
            "python-3",
            "demonstrate-skills",
            "jupyter notebook",
            "programming",
            "practice"
        ]
    },
    "https://github.com/gnab/remark": {
        "extra-tags": [],
        "date": "2011-10-11",
        "title": "remark",
        "summary": "A simple, in-browser, markdown-driven slideshow tool.",
        "tags": [
            "html",
            "javascript",
            "slideshow",
            "markdown"
        ]
    },
    "https://github.com/gonum/gonum": {
        "extra-tags": [],
        "date": "2017-03-25",
        "title": "gonum",
        "summary": "Gonum is a set of numeric libraries for the Go programming language. It contains libraries for matrices, statistics, optimization, and more",
        "tags": [
            "go",
            "matrix",
            "graph",
            "data-analysis",
            "golang",
            "statistics",
            "scientific-computing"
        ]
    },
    "https://github.com/geopy/geopy": {
        "extra-tags": [],
        "date": "2010-03-04",
        "title": "geopy",
        "summary": "Geocoding library for Python.",
        "tags": [
            "python",
            "geocoding",
            "geocoder"
        ]
    },
    "https://github.com/kamranahmedse/design-patterns-for-humans": {
        "extra-tags": [],
        "date": "2017-02-16",
        "title": "design-patterns-for-humans",
        "summary": "An ultra-simplified explanation to design patterns",
        "tags": [
            "architecture",
            "computer-science",
            "principles",
            "software-engineering",
            "design-patterns",
            "engineering"
        ]
    },
    "https://github.com/gaubert/gmvault": {
        "extra-tags": [],
        "date": "2011-12-16",
        "title": "gmvault",
        "summary": "gmail backup software",
        "tags": [
            "oauth2",
            "python",
            "gmail",
            "backup",
            "gmvault",
            "sync",
            "restore"
        ]
    },
    "https://github.com/getredash/redash": {
        "extra-tags": [],
        "date": "2013-10-28",
        "title": "redash",
        "summary": "Make Your Company Data Driven. Connect to any data source, easily visualize, dashboard and share your data.",
        "tags": [
            "spark-sql",
            "python",
            "analytics",
            "dashboard",
            "athena",
            "redash",
            "bigquery",
            "javascript",
            "spark",
            "business-intelligence",
            "hacktoberfest",
            "postgresql",
            "redshift",
            "bi",
            "databricks",
            "visualization",
            "mysql"
        ]
    },
    "https://github.com/facebookarchive/fbpca": {
        "extra-tags": [],
        "date": "2014-09-09",
        "title": "fbpca",
        "summary": "Fast Randomized PCA/SVD",
        "tags": [
            "python"
        ]
    },
    "https://github.com/tidwall/tile38": {
        "extra-tags": [
            "real-time"
        ],
        "date": "2016-03-04",
        "title": "tile38",
        "summary": "Real-time Geospatial and Geofencing",
        "tags": [
            "geospatial",
            "go",
            "spatial",
            "geofences",
            "geo",
            "location",
            "index",
            "database"
        ]
    },
    "https://github.com/jasalt/kuittiskanneri": {
        "extra-tags": [
            "flask",
            "opencv"
        ],
        "date": "2014-06-23",
        "title": "kuittiskanneri",
        "summary": "Receipt Scanner Prototype [AngularJS Flask OpenCV]",
        "tags": [
            "javascript"
        ]
    },
    "https://github.com/kilimchoi/engineering-blogs": {
        "extra-tags": [],
        "date": "2015-06-13",
        "title": "engineering-blogs",
        "summary": "A curated list of engineering blogs",
        "tags": [
            "software-development",
            "engineering-blogs",
            "lists",
            "ruby",
            "tech",
            "programming-blogs"
        ]
    },
    "https://github.com/tloen/llama-int8": {
        "extra-tags": [],
        "date": "2023-03-03",
        "title": "llama-int8",
        "summary": "Quantized inference code for LLaMA models",
        "tags": [
            "python"
        ]
    },
    "https://github.com/TimDettmers/bitsandbytes": {
        "extra-tags": [],
        "date": "2021-06-04",
        "title": "bitsandbytes",
        "summary": "8-bit CUDA functions for PyTorch",
        "tags": [
            "python"
        ]
    },
    "https://github.com/PaladinEE15/RSAC": {
        "extra-tags": [
            "sac"
        ],
        "date": "2020-10-30",
        "title": "RSAC",
        "summary": "Codes for RSAC-AE, SAC-AE and SAC-AO",
        "tags": [
            "python"
        ]
    },
    "https://github.com/unum-cloud/ujrpc": {
        "extra-tags": [],
        "date": "2023-01-03",
        "title": "ujrpc",
        "summary": "Up to 100x Faster FastAPI. JSON-RPC with io_uring, SIMDJSON, and pure CPython bindings",
        "tags": [
            "linux-kernel",
            "epoll",
            "rpc",
            "simd",
            "tcp",
            "io-uring",
            "json",
            "simdjson",
            "python",
            "http-server",
            "flask",
            "json-rpc",
            "backend",
            "liburing",
            "tcp-ip",
            "http",
            "rpc-framework",
            "dpdk",
            "cpython",
            "fast-api",
            "c++"
        ]
    },
    "https://github.com/lgienapp/aquarel": {
        "extra-tags": [],
        "date": "2022-08-07",
        "title": "aquarel",
        "summary": "Styling matplotlib made easy",
        "tags": [
            "python",
            "matplotlib",
            "theming",
            "plotting",
            "data-science",
            "data-visualization",
            "theme",
            "theme-development",
            "visualization"
        ]
    },
    "https://github.com/Kludex/kwonly-transformer": {
        "extra-tags": [
            "transformer"
        ],
        "date": "2022-06-17",
        "title": "kwonly-transformer",
        "summary": "We don't like positional args, we like keyword only args! \ud83c\udf89",
        "tags": [
            "python"
        ]
    },
    "https://github.com/nannou-org/nannou": {
        "extra-tags": [],
        "date": "2017-09-01",
        "title": "nannou",
        "summary": "A Creative Coding Framework for Rust.",
        "tags": [
            "creative-coding",
            "rust"
        ]
    },
    "https://github.com/paulnovello/HSIC-Attribution-Method": {
        "extra-tags": [
            "attribution"
        ],
        "date": "2022-05-18",
        "title": "HSIC-Attribution-Method",
        "summary": "",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/deel-ai/oodeel": {
        "extra-tags": [],
        "date": "2023-01-04",
        "title": "oodeel",
        "summary": "Simple, compact, and hackable post-hoc deep OOD detection on already trained image classifiers in tensorflow and pytorch.",
        "tags": [
            "python",
            "tensorflow",
            "robustness",
            "post-hoc-analysis",
            "pytorch",
            "deep-neural-networks",
            "out-of-distribution-detection"
        ]
    },
    "https://github.com/m-bain/whisperX": {
        "extra-tags": [
            "automatic"
        ],
        "date": "2022-12-09",
        "title": "whisperX",
        "summary": "WhisperX:  Automatic Speech Recognition with Word-level Timestamps (& Diarization)",
        "tags": [
            "speech-to-text",
            "python",
            "speech-recognition",
            "whisper",
            "speech",
            "asr"
        ]
    },
    "https://github.com/jgaud/streamndr": {
        "extra-tags": [],
        "date": "2023-02-22",
        "title": "streamndr",
        "summary": "Novelty detection for data streams in Python",
        "tags": [
            "python",
            "incremental-learning",
            "machine-learning",
            "novelty-detection",
            "stream",
            "data-stream"
        ]
    },
    "https://github.com/igiagkiozis/plotly": {
        "extra-tags": [],
        "date": "2020-01-26",
        "title": "plotly",
        "summary": "Plotly for Rust",
        "tags": [
            "plotlyjs",
            "candlestick-chart",
            "barchart",
            "rust",
            "scatterplot",
            "plot",
            "scatter",
            "chart",
            "financial-analysis",
            "plotly",
            "data-visualization",
            "financial",
            "statistics",
            "data-vizualisation"
        ]
    },
    "https://github.com/Adel-Moumen/fast_ligru": {
        "extra-tags": [],
        "date": "2023-02-15",
        "title": "fast_ligru",
        "summary": "",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/rust-cv/cv": {
        "extra-tags": [],
        "date": "2020-04-04",
        "title": "cv",
        "summary": "Rust CV mono-repo. Contains pure-Rust dependencies which attempt to encapsulate the capability of OpenCV, OpenMVG, and vSLAM frameworks in a cohesive set of APIs.",
        "tags": [
            "algorithms",
            "rust-cv",
            "rust",
            "crates",
            "computer-vision"
        ]
    },
    "https://github.com/rerun-io/rerun": {
        "extra-tags": [
            "images"
        ],
        "date": "2022-04-08",
        "title": "rerun",
        "summary": "Log images, point clouds, etc, and visualize them effortlessly. Built in Rust using egui",
        "tags": [
            "python",
            "rust",
            "robotics",
            "computer-vision",
            "visualization"
        ]
    },
    "https://github.com/askanium/rustplotlib": {
        "extra-tags": [],
        "date": "2020-03-27",
        "title": "rustplotlib",
        "summary": "A pure Rust visualization library inspired by D3.js",
        "tags": [
            "dataviz",
            "visualization",
            "rust-library",
            "rust"
        ]
    },
    "https://github.com/brihernandez/SimpleWings": {
        "extra-tags": [
            "simple",
            "location"
        ],
        "date": "2017-10-22",
        "title": "SimpleWings",
        "summary": "A simple, configurable aerodynamic wing that applies lift at its location. Includes flyable example plane.",
        "tags": [
            "c#"
        ]
    },
    "https://github.com/openai/triton": {
        "extra-tags": [],
        "date": "2014-08-30",
        "title": "triton",
        "summary": "Development repository for the Triton language and compiler",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/MaugrimEP/DiffusionModel": {
        "extra-tags": [
            "pytorch",
            "diffusion"
        ],
        "date": "2023-02-08",
        "title": "DiffusionModel",
        "summary": "Pytorch Lightning Diffusion model implementation. DDPM and DDIM.",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/Lucas-rbnt/aml_adversarial_attacks": {
        "extra-tags": [],
        "date": "2022-01-22",
        "title": "aml_adversarial_attacks",
        "summary": "Projet dans le cadre du cursus SDD de l'ISAE Supaero",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/axodotdev/cargo-dist": {
        "extra-tags": [],
        "date": "2022-10-13",
        "title": "cargo-dist",
        "summary": "\ud83d\udce6 shippable application packaging for Rust",
        "tags": [
            "release-automation",
            "cargo",
            "rust",
            "packaging",
            "installers"
        ]
    },
    "https://github.com/fathyb/carbonyl": {
        "extra-tags": [],
        "date": "2023-01-20",
        "title": "carbonyl",
        "summary": "Chromium running inside your terminal",
        "tags": [
            "chromium",
            "rust",
            "terminal",
            "browser"
        ]
    },
    "https://github.com/online-ml/beaver": {
        "extra-tags": [],
        "date": "2023-01-26",
        "title": "beaver",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/initialcommit-com/git-sim": {
        "extra-tags": [
            "terminal"
        ],
        "date": "2022-10-18",
        "title": "git-sim",
        "summary": "Visually simulate Git operations in your own repos with a single terminal command.",
        "tags": [
            "python3",
            "python",
            "opencv",
            "manim",
            "git",
            "gitpython",
            "visualization"
        ]
    },
    "https://github.com/gvinciguerra/PGM-index": {
        "extra-tags": [],
        "date": "2019-10-18",
        "title": "PGM-index",
        "summary": "\ud83c\udfc5State-of-the-art learned data structure that enables fast lookup, predecessor, range searches and updates in arrays of billions of items using orders of magnitude less space than traditional indexes",
        "tags": [
            "compression",
            "multidimensional-trees",
            "b-tree",
            "indexing",
            "research",
            "big-data",
            "machine-learning",
            "spatial-index",
            "data-structures",
            "succinct-data-structure",
            "database",
            "multidimensional",
            "cpp",
            "c++",
            "header-only"
        ]
    },
    "https://github.com/gvinciguerra/PyGM": {
        "extra-tags": [],
        "date": "2020-06-30",
        "title": "PyGM",
        "summary": "\ud83d\udc0d Python library implementing sorted containers with state-of-the-art query performance and compressed memory usage",
        "tags": [
            "algorithms",
            "compression",
            "python",
            "research",
            "machine-learning",
            "data-structures",
            "data-science",
            "database",
            "python-library"
        ]
    },
    "https://github.com/sickcodes/Docker-OSX": {
        "extra-tags": [],
        "date": "2020-06-04",
        "title": "Docker-OSX",
        "summary": "Run macOS VM in a Docker! Run near native OSX-KVM in Docker! X11 Forwarding! CI/CD for OS X Security Research! Docker mac Containers.",
        "tags": [
            "docker-osx",
            "docker",
            "kvm",
            "x11",
            "shell",
            "os",
            "container",
            "macos",
            "x",
            "osx",
            "osx-kvm"
        ]
    },
    "https://github.com/Rolv-Arild/Necto": {
        "extra-tags": [],
        "date": "2021-09-26",
        "title": "Necto",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/CyberAgentAILab/cmaes": {
        "extra-tags": [],
        "date": "2020-01-30",
        "title": "cmaes",
        "summary": "Python library for CMA Evolution Strategy.",
        "tags": [
            "optuna",
            "python",
            "evolution-strategy",
            "cma-es",
            "hyperparameter-optimization"
        ]
    },
    "https://github.com/pyutils/line_profiler": {
        "extra-tags": [
            "profiling",
            "python"
        ],
        "date": "2019-12-10",
        "title": "line_profiler",
        "summary": "Line-by-line profiling for Python",
        "tags": [
            "cython"
        ]
    },
    "https://github.com/lerrel/gym-adv": {
        "extra-tags": [],
        "date": "2017-03-20",
        "title": "gym-adv",
        "summary": "Gym environments modified with adversarial agents",
        "tags": [
            "reinforcement-learning",
            "python",
            "adversarial-learning"
        ]
    },
    "https://github.com/inspirai/TimeChamber": {
        "extra-tags": [],
        "date": "2022-08-17",
        "title": "TimeChamber",
        "summary": "A Massively Parallel Large Scale Self-Play Framework",
        "tags": [
            "python",
            "self-play",
            "reinforcement-learning",
            "deep-reinforcement-learning",
            "isaac-gym",
            "multi-agent"
        ]
    },
    "https://github.com/db0/AI-Horde": {
        "extra-tags": [],
        "date": "2022-09-13",
        "title": "AI-Horde",
        "summary": "A crowdsourced distributed cluster for AI art and text generation",
        "tags": [
            "stable-diffusion",
            "python",
            "opt",
            "ai",
            "distributed-computing",
            "gpt",
            "flask-api"
        ]
    },
    "https://github.com/AlexandreChaussard/AMFLearning": {
        "extra-tags": [],
        "date": "2022-12-27",
        "title": "AMFLearning",
        "summary": "Data Stream processing project (M2DS - Institut polytechnique de Paris)",
        "tags": [
            "python"
        ]
    },
    "https://github.com/datrs/flat-tree": {
        "extra-tags": [
            "tree",
            "vector"
        ],
        "date": "2016-03-09",
        "title": "flat-tree",
        "summary": "Map a binary tree to a vector.",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/zakird/crux-top-lists": {
        "extra-tags": [],
        "date": "2022-12-29",
        "title": "crux-top-lists",
        "summary": "Downloadable snapshots of the Chrome Top Million Websites pulled from public CrUX data in BigQuery.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/LAION-AI/Open-Assistant": {
        "extra-tags": [
            "systems"
        ],
        "date": "2022-12-13",
        "title": "Open-Assistant",
        "summary": "OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so.",
        "tags": [
            "chatgpt",
            "python",
            "assistant",
            "nextjs",
            "machine-learning",
            "rlhf",
            "ai",
            "discord-bot",
            "language-model"
        ]
    },
    "https://github.com/evcxr/evcxr": {
        "extra-tags": [],
        "date": "2018-09-25",
        "title": "evcxr",
        "summary": "",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/refined-github/refined-github": {
        "extra-tags": [],
        "date": "2016-02-15",
        "title": "refined-github",
        "summary": ":octocat: Browser extension that simplifies the GitHub interface and adds useful features",
        "tags": [
            "safari-extension",
            "firefox-addon",
            "chrome-extension",
            "github",
            "github-extension",
            "browser-extension",
            "typescript",
            "userstyle"
        ]
    },
    "https://github.com/deel-ai/influenciae": {
        "extra-tags": [],
        "date": "2021-10-04",
        "title": "influenciae",
        "summary": "\ud83d\udc4b Influenciae is a Tensorflow Toolbox for Influence Functions",
        "tags": [
            "python",
            "influence-functions",
            "outlier-detection",
            "fairness-ai",
            "misclassification",
            "explainable-ai",
            "explainability"
        ]
    },
    "https://github.com/SuReLI/SGQN": {
        "extra-tags": [],
        "date": "2022-10-03",
        "title": "SGQN",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/jhspetersson/fselect": {
        "extra-tags": [],
        "date": "2018-01-26",
        "title": "fselect",
        "summary": "Find files with SQL-like queries",
        "tags": [
            "files",
            "sql",
            "cli",
            "rust",
            "tool",
            "find",
            "filesystem",
            "hacktoberfest",
            "utility",
            "query",
            "sql-like"
        ]
    },
    "https://github.com/m1guelpf/auto-commit": {
        "extra-tags": [
            "cli",
            "tool"
        ],
        "date": "2022-10-30",
        "title": "auto-commit",
        "summary": "A CLI tool that automatically writes commit messages for you.",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/hpcaitech/ColossalAI": {
        "extra-tags": [
            "models"
        ],
        "date": "2021-10-28",
        "title": "ColossalAI",
        "summary": "Making large AI models cheaper, faster and more accessible",
        "tags": [
            "large-scale",
            "model-parallelism",
            "data-parallelism",
            "python",
            "hpc",
            "pipeline-parallelism",
            "heterogeneous-training",
            "big-model",
            "inference",
            "ai",
            "distributed-computing",
            "deep-learning",
            "foundation-models"
        ]
    },
    "https://github.com/LaurentMazare/diffusers-rs": {
        "extra-tags": [
            "api"
        ],
        "date": "2022-11-05",
        "title": "diffusers-rs",
        "summary": "An implementation of the diffusers api in Rust",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/qu3vipon/python-ddd": {
        "extra-tags": [
            "fastapi"
        ],
        "date": "2022-10-23",
        "title": "python-ddd",
        "summary": "Python DDD pattern example using FastAPI",
        "tags": [
            "python"
        ]
    },
    "https://github.com/ELS-RD/kernl": {
        "extra-tags": [],
        "date": "2022-08-05",
        "title": "kernl",
        "summary": "Kernl lets you run PyTorch transformer models several times faster on GPU with a single line of code, and is designed to be easily hackable.",
        "tags": [
            "transformer",
            "triton",
            "cuda",
            "jupyter notebook",
            "pytorch",
            "cuda-kernel"
        ]
    },
    "https://github.com/galleon/gym-jsbsim": {
        "extra-tags": [],
        "date": "2022-10-26",
        "title": "gym-jsbsim",
        "summary": "A minimal gym environment for jsbsim ",
        "tags": []
    },
    "https://github.com/Farama-Foundation/Gymnasium": {
        "extra-tags": [],
        "date": "2022-09-08",
        "title": "Gymnasium",
        "summary": "A standard API for single-agent reinforcement learning environments, with popular reference environments and related utilities (formerly Gym)",
        "tags": [
            "gym",
            "api",
            "python",
            "reinforcement-learning"
        ]
    },
    "https://github.com/SamsungLabs/tqc_pytorch": {
        "extra-tags": [],
        "date": "2020-04-29",
        "title": "tqc_pytorch",
        "summary": "Implementation of Truncated Quantile Critics method for continuous reinforcement learning. https://bayesgroup.github.io/tqc/",
        "tags": [
            "python"
        ]
    },
    "https://github.com/rust-lang/rust-forge": {
        "extra-tags": [],
        "date": "2016-01-22",
        "title": "rust-forge",
        "summary": "Information useful to people contributing to Rust",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life": {
        "extra-tags": [
            "cvpr"
        ],
        "date": "2020-06-24",
        "title": "Bringing-Old-Photos-Back-to-Life",
        "summary": "Bringing Old Photo Back to Life (CVPR 2020 oral)",
        "tags": [
            "image-manipulation",
            "python",
            "generative-adversarial-network",
            "photos",
            "old-photo-restoration",
            "gans",
            "photo-restoration",
            "pytorch",
            "image-restoration"
        ]
    },
    "https://github.com/Futurne/anime_vae": {
        "extra-tags": [
            "anime"
        ],
        "date": "2021-10-11",
        "title": "anime_vae",
        "summary": "Small Tkinter app using a VAE to produce anime faces.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/bensadeh/circumflex": {
        "extra-tags": [],
        "date": "2020-07-28",
        "title": "circumflex",
        "summary": "\ud83c\udf3f It's Hacker News in your terminal",
        "tags": [
            "hackernews",
            "console",
            "go",
            "command-line",
            "terminal",
            "cli",
            "client",
            "reader",
            "ycombinator",
            "tui",
            "news",
            "hacker-news",
            "hacker"
        ]
    },
    "https://github.com/mxschmitt/action-tmate": {
        "extra-tags": [],
        "date": "2019-08-23",
        "title": "action-tmate",
        "summary": "Debug your GitHub Actions via SSH by using tmate to get access to the runner system itself.",
        "tags": [
            "debugging",
            "ssh",
            "tmate",
            "actions",
            "javascript",
            "hacktoberfest",
            "github-action",
            "github-actions"
        ]
    },
    "https://github.com/Algue-Rythme/SinkhornMuGP": {
        "extra-tags": [],
        "date": "2022-10-10",
        "title": "SinkhornMuGP",
        "summary": "Gaussian Processes on Distributions based on Regularized OT",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/instadeepai/poppy": {
        "extra-tags": [],
        "date": "2022-09-29",
        "title": "poppy",
        "summary": ":hibiscus: Population-Based Reinforcement Learning for Combinatorial Optimization",
        "tags": [
            "python"
        ]
    },
    "https://github.com/pytorch/rl": {
        "extra-tags": [],
        "date": "2022-02-01",
        "title": "rl",
        "summary": "A modular, primitive-first, python-first PyTorch library for Reinforcement Learning.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/soxoj/maigret": {
        "extra-tags": [],
        "date": "2020-06-27",
        "title": "maigret",
        "summary": "\ud83d\udd75\ufe0f\u200d\u2642\ufe0f Collect a dossier on a person by username from thousands of sites",
        "tags": [
            "page-parsing",
            "username-checker",
            "osint",
            "dossier",
            "detective",
            "python3",
            "identification",
            "profiles",
            "nickname",
            "investigation",
            "sherlock",
            "python",
            "recursive-search",
            "socmint",
            "namechecker",
            "parsing",
            "social-network",
            "username",
            "username-search"
        ]
    },
    "https://github.com/noib3/nvim-oxi": {
        "extra-tags": [],
        "date": "2022-05-12",
        "title": "nvim-oxi",
        "summary": ":link: Rust bindings to all things Neovim",
        "tags": [
            "rust",
            "rust-bindings",
            "neovim"
        ]
    },
    "https://github.com/williamboman/mason.nvim": {
        "extra-tags": [],
        "date": "2022-07-06",
        "title": "mason.nvim",
        "summary": "Portable package manager for Neovim that runs everywhere Neovim runs. Easily install and manage LSP servers, DAP servers, linters, and formatters.",
        "tags": [
            "lua",
            "packages",
            "masoninstall",
            "mason",
            "package",
            "package-manager",
            "hacktoberfest",
            "nvim-lsp-installer",
            "lspinstall",
            "nvim",
            "manager",
            "neovim"
        ]
    },
    "https://github.com/cruft/cruft": {
        "extra-tags": [],
        "date": "2019-09-22",
        "title": "cruft",
        "summary": "Allows you to maintain all the necessary cruft for packaging and building projects separate from the code you intentionally write. Built on-top of, and fully compatible with, CookieCutter.",
        "tags": [
            "python3",
            "cookiecutter",
            "python",
            "instantly",
            "boilerplate",
            "templating",
            "cruft",
            "quickly"
        ]
    },
    "https://github.com/araffin/sbx": {
        "extra-tags": [
            "baselines",
            "jax"
        ],
        "date": "2022-09-29",
        "title": "sbx",
        "summary": "SBX: Stable Baselines Jax (SB3 + Jax)",
        "tags": [
            "python"
        ]
    },
    "https://github.com/dosisod/refurb": {
        "extra-tags": [
            "tool"
        ],
        "date": "2022-07-27",
        "title": "refurb",
        "summary": "A tool for refurbishing and modernizing Python codebases",
        "tags": [
            "testing",
            "python",
            "cli",
            "mypy",
            "hacktoberfest",
            "gplv3",
            "python310"
        ]
    },
    "https://github.com/jamestthompson3/nvim-remote-containers": {
        "extra-tags": [],
        "date": "2020-02-23",
        "title": "nvim-remote-containers",
        "summary": "Develop inside docker containers, just like VSCode",
        "tags": [
            "docker",
            "lua",
            "nvim",
            "remote-containers",
            "neovim"
        ]
    },
    "https://github.com/dtolnay/rust-toolchain": {
        "extra-tags": [
            "github",
            "rust"
        ],
        "date": "2020-05-02",
        "title": "rust-toolchain",
        "summary": "Concise GitHub Action for installing a Rust toolchain",
        "tags": [
            "shell"
        ]
    },
    "https://github.com/m1guelpf/yt-whisper": {
        "extra-tags": [],
        "date": "2022-09-22",
        "title": "yt-whisper",
        "summary": "Using OpenAI's Whisper to automatically generate YouTube subtitles",
        "tags": [
            "ffmpeg",
            "python",
            "openai",
            "whisper",
            "openai-whisper",
            "transcribe",
            "youtube-dl",
            "youtube",
            "subtitles",
            "subtitles-generated"
        ]
    },
    "https://github.com/openai/whisper": {
        "extra-tags": [],
        "date": "2022-09-16",
        "title": "whisper",
        "summary": "Robust Speech Recognition via Large-Scale Weak Supervision",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/google/tensorstore": {
        "extra-tags": [
            "library",
            "arrays"
        ],
        "date": "2020-03-30",
        "title": "tensorstore",
        "summary": "Library for reading and writing large multi-dimensional arrays.",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/jonhoo/rust-ci-conf": {
        "extra-tags": [],
        "date": "2022-09-17",
        "title": "rust-ci-conf",
        "summary": "Collection of CI configuration files for Rust projects",
        "tags": []
    },
    "https://github.com/nicklashansen/tdmpc": {
        "extra-tags": [],
        "date": "2022-02-22",
        "title": "tdmpc",
        "summary": "Code for \"Temporal Difference Learning for Model Predictive Control\"",
        "tags": [
            "python"
        ]
    },
    "https://github.com/sebiwtt/flaskriver": {
        "extra-tags": [],
        "date": "2022-09-13",
        "title": "flaskriver",
        "summary": "This is a repository for the open-source project flaskriver which will make it easier to combine the lightweight web-framework flask with the online-ML library river.",
        "tags": [
            "online-learning",
            "python",
            "river",
            "machine-learning",
            "flask",
            "data-science",
            "streaming"
        ]
    },
    "https://github.com/divamgupta/diffusionbee-stable-diffusion-ui": {
        "extra-tags": [],
        "date": "2022-09-06",
        "title": "diffusionbee-stable-diffusion-ui",
        "summary": "Diffusion Bee is the easiest way to run Stable Diffusion locally on your M1 Mac. Comes with a one-click installer. No dependencies or technical knowledge needed.",
        "tags": [
            "stable-diffusion",
            "electron-app",
            "macos",
            "javascript"
        ]
    },
    "https://github.com/deepmind/mujoco_menagerie": {
        "extra-tags": [],
        "date": "2022-09-05",
        "title": "mujoco_menagerie",
        "summary": "A collection of high-quality models for the MuJoCo physics engine, curated by DeepMind.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/smartcorelib/smartcore": {
        "extra-tags": [],
        "date": "2019-05-08",
        "title": "smartcore",
        "summary": "A comprehensive library for machine learning and numerical computing. The library provides a set of tools for linear algebra, numerical computing, optimization, and enables a generic, powerful yet still efficient approach to machine learning.",
        "tags": [
            "statistical-models",
            "rust",
            "machine-learning",
            "classification",
            "rust-lang",
            "clustering",
            "regression",
            "statistical-learning",
            "scientific-computing",
            "machine-learning-algorithms",
            "model-selection"
        ]
    },
    "https://github.com/fadeevab/design-patterns-rust": {
        "extra-tags": [],
        "date": "2022-07-19",
        "title": "design-patterns-rust",
        "summary": "Rust examples for all 23 classic GoF design patterns, and even a little more",
        "tags": [
            "rust",
            "design-patterns"
        ]
    },
    "https://github.com/charliermarsh/ruff": {
        "extra-tags": [],
        "date": "2022-08-09",
        "title": "ruff",
        "summary": "An extremely fast Python linter, written in Rust.",
        "tags": [
            "python3",
            "rustpython",
            "python",
            "linter",
            "static-analysis",
            "styleguide",
            "style-guide",
            "rust",
            "ruff",
            "static-code-analysis",
            "pep8"
        ]
    },
    "https://github.com/instadeepai/jumanji": {
        "extra-tags": [],
        "date": "2022-08-11",
        "title": "jumanji",
        "summary": "\ud83c\udf34 A Suite of Industry-Driven Hardware-Accelerated RL Environments written in JAX",
        "tags": [
            "research",
            "jax",
            "python",
            "reinforcement-learning"
        ]
    },
    "https://github.com/mainrs/git-cm": {
        "extra-tags": [],
        "date": "2020-07-16",
        "title": "git-cm",
        "summary": "Easily create conventional-commits friendly commit messages.",
        "tags": [
            "conventional-commits",
            "rust",
            "cli",
            "git-subcommand",
            "git"
        ]
    },
    "https://github.com/Volham22/raptor": {
        "extra-tags": [
            "http",
            "server"
        ],
        "date": "2022-08-18",
        "title": "raptor",
        "summary": "Yet another lightweight and easy to use HTTP(S) server",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/raphaelsty/kgsearch": {
        "extra-tags": [
            "query",
            "knowledge"
        ],
        "date": "2022-08-11",
        "title": "kgsearch",
        "summary": "Query and visualize knowledge graphs",
        "tags": [
            "python",
            "3d-graph",
            "information-retrieval",
            "graph",
            "knowledge-graph",
            "visualization"
        ]
    },
    "https://github.com/brainfucksec/neovim-lua": {
        "extra-tags": [],
        "date": "2021-05-29",
        "title": "neovim-lua",
        "summary": "Neovim KISS configuration with Lua",
        "tags": [
            "neovim-dotfiles",
            "vim",
            "dotfiles",
            "lua",
            "vimrc",
            "kiss",
            "nvim-lua",
            "ide",
            "nvim",
            "neovim-configuration",
            "nvim-configs",
            "neovim-lua",
            "neovim-config",
            "neovim"
        ]
    },
    "https://github.com/gemseo/gemseo-pymoo": {
        "extra-tags": [],
        "date": "2022-08-01",
        "title": "gemseo-pymoo",
        "summary": " A GEMSEO plugin for interfacing pymoo with GEMSEO. This is a MIRROR of our gitlab repository, the development activity and support happen over there.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/gemseo/gemseo-umdo": {
        "extra-tags": [],
        "date": "2022-08-01",
        "title": "gemseo-umdo",
        "summary": "Capability for MDO under uncertainty. This is a MIRROR of our gitlab repository, the development activity and support happen over there. ",
        "tags": [
            "python"
        ]
    },
    "https://github.com/gemseo/gemseo-mlearning": {
        "extra-tags": [],
        "date": "2022-08-01",
        "title": "gemseo-mlearning",
        "summary": "Miscellaneous machine learning capabilities. This is a MIRROR of our gitlab repository, the development activity and support happen over there.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/gemseo/gemseo-calibration": {
        "extra-tags": [],
        "date": "2022-08-01",
        "title": "gemseo-calibration",
        "summary": "Capability to calibrate GEMSEO disciplines from data.  This is a MIRROR of our gitlab repository, the development activity and support happen over there. ",
        "tags": [
            "python"
        ]
    },
    "https://github.com/3outeille/torchinfer": {
        "extra-tags": [],
        "date": "2022-07-08",
        "title": "torchinfer",
        "summary": "Deep learning inference framework [WIP]",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/languagetool-org/languagetool": {
        "extra-tags": [
            "languages"
        ],
        "date": "2013-08-08",
        "title": "languagetool",
        "summary": "Style and Grammar Checker for 25+ Languages",
        "tags": [
            "natural-language-processing",
            "java",
            "grammar",
            "spellcheck",
            "style-checker",
            "natural-language",
            "proofreading"
        ]
    },
    "https://github.com/b7leung/MLE-Flashcards": {
        "extra-tags": [],
        "date": "2022-07-23",
        "title": "MLE-Flashcards",
        "summary": "200+ detailed flashcards useful for reviewing topics in machine learning, computer vision, and computer science.",
        "tags": [
            "interview",
            "review",
            "machine-learning",
            "interview-preparation",
            "ai",
            "computer-science",
            "flashcards",
            "computer-vision",
            "artificial-intelligence"
        ]
    },
    "https://github.com/twni2016/pomdp-baselines": {
        "extra-tags": [],
        "date": "2021-10-09",
        "title": "pomdp-baselines",
        "summary": "Simple (but often Strong) Baselines for POMDPs in PyTorch - ICML 2022",
        "tags": [
            "recurrent-neural-networks",
            "discrete-sac",
            "python",
            "sac",
            "pomdp",
            "deep-reinforcement-learning",
            "generalization",
            "robust-rl",
            "td3",
            "pytorch",
            "meta-rl",
            "credit-assignment"
        ]
    },
    "https://github.com/latex-lsp/texlab": {
        "extra-tags": [],
        "date": "2018-12-21",
        "title": "texlab",
        "summary": "An implementation of the Language Server Protocol for LaTeX",
        "tags": [
            "latex",
            "rust",
            "language-server"
        ]
    },
    "https://github.com/vwxyzjn/cleanrl": {
        "extra-tags": [],
        "date": "2019-06-07",
        "title": "cleanrl",
        "summary": "High-quality single file implementation of Deep Reinforcement Learning algorithms with research-friendly features (PPO, DQN, C51, DDPG, TD3, SAC, PPG)",
        "tags": [
            "gym",
            "actor-critic",
            "python",
            "reinforcement-learning",
            "a2c",
            "atari",
            "ale",
            "advantage-actor-critic",
            "machine-learning",
            "deep-reinforcement-learning",
            "proximal-policy-optimization",
            "pytorch",
            "deep-learning",
            "wandb",
            "ppo",
            "phasic-policy-gradient"
        ]
    },
    "https://github.com/dam-grassman/Drone-Interception-Env": {
        "extra-tags": [],
        "date": "2022-07-25",
        "title": "Drone-Interception-Env",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/instadeepai/fastpbrl": {
        "extra-tags": [],
        "date": "2022-06-22",
        "title": "fastpbrl",
        "summary": "Vectorization techniques for fast population-based training.",
        "tags": [
            "python",
            "reinforcement-learning",
            "jax",
            "vectorization",
            "population-based-training"
        ]
    },
    "https://github.com/NvChad/NvChad": {
        "extra-tags": [],
        "date": "2021-03-07",
        "title": "NvChad",
        "summary": "An attempt to make neovim cli functional like an IDE while being very beautiful, blazing fast startuptime ",
        "tags": [
            "dotfiles",
            "open-source",
            "neovim-configuration",
            "vscode",
            "nvim-configs",
            "neovim-dotfiles",
            "lua",
            "rice",
            "neovim-lua",
            "neovim-config",
            "neovim",
            "vim",
            "foss",
            "ricing",
            "ide",
            "editor",
            "vimrc",
            "neovim-setup",
            "hacktoberfest",
            "nvim"
        ]
    },
    "https://github.com/entity-neural-network/entity-gym-rs": {
        "extra-tags": [],
        "date": "2022-07-16",
        "title": "entity-gym-rs",
        "summary": "Rust bindings for entity-gym.",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/coreylowman/dfdx": {
        "extra-tags": [],
        "date": "2021-10-12",
        "title": "dfdx",
        "summary": "Deep learning in Rust, with shape checked tensors and neural networks",
        "tags": [
            "neural-network",
            "tensor",
            "cuda-kernels",
            "gpu-acceleration",
            "cuda-toolkit",
            "cuda",
            "autograd",
            "autodiff",
            "gpu",
            "cuda-support",
            "machine-learning",
            "backpropagation",
            "rust",
            "rust-lang",
            "deep-neural-networks",
            "deep-learning",
            "autodifferentiation",
            "gpu-computing"
        ]
    },
    "https://github.com/google/jaxtyping": {
        "extra-tags": [],
        "date": "2022-06-23",
        "title": "jaxtyping",
        "summary": "Type annotations and runtime checking for shape and dtype of JAX arrays, and PyTrees.",
        "tags": [
            "typing",
            "jax",
            "python-typing",
            "python"
        ]
    },
    "https://github.com/Algue-Rythme/CertifiedQuantileRegression": {
        "extra-tags": [],
        "date": "2022-06-29",
        "title": "CertifiedQuantileRegression",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/facebookresearch/torchdim": {
        "extra-tags": [
            "tensors",
            "pytorch"
        ],
        "date": "2022-06-14",
        "title": "torchdim",
        "summary": "Named tensors with first-class dimensions for PyTorch",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/RobertTLange/gymnax": {
        "extra-tags": [
            "rl",
            "jax"
        ],
        "date": "2020-12-26",
        "title": "gymnax",
        "summary": "RL Environments in JAX  \ud83c\udf0d",
        "tags": [
            "python"
        ]
    },
    "https://github.com/OpenDebates/openskill.py": {
        "extra-tags": [],
        "date": "2020-12-22",
        "title": "openskill.py",
        "summary": "Multiplayer rating system. Better than Elo.",
        "tags": [
            "elo",
            "openskill",
            "python",
            "rating",
            "rating-system",
            "ranking",
            "openskill-py",
            "pypy",
            "ranking-system"
        ]
    },
    "https://github.com/e-cal/evim": {
        "extra-tags": [],
        "date": "2021-04-21",
        "title": "evim",
        "summary": "Neovim meets VS Code. The best of both worlds, in your terminal, ready to code.",
        "tags": [
            "lua"
        ]
    },
    "https://github.com/huggingface/ml-agents": {
        "extra-tags": [],
        "date": "2022-04-20",
        "title": "ml-agents",
        "summary": "Unity Machine Learning Agents Toolkit",
        "tags": [
            "c#"
        ]
    },
    "https://github.com/actions-rs/clippy-check": {
        "extra-tags": [],
        "date": "2019-09-26",
        "title": "clippy-check",
        "summary": "\ud83d\udcce GitHub Action for PR annotations with clippy warnings",
        "tags": [
            "linter",
            "cargo",
            "pull-requests",
            "rust",
            "github",
            "clippy",
            "rust-lang",
            "typescript",
            "lint"
        ]
    },
    "https://github.com/deel-ai/hijacking-acas": {
        "extra-tags": [
            "experiment"
        ],
        "date": "2021-09-06",
        "title": "hijacking-acas",
        "summary": "Experiment results for the ERTS2022 article",
        "tags": []
    },
    "https://github.com/WindVChen/LEVIR-Ship": {
        "extra-tags": [],
        "date": "2022-06-09",
        "title": "LEVIR-Ship",
        "summary": "This is the official release of LEVIR-Ship, which is a dataset for tiny ship detection under medium-resolution remote sensing images",
        "tags": [
            "remote-sensing",
            "python",
            "tiny-object-detection",
            "dataset",
            "optical-imaging",
            "ship-dataset",
            "ship-detection-dataset",
            "ship-detection"
        ]
    },
    "https://github.com/Volham22/light-lang": {
        "extra-tags": [],
        "date": "2022-05-31",
        "title": "light-lang",
        "summary": "",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/deel-ai/deel-torchlip": {
        "extra-tags": [],
        "date": "2021-07-26",
        "title": "deel-torchlip",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/raphaelsty/gokapi": {
        "extra-tags": [],
        "date": "2022-05-27",
        "title": "gokapi",
        "summary": "Okapi BM25 with Go",
        "tags": [
            "go",
            "bm25",
            "retriever",
            "golang",
            "disk"
        ]
    },
    "https://github.com/osigaud/bbrl": {
        "extra-tags": [
            "rl",
            "library"
        ],
        "date": "2022-05-25",
        "title": "bbrl",
        "summary": "A lightweight RL library inspired from salina",
        "tags": [
            "python"
        ]
    },
    "https://github.com/airbus/scikit-decide": {
        "extra-tags": [],
        "date": "2019-12-20",
        "title": "scikit-decide",
        "summary": "AI framework for Reinforcement Learning, Automated Planning and Scheduling",
        "tags": [
            "decision-making",
            "planning-algorithms",
            "python",
            "reinforcement-learning",
            "scheduling-algorithms",
            "artificial-intelligence"
        ]
    },
    "https://github.com/jangirrishabh/look-closer": {
        "extra-tags": [
            "code",
            "github"
        ],
        "date": "2022-01-19",
        "title": "look-closer",
        "summary": "Code for https://jangirrishabh.github.io/lookcloser/",
        "tags": [
            "python"
        ]
    },
    "https://github.com/phohenecker/switch-cuda": {
        "extra-tags": [],
        "date": "2018-05-15",
        "title": "switch-cuda",
        "summary": "A simple bash script for switching between installed versions of CUDA.",
        "tags": [
            "cuda-toolkit",
            "shell",
            "bash-script"
        ]
    },
    "https://github.com/facebookresearch/natural_rl_environment": {
        "extra-tags": [],
        "date": "2019-05-09",
        "title": "natural_rl_environment",
        "summary": "Natural Environment Benchmarks for Reinforcement Learning",
        "tags": [
            "python"
        ]
    },
    "https://github.com/gemseo/gemseo": {
        "extra-tags": [],
        "date": "2021-07-26",
        "title": "gemseo",
        "summary": "Generic Engine for Multi-disciplinary Scenarios, Exploration and Optimization. This is a MIRROR of our gitlab repository, the development activity and support happen over there.",
        "tags": [
            "python",
            "optmization",
            "optimization-algorithms"
        ]
    },
    "https://github.com/jidiai/Competition_Olympics-Integrated": {
        "extra-tags": [],
        "date": "2022-04-11",
        "title": "Competition_Olympics-Integrated",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/sharkdp/bat": {
        "extra-tags": [],
        "date": "2018-04-21",
        "title": "bat",
        "summary": "A cat(1) clone with wings.",
        "tags": [
            "command-line",
            "terminal",
            "cli",
            "rust",
            "syntax-highlighting",
            "hacktoberfest",
            "git",
            "tool"
        ]
    },
    "https://github.com/SixArm/project-management-rope-estimate": {
        "extra-tags": [
            "project-management",
            "project"
        ],
        "date": "2018-11-04",
        "title": "project-management-rope-estimate",
        "summary": "Project management ROPE\u2122 estimate: realistic estimate, optimistic estimate, pessimistic estimate, equilibristic estimate",
        "tags": []
    },
    "https://github.com/onceupon/Bash-Oneliner": {
        "extra-tags": [],
        "date": "2016-06-14",
        "title": "Bash-Oneliner",
        "summary": "A collection of handy Bash One-Liners and terminal tricks for data processing and Linux system maintenance.",
        "tags": [
            "one-liners",
            "system",
            "bash",
            "shell",
            "shell-oneliner",
            "terminal",
            "xargs",
            "xwindow",
            "hardware",
            "linux",
            "oneliner-commands",
            "variables",
            "linux-administration",
            "data-processing",
            "grep"
        ]
    },
    "https://github.com/tuero/muzero-cpp": {
        "extra-tags": [],
        "date": "2021-11-24",
        "title": "muzero-cpp",
        "summary": "A C++ pytorch implementation of MuZero",
        "tags": [
            "muzero",
            "reinforcement-learning",
            "machine-learning",
            "mcts",
            "libtorch",
            "pytorch",
            "alphazero",
            "cpp",
            "c++"
        ]
    },
    "https://github.com/sansyrox/robyn": {
        "extra-tags": [],
        "date": "2021-06-18",
        "title": "robyn",
        "summary": "Robyn is a fast and extensible async python web server with a rust runtime",
        "tags": [
            "async",
            "python3",
            "python",
            "rust",
            "hacktoberfest",
            "backend"
        ]
    },
    "https://github.com/vict0rsch/PaperMemory": {
        "extra-tags": [],
        "date": "2019-10-16",
        "title": "PaperMemory",
        "summary": "Your browser's reference manager: automatic paper detection (Arxiv, OpenReview & more), publication venue matching and code repository discovery! Also enhances ArXiv: BibTex citation, Markdown link, direct download and more!",
        "tags": [
            "productivity",
            "firefox-addon",
            "chrome-extension",
            "open-source",
            "research",
            "javascript",
            "arxiv-api",
            "code",
            "arxiv",
            "brave",
            "reference-manager",
            "repository",
            "brave-extension",
            "bibtex-citation"
        ]
    },
    "https://github.com/DavidBert/CLOP": {
        "extra-tags": [],
        "date": "2022-04-21",
        "title": "CLOP",
        "summary": "CLOP: Local Feature Swapping for Generalization in Reinforcement Learning",
        "tags": [
            "python"
        ]
    },
    "https://github.com/bloomberg/memray": {
        "extra-tags": [],
        "date": "2022-04-08",
        "title": "memray",
        "summary": "Memray is a memory profiler for Python",
        "tags": [
            "python3",
            "memory-profiler",
            "python",
            "memory-leak-detection",
            "hacktoberfest",
            "memory-leak",
            "memory",
            "profiler"
        ]
    },
    "https://github.com/3outeille/pathtracer-rust": {
        "extra-tags": [
            "naive"
        ],
        "date": "2022-02-24",
        "title": "pathtracer-rust",
        "summary": "A naive Monte Carlo Pathtracer written in Rust",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/LouisRouss/Diffusion-Based-Model-for-Colorization": {
        "extra-tags": [
            "diffusion"
        ],
        "date": "2021-11-30",
        "title": "Diffusion-Based-Model-for-Colorization",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/maciej-sypetkowski/autoascend": {
        "extra-tags": [],
        "date": "2022-03-08",
        "title": "autoascend",
        "summary": "The first place solution for the NeurIPS 2021 Nethack Challenge -- https://www.aicrowd.com/challenges/neurips-2021-the-nethack-challenge",
        "tags": [
            "python"
        ]
    },
    "https://github.com/est31/cargo-udeps": {
        "extra-tags": [
            "find",
            "cargo"
        ],
        "date": "2019-08-26",
        "title": "cargo-udeps",
        "summary": "Find unused dependencies in Cargo.toml",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/pmeier/light-the-torch": {
        "extra-tags": [],
        "date": "2020-07-09",
        "title": "light-the-torch",
        "summary": "Install PyTorch distributions with computation backend auto-detection",
        "tags": [
            "install",
            "python",
            "cuda",
            "pip",
            "pytorch"
        ]
    },
    "https://github.com/David-OConnor/pyflow": {
        "extra-tags": [
            "system",
            "python"
        ],
        "date": "2019-07-15",
        "title": "pyflow",
        "summary": "An installation and dependency system for Python",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/Wilfred/difftastic": {
        "extra-tags": [
            "syntax"
        ],
        "date": "2018-12-18",
        "title": "difftastic",
        "summary": "a structural diff that understands syntax \ud83d\udfe5\ud83d\udfe9",
        "tags": [
            "diff",
            "tree-sitter",
            "rust"
        ]
    },
    "https://github.com/cvxgrp/cvxpylayers": {
        "extra-tags": [
            "differentiable",
            "optimization"
        ],
        "date": "2019-10-27",
        "title": "cvxpylayers",
        "summary": "Differentiable convex optimization layers",
        "tags": [
            "python"
        ]
    },
    "https://github.com/AmineZouitine/VolumeList-Cpp": {
        "extra-tags": [],
        "date": "2022-03-27",
        "title": "VolumeList-Cpp",
        "summary": "\ud83d\uddc4Make a list that has a notion of volume \ud83d\uddc4",
        "tags": [
            "data-structures",
            "cpp",
            "c++"
        ]
    },
    "https://github.com/MehdiZouitine/pybrook": {
        "extra-tags": [],
        "date": "2020-12-22",
        "title": "pybrook",
        "summary": "",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/PyO3/rust-numpy": {
        "extra-tags": [],
        "date": "2017-04-21",
        "title": "rust-numpy",
        "summary": "PyO3-based Rust bindings of the NumPy C-API",
        "tags": [
            "rust-ndarray",
            "rust",
            "numpy",
            "ndarray",
            "rust-bindings",
            "rust-numpy",
            "numpy-capi"
        ]
    },
    "https://github.com/xtma/dsac": {
        "extra-tags": [],
        "date": "2020-01-30",
        "title": "dsac",
        "summary": "Distributional Soft Actor Critic",
        "tags": [
            "python",
            "reinforcement-learning",
            "pytorch"
        ]
    },
    "https://github.com/nicklashansen/dmcontrol-generalization-benchmark": {
        "extra-tags": [
            "generalization",
            "benchmark"
        ],
        "date": "2020-11-17",
        "title": "dmcontrol-generalization-benchmark",
        "summary": "DMControl Generalization Benchmark",
        "tags": [
            "python"
        ]
    },
    "https://github.com/kimbring2/AlphaStar_Implementation": {
        "extra-tags": [
            "project",
            "code"
        ],
        "date": "2019-05-22",
        "title": "AlphaStar_Implementation",
        "summary": "This project is implementation code of AlphaStar",
        "tags": [
            "python"
        ]
    },
    "https://github.com/ma3oun/abc_metric": {
        "extra-tags": [
            "deep"
        ],
        "date": "2022-03-07",
        "title": "abc_metric",
        "summary": "Attribution based Confidence for deep neural networks",
        "tags": [
            "attribution",
            "python",
            "pytorch",
            "confidence-score"
        ]
    },
    "https://github.com/msaroufim/awesome-profiling": {
        "extra-tags": [],
        "date": "2022-02-21",
        "title": "awesome-profiling",
        "summary": "Awesome utilities for performance profiling",
        "tags": []
    },
    "https://github.com/quenhus/uBlock-Origin-dev-filter": {
        "extra-tags": [],
        "date": "2021-12-15",
        "title": "uBlock-Origin-dev-filter",
        "summary": "Filters to block and remove copycat-websites from DuckDuckGo, Google and other search engines. Specific to dev websites like StackOverflow or GitHub.",
        "tags": [
            "ublock-origin",
            "python",
            "dev",
            "ublock"
        ]
    },
    "https://github.com/RobertTLange/evosax": {
        "extra-tags": [
            "strategies",
            "jax"
        ],
        "date": "2020-12-30",
        "title": "evosax",
        "summary": "Evolution Strategies in JAX \ud83e\udd8e",
        "tags": [
            "python"
        ]
    },
    "https://github.com/ma3oun/RPSnet": {
        "extra-tags": [
            "learning",
            "paper"
        ],
        "date": "2021-12-21",
        "title": "RPSnet",
        "summary": "Official Implementation of \"Random Path Selection for Incremental Learning\" paper. NeurIPS 2019",
        "tags": []
    },
    "https://github.com/denisyarats/pytorch_sac": {
        "extra-tags": [],
        "date": "2020-01-22",
        "title": "pytorch_sac",
        "summary": "PyTorch implementation of Soft Actor-Critic (SAC)",
        "tags": [
            "gym",
            "dm-control",
            "actor-critic",
            "reinforcement-learning",
            "sac",
            "continuous-control",
            "soft-actor-critic",
            "mujoco",
            "deep-reinforcement-learning",
            "jupyter notebook",
            "d4pg",
            "deep-learning",
            "pytorch"
        ]
    },
    "https://github.com/stillonearth/CheckersOnBevy": {
        "extra-tags": [],
        "date": "2022-01-13",
        "title": "CheckersOnBevy",
        "summary": "\ud83c\udfc1 Checkers on\ud83e\udd80 Rust and \ud83d\udd4a Bevy;\ud83c\udfcb\ud83c\udfff Gym Environment and\ud83d\udc7e AI Agent based on \ud83c\udf34 Monte Carlo Tree Search Trees with \ud83e\udde0 Neural Heuristics (AlphaZero) on\ud83d\udd25PyTorch",
        "tags": [
            "rust",
            "checkers",
            "bevy"
        ]
    },
    "https://github.com/connorjoleary/DeepCite": {
        "extra-tags": [],
        "date": "2019-10-16",
        "title": "DeepCite",
        "summary": "Traversing links to find the deep source of information",
        "tags": [
            "html",
            "javascript",
            "python",
            "machine-learning"
        ]
    },
    "https://github.com/rail-berkeley/rlkit": {
        "extra-tags": [],
        "date": "2018-01-25",
        "title": "rlkit",
        "summary": "Collection of reinforcement learning algorithms",
        "tags": [
            "python"
        ]
    },
    "https://github.com/eugenevinitsky/robust_RL_multi_adversary": {
        "extra-tags": [],
        "date": "2019-08-28",
        "title": "robust_RL_multi_adversary",
        "summary": "We investigate the effect of populations on finding good solutions to the robust MDP",
        "tags": [
            "python"
        ]
    },
    "https://github.com/facebookresearch/svg": {
        "extra-tags": [],
        "date": "2021-05-08",
        "title": "svg",
        "summary": "On the model-based stochastic value gradient for continuous reinforcement learning",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/SuReLI/DiCyR_code": {
        "extra-tags": [
            "code"
        ],
        "date": "2020-10-07",
        "title": "DiCyR_code",
        "summary": "Code for DiCyR: Disentangled  Cyclic  Reconstruction for domain adaptation",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/LABouteille/torchcompress": {
        "extra-tags": [],
        "date": "2021-09-26",
        "title": "torchcompress",
        "summary": "Deep learning compression framework in Pytorch [WIP]",
        "tags": [
            "python",
            "quantization",
            "deep-learning",
            "knowledge-distillation",
            "model-compression",
            "pruning"
        ]
    },
    "https://github.com/CMA-ES/pycma": {
        "extra-tags": [
            "cma-es"
        ],
        "date": "2016-09-22",
        "title": "pycma",
        "summary": "Python implementation of CMA-ES",
        "tags": [
            "python"
        ]
    },
    "https://github.com/ma3oun/cl_datasets": {
        "extra-tags": [
            "datasets"
        ],
        "date": "2022-01-11",
        "title": "cl_datasets",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/Farama-Foundation/Gymnasium-Robotics": {
        "extra-tags": [],
        "date": "2021-10-25",
        "title": "Gymnasium-Robotics",
        "summary": "A collection of robotics simulation environments for reinforcement learning",
        "tags": [
            "python"
        ]
    },
    "https://github.com/tauri-apps/tauri": {
        "extra-tags": [],
        "date": "2019-07-13",
        "title": "tauri",
        "summary": "Build smaller, faster, and more secure desktop applications with a web frontend.",
        "tags": [
            "works-with-quasar",
            "works-with-vue",
            "works-with-react",
            "works-with-reason",
            "rust",
            "works-with-elm",
            "works-with-gatsby",
            "works-with-svelte",
            "webview",
            "works-with-construct",
            "works-with-flutter",
            "hacktoberfest",
            "high-performance",
            "works-with-mint",
            "works-with-phaser",
            "works-with-clojurescript",
            "works-with-yew"
        ]
    },
    "https://github.com/tengbao/vanta": {
        "extra-tags": [],
        "date": "2017-07-18",
        "title": "vanta",
        "summary": "Animated 3D backgrounds for your website",
        "tags": [
            "animation",
            "threejs",
            "background",
            "animations",
            "javascript",
            "3d",
            "three-js"
        ]
    },
    "https://github.com/PyO3/pyo3": {
        "extra-tags": [],
        "date": "2017-05-13",
        "title": "pyo3",
        "summary": "Rust bindings for the Python interpreter",
        "tags": [
            "python",
            "rust",
            "binding",
            "python-c-api",
            "ffi"
        ]
    },
    "https://github.com/MrRobb/gym-rs": {
        "extra-tags": [],
        "date": "2019-08-20",
        "title": "gym-rs",
        "summary": "OpenAI Gym bindings for Rust",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/MathisWellmann/gym-rs": {
        "extra-tags": [],
        "date": "2020-09-30",
        "title": "gym-rs",
        "summary": "OpenAI's Gym written in pure Rust for blazingly fast performance",
        "tags": [
            "reinforcement-learning",
            "rust",
            "ai",
            "openai-gym",
            "ml",
            "rl"
        ]
    },
    "https://github.com/Instagram/MonkeyType": {
        "extra-tags": [],
        "date": "2017-07-11",
        "title": "MonkeyType",
        "summary": "A Python library that generates static type annotations by collecting runtime types",
        "tags": [
            "python"
        ]
    },
    "https://github.com/edbeeching/godot_rl_agents": {
        "extra-tags": [],
        "date": "2021-07-01",
        "title": "godot_rl_agents",
        "summary": "An Open Source package that allows video game creators, AI researchers and hobbyists the opportunity to learn complex behaviors for their Non Player Characters or agents",
        "tags": [
            "reinforcement-learning",
            "simulation",
            "python",
            "godot"
        ]
    },
    "https://github.com/MaxHalford/svg2stl": {
        "extra-tags": [
            "svg",
            "stencil"
        ],
        "date": "2021-12-22",
        "title": "svg2stl",
        "summary": "\ud83d\udef9 Turn an SVG into an STL for stencil creation purposes",
        "tags": [
            "python"
        ]
    },
    "https://github.com/entity-neural-network/incubator": {
        "extra-tags": [
            "collection"
        ],
        "date": "2021-11-23",
        "title": "incubator",
        "summary": "Collection of in-progress libraries for entity neural networks.",
        "tags": []
    },
    "https://github.com/heavenshell/vim-pydocstring": {
        "extra-tags": [],
        "date": "2012-01-18",
        "title": "vim-pydocstring",
        "summary": "Generate Python docstring to your Python source code.",
        "tags": [
            "vim",
            "vim script",
            "python",
            "docstring"
        ]
    },
    "https://github.com/ropas/pytea": {
        "extra-tags": [],
        "date": "2020-12-16",
        "title": "pytea",
        "summary": "PyTea: PyTorch Tensor shape error analyzer",
        "tags": [
            "typescript",
            "static-analysis",
            "machine-learning",
            "pytorch"
        ]
    },
    "https://github.com/3outeille/dotfiles": {
        "extra-tags": [
            "dotfiles"
        ],
        "date": "2021-01-31",
        "title": "dotfiles",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/mchong6/JoJoGAN": {
        "extra-tags": [
            "pytorch"
        ],
        "date": "2021-12-17",
        "title": "JoJoGAN",
        "summary": "Official PyTorch repo for JoJoGAN: One Shot Face Stylization",
        "tags": [
            "anime",
            "jupyter notebook",
            "gans",
            "image-translation"
        ]
    },
    "https://github.com/neoclide/coc.nvim": {
        "extra-tags": [],
        "date": "2018-05-01",
        "title": "coc.nvim",
        "summary": "Nodejs extension host for vim & neovim, load extensions like VSCode and host language servers.",
        "tags": [
            "vim",
            "vim-plugin",
            "lsp",
            "language-client",
            "typescript",
            "nvim",
            "neovim-plugin",
            "autocompletion"
        ]
    },
    "https://github.com/ducfilan/Dark-mode-Franz-Ferdi": {
        "extra-tags": [],
        "date": "2019-01-31",
        "title": "Dark-mode-Franz-Ferdi",
        "summary": "Support Dark mode for Franz and Ferdi's services (Facebook messenger, Workplace, Slack, Whatsapp etc.)",
        "tags": [
            "whatsapp",
            "slack",
            "workplace",
            "telegram",
            "css",
            "franz-services",
            "dark-theme",
            "facebook-messenger",
            "franz",
            "ferdi",
            "ferdi-services"
        ]
    },
    "https://github.com/ogham/exa": {
        "extra-tags": [
            "modern"
        ],
        "date": "2014-05-22",
        "title": "exa",
        "summary": "A modern replacement for \u2018ls\u2019.",
        "tags": [
            "rust",
            "files",
            "ls",
            "command-line"
        ]
    },
    "https://github.com/obskyr/colorgram.py": {
        "extra-tags": [],
        "date": "2016-09-14",
        "title": "colorgram.py",
        "summary": "A Python module for extracting colors from images. Get a palette of any picture!",
        "tags": [
            "color",
            "python",
            "image-processing",
            "color-extraction",
            "pillow",
            "python-library"
        ]
    },
    "https://github.com/xinntao/Real-ESRGAN": {
        "extra-tags": [],
        "date": "2021-07-19",
        "title": "Real-ESRGAN",
        "summary": "Real-ESRGAN aims at developing Practical Algorithms for General Image/Video Restoration.",
        "tags": [
            "esrgan",
            "amine",
            "real-esrgan",
            "super-resolution",
            "python",
            "denoise",
            "jpeg-compression",
            "pytorch",
            "image-restoration"
        ]
    },
    "https://github.com/lapce/lapce": {
        "extra-tags": [],
        "date": "2018-02-06",
        "title": "lapce",
        "summary": "Lightning-fast and Powerful Code Editor written in Rust",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/apple/ml-core": {
        "extra-tags": [
            "ml"
        ],
        "date": "2021-11-29",
        "title": "ml-core",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/kengz/SLM-Lab": {
        "extra-tags": [],
        "date": "2017-10-02",
        "title": "SLM-Lab",
        "summary": "Modular Deep Reinforcement Learning framework in PyTorch. Companion library of the book \"Foundations of Deep Reinforcement Learning\".",
        "tags": [
            "a3c",
            "python",
            "reinforcement-learning",
            "a2c",
            "benchmark",
            "sac",
            "dqn",
            "deep-reinforcement-learning",
            "policy-gradient",
            "pytorch",
            "ppo"
        ]
    },
    "https://github.com/irom-lab/Invariant-Policy-Optimization": {
        "extra-tags": [],
        "date": "2020-07-07",
        "title": "Invariant-Policy-Optimization",
        "summary": "Code for Invariant Policy Optimization",
        "tags": [
            "python"
        ]
    },
    "https://github.com/k2bd/action-python-poetry": {
        "extra-tags": [],
        "date": "2021-12-04",
        "title": "action-python-poetry",
        "summary": "Template repo to quickly make a tested and documented GitHub action in Python with Poetry",
        "tags": [
            "python3",
            "python",
            "actions",
            "poetry",
            "poetry-python",
            "template",
            "templates",
            "github-actions"
        ]
    },
    "https://github.com/kngwyu/rogue-gym": {
        "extra-tags": [],
        "date": "2018-04-20",
        "title": "rogue-gym",
        "summary": "[WIP] Highly customizable rogue-like game for AI expmeriments",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/Rust-GPU/Rust-CUDA": {
        "extra-tags": [],
        "date": "2021-10-17",
        "title": "Rust-CUDA",
        "summary": "Ecosystem of libraries and tools for writing and executing fast GPU code fully in Rust.",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/befelix/safe_learning": {
        "extra-tags": [],
        "date": "2017-11-03",
        "title": "safe_learning",
        "summary": "Safe reinforcement learning with stability guarantees",
        "tags": [
            "safety",
            "reinforcement-learning",
            "python",
            "dynamic-programming",
            "gaussian-processes",
            "stability"
        ]
    },
    "https://github.com/fel-thomas/Sobol-Attribution-Method": {
        "extra-tags": [],
        "date": "2021-05-28",
        "title": "Sobol-Attribution-Method",
        "summary": "\ud83d\udc4b Code for the paper: \"Look at the Variance! Efficient Black-box Explanations with Sobol-based Sensitivity Analysis\" (NeurIPS 2021)",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/starship/starship": {
        "extra-tags": [],
        "date": "2019-04-02",
        "title": "starship",
        "summary": "\u2604\ud83c\udf0c\ufe0f  The minimal, blazing-fast, and infinitely customizable prompt for any shell!",
        "tags": [
            "oh-my-zsh",
            "zsh-prompt",
            "starship",
            "powershell",
            "rust",
            "zsh",
            "zsh-theme",
            "fish",
            "shell-prompt",
            "fish-theme",
            "bash",
            "fish-prompt"
        ]
    },
    "https://github.com/tldr-pages/tldr": {
        "extra-tags": [],
        "date": "2013-12-08",
        "title": "tldr",
        "summary": "\ud83d\udcda Collaborative cheatsheets for console commands",
        "tags": [
            "console",
            "shell",
            "command-line",
            "terminal",
            "examples",
            "man-page",
            "hacktoberfest",
            "manual",
            "markdown",
            "manpages",
            "documentation",
            "tldr",
            "help"
        ]
    },
    "https://github.com/terrencepreilly/darglint": {
        "extra-tags": [],
        "date": "2017-09-25",
        "title": "darglint",
        "summary": "A python documentation linter which checks that the docstring description matches the definition.",
        "tags": [
            "documentation-tool",
            "python",
            "linter"
        ]
    },
    "https://github.com/aqlaboratory/openfold": {
        "extra-tags": [],
        "date": "2021-09-14",
        "title": "openfold",
        "summary": "Trainable, memory-efficient, and GPU-friendly PyTorch reproduction of AlphaFold 2",
        "tags": [
            "protein-structure",
            "python",
            "alphafold2",
            "pytorch"
        ]
    },
    "https://github.com/neovide/neovide": {
        "extra-tags": [],
        "date": "2019-12-06",
        "title": "neovide",
        "summary": "No Nonsense Neovim Client in Rust",
        "tags": [
            "skia",
            "gpu",
            "rust",
            "neovim-guis",
            "neovim"
        ]
    },
    "https://github.com/ryanoasis/nerd-fonts": {
        "extra-tags": [],
        "date": "2014-12-05",
        "title": "nerd-fonts",
        "summary": "Iconic font aggregator, collection, & patcher. 3,600+ icons, 50+ patched fonts: Hack, Source Code Pro, more. Glyph collections: Font Awesome, Material Design Icons, Octicons, & more",
        "tags": [
            "patcher",
            "python",
            "shell",
            "statusline",
            "iconic-fonts",
            "patched-fonts",
            "fonts",
            "css",
            "font",
            "hacktoberfest",
            "font-awesome",
            "octicons",
            "powerline",
            "icon-font"
        ]
    },
    "https://github.com/koaning/doubtlab": {
        "extra-tags": [
            "data",
            "find"
        ],
        "date": "2021-11-05",
        "title": "doubtlab",
        "summary": "Doubt your data, find bad labels. ",
        "tags": [
            "python"
        ]
    },
    "https://github.com/sail-sg/envpool": {
        "extra-tags": [],
        "date": "2021-10-20",
        "title": "envpool",
        "summary": "C++-based high-performance parallel environment execution engine (vectorized env) for general RL environments.",
        "tags": [
            "gym",
            "dm-control",
            "cpp17",
            "reinforcement-learning",
            "reinforcement-learning-environments",
            "threadpool",
            "dm-env",
            "mujoco",
            "atari-games",
            "parallel-processing",
            "vizdoom",
            "box2d",
            "robotics",
            "lock-free-queue",
            "c++",
            "pybind11",
            "high-performance-computing"
        ]
    },
    "https://github.com/MiscellaneousStuff/tlol": {
        "extra-tags": [],
        "date": "2021-08-23",
        "title": "tlol",
        "summary": "TLoL - League of Legends Deep Learning AI (Research and Development)",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/kaizouman/gtest-cmake-example": {
        "extra-tags": [],
        "date": "2014-12-24",
        "title": "gtest-cmake-example",
        "summary": "A sample project using GoogleTest with CMake",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/AntoineD/docstring-inheritance": {
        "extra-tags": [],
        "date": "2021-11-04",
        "title": "docstring-inheritance",
        "summary": "A python package to avoid writing and maintaining duplicated python docstrings.",
        "tags": [
            "python",
            "docstrings"
        ]
    },
    "https://github.com/jdegre/5GC_APIs": {
        "extra-tags": [],
        "date": "2018-05-30",
        "title": "5GC_APIs",
        "summary": "RESTful APIs of main Network Functions in the 3GPP 5G Core Network",
        "tags": [
            "5g",
            "openapi",
            "restful",
            "swagger",
            "3gpp"
        ]
    },
    "https://github.com/airbus/decomon": {
        "extra-tags": [],
        "date": "2021-02-25",
        "title": "decomon",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/csurfer/pyheat": {
        "extra-tags": [],
        "date": "2017-02-04",
        "title": "pyheat",
        "summary": "pprofile + matplotlib = Python program profiled as an awesome heatmap!",
        "tags": [
            "profiling",
            "matplotlib",
            "python",
            "heatmap"
        ]
    },
    "https://github.com/YeWR/EfficientZero": {
        "extra-tags": [],
        "date": "2021-10-21",
        "title": "EfficientZero",
        "summary": "Open-source codebase for EfficientZero, from \"Mastering Atari Games with Limited Data\" at NeurIPS 2021.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/uclnlp/torch-imle": {
        "extra-tags": [
            "torch"
        ],
        "date": "2021-10-24",
        "title": "torch-imle",
        "summary": "Implicit MLE: Backpropagating Through Discrete Exponential Family Distributions",
        "tags": [
            "python"
        ]
    },
    "https://github.com/github/copilot-docs": {
        "extra-tags": [],
        "date": "2021-10-23",
        "title": "copilot-docs",
        "summary": "Documentation for GitHub Copilot",
        "tags": []
    },
    "https://github.com/VITA-Group/AugMax": {
        "extra-tags": [
            "adversarial",
            "training"
        ],
        "date": "2021-09-28",
        "title": "AugMax",
        "summary": "[NeurIPS'21] \"AugMax: Adversarial Composition of Random Augmentations for Robust Training\" by Haotao Wang, Chaowei Xiao, Jean Kossaifi, Zhiding Yu, Animashree Anandkumar, and Zhangyang Wang.",
        "tags": [
            "python",
            "adversarial-learning",
            "normalization-techniques",
            "out-of-distribution",
            "model-robustness",
            "imagenet-c"
        ]
    },
    "https://github.com/mercari/ml-system-design-pattern": {
        "extra-tags": [],
        "date": "2020-04-22",
        "title": "ml-system-design-pattern",
        "summary": "System design patterns for machine learning",
        "tags": []
    },
    "https://github.com/google/ml_collections": {
        "extra-tags": [],
        "date": "2020-08-20",
        "title": "ml_collections",
        "summary": "ML Collections is a library of Python Collections designed for ML use cases.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/shawwn/pytreez": {
        "extra-tags": [
            "jax"
        ],
        "date": "2021-07-22",
        "title": "pytreez",
        "summary": "An implementation of Jax pytrees in pure python",
        "tags": [
            "python"
        ]
    },
    "https://github.com/deepmind/mujoco": {
        "extra-tags": [],
        "date": "2021-08-27",
        "title": "mujoco",
        "summary": "Multi-Joint dynamics with Contact. A general purpose physics simulator.",
        "tags": [
            "physics",
            "mujoco",
            "robotics",
            "c"
        ]
    },
    "https://github.com/cresset-template/cresset": {
        "extra-tags": [],
        "date": "2021-09-16",
        "title": "cresset",
        "summary": "Template repository to build PyTorch projects from source on any version of PyTorch/CUDA/cuDNN.",
        "tags": [
            "docker",
            "python",
            "template-repository",
            "build",
            "cuda",
            "dockerfile",
            "source-python",
            "machine-learning",
            "mlops",
            "deep-learning-tutorial",
            "mlops-template",
            "deep-learning",
            "makefile",
            "docker-compose",
            "pytorch",
            "wheel",
            "source",
            "template"
        ]
    },
    "https://github.com/jendrikseipp/vulture": {
        "extra-tags": [],
        "date": "2017-03-06",
        "title": "vulture",
        "summary": "Find dead Python code",
        "tags": [
            "dead-code-removal",
            "python"
        ]
    },
    "https://github.com/abatilo/actions-poetry": {
        "extra-tags": [],
        "date": "2019-05-30",
        "title": "actions-poetry",
        "summary": "GitHub Actions for Python projects using poetry",
        "tags": []
    },
    "https://github.com/neovim/neovim": {
        "extra-tags": [],
        "date": "2014-01-31",
        "title": "neovim",
        "summary": "Vim-fork focused on extensibility and usability",
        "tags": [
            "vim",
            "lua",
            "c",
            "nvim",
            "api",
            "text-editor",
            "vim script",
            "neovim"
        ]
    },
    "https://github.com/samrocketman/gitlab-mirrors": {
        "extra-tags": [
            "set"
        ],
        "date": "2013-09-11",
        "title": "gitlab-mirrors",
        "summary": "A set of scripts adding the ability of managing remote mirrors to GitLab.",
        "tags": [
            "mirrored-repositories",
            "gitlab",
            "replication",
            "shell",
            "mirroring",
            "github-backups",
            "gitlab-mirror",
            "backups"
        ]
    },
    "https://github.com/lucashervier/aibt-slides": {
        "extra-tags": [
            "slides",
            "presentation"
        ],
        "date": "2021-10-06",
        "title": "aibt-slides",
        "summary": "Presentation Materials on the Hands-On module of AIBT",
        "tags": [
            "javascript"
        ]
    },
    "https://github.com/google/jaxopt": {
        "extra-tags": [],
        "date": "2021-07-12",
        "title": "jaxopt",
        "summary": "Hardware accelerated, batchable and differentiable optimizers in JAX.",
        "tags": [
            "python",
            "differentiable-programming",
            "jax",
            "bi-level",
            "optimization",
            "deep-learning"
        ]
    },
    "https://github.com/openai/ppo-ewma": {
        "extra-tags": [],
        "date": "2021-08-23",
        "title": "ppo-ewma",
        "summary": "Code for the paper \"Batch size invariance for policy optimization\"",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/unlimblue/KNN_CUDA": {
        "extra-tags": [
            "pytorch"
        ],
        "date": "2018-11-20",
        "title": "KNN_CUDA",
        "summary": "pytorch knn [cuda version]",
        "tags": [
            "cuda",
            "pytorch-knn",
            "knn-cuda"
        ]
    },
    "https://github.com/pytorch/data": {
        "extra-tags": [
            "data",
            "pytorch"
        ],
        "date": "2021-05-12",
        "title": "data",
        "summary": "A PyTorch repo for data loading and utilities to be shared by the PyTorch domain libraries.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/ratansingh98/CPP-Learning": {
        "extra-tags": [],
        "date": "2019-10-25",
        "title": "CPP-Learning",
        "summary": "This repo contains my roadmap for learning C++ from zero to hero.",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/deel-ai/xplique": {
        "extra-tags": [
            "explainability",
            "toolbox"
        ],
        "date": "2020-04-05",
        "title": "xplique",
        "summary": "\ud83d\udc4b Xplique is a Neural Networks Explainability Toolbox",
        "tags": [
            "explainable-ml",
            "python",
            "explainable-ai",
            "xai",
            "interpretability"
        ]
    },
    "https://github.com/cyanrain7/TRPO-in-MARL": {
        "extra-tags": [
            "trpo",
            "marl"
        ],
        "date": "2021-09-23",
        "title": "TRPO-in-MARL",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/scikit-hep/awkward": {
        "extra-tags": [],
        "date": "2019-08-14",
        "title": "awkward",
        "summary": "Manipulate JSON-like data with NumPy-like idioms.",
        "tags": [
            "jagged-array",
            "python",
            "apache-arrow",
            "data-analysis",
            "cern-root",
            "numpy",
            "columnar-format",
            "numba",
            "json",
            "pandas",
            "ragged-array",
            "scikit-hep"
        ]
    },
    "https://github.com/pre-commit/pre-commit": {
        "extra-tags": [],
        "date": "2014-03-13",
        "title": "pre-commit",
        "summary": "A framework for managing and maintaining multi-language pre-commit hooks.",
        "tags": [
            "python",
            "refactoring",
            "linter",
            "pre-commit",
            "git"
        ]
    },
    "https://github.com/danijar/crafter": {
        "extra-tags": [
            "benchmarking",
            "agent"
        ],
        "date": "2021-03-10",
        "title": "crafter",
        "summary": "Benchmarking the Spectrum of Agent Capabilities",
        "tags": [
            "python",
            "reinforcement-learning",
            "simulation",
            "minecraft",
            "deep-learning",
            "environment",
            "artificial-intelligence"
        ]
    },
    "https://github.com/openai/procgen": {
        "extra-tags": [],
        "date": "2019-11-22",
        "title": "procgen",
        "summary": "Procgen Benchmark: Procedurally-Generated Game-Like Gym-Environments",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/alexrame/fishr": {
        "extra-tags": [],
        "date": "2021-09-06",
        "title": "fishr",
        "summary": "Official PyTorch implementation of the Fishr regularization for out-of-distribution generalization",
        "tags": [
            "python"
        ]
    },
    "https://github.com/aadharna/UntouchableThunder": {
        "extra-tags": [
            "agents",
            "ai"
        ],
        "date": "2019-07-30",
        "title": "UntouchableThunder",
        "summary": "Co-evolution of agents and environments in GVG-AI ",
        "tags": [
            "python",
            "reinforcement-learning",
            "coevolution",
            "neuroevolution",
            "poet",
            "open-ended-evolution",
            "transfer-learning"
        ]
    },
    "https://github.com/Visual-Behavior/aloception-oss": {
        "extra-tags": [],
        "date": "2021-09-06",
        "title": "aloception-oss",
        "summary": "Aloception is a set of package for computer vision: aloscene, alodataset, alonet.",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/liuruoze/mini-AlphaStar": {
        "extra-tags": [],
        "date": "2021-01-31",
        "title": "mini-AlphaStar",
        "summary": "A mini-scale reproduction code of the AlphaStar program. Note: the original AlphaStar is the AI proposed by DeepMind to play StarCraft II.",
        "tags": [
            "self-playing-bot",
            "python",
            "reinforcement-learning",
            "sc2replay",
            "starcraft2-ai",
            "multi-agent-systems",
            "starcraft2",
            "deep-reinforcement-learning",
            "deep-neural-networks",
            "imitation-learning",
            "gaming",
            "deep-learning",
            "mini-alphastar",
            "pytorch",
            "starcraft-ii-bot",
            "supervised-learning"
        ]
    },
    "https://github.com/gvwilson/10-lesson": {
        "extra-tags": [],
        "date": "2018-09-04",
        "title": "10-lesson",
        "summary": "Ten Quick Tips for Creating an Effective Lesson",
        "tags": [
            "tex"
        ]
    },
    "https://github.com/gvwilson/12-design": {
        "extra-tags": [],
        "date": "2021-07-05",
        "title": "12-design",
        "summary": "Ten Quick Software Design Tips for Data Scientists",
        "tags": [
            "perl"
        ]
    },
    "https://github.com/dblalock/bolt": {
        "extra-tags": [
            "matrix",
            "vector"
        ],
        "date": "2017-02-01",
        "title": "bolt",
        "summary": "10x faster matrix and vector operations",
        "tags": [
            "data-mining",
            "compression",
            "machine-learning",
            "database",
            "c++"
        ]
    },
    "https://github.com/enthought/traits": {
        "extra-tags": [],
        "date": "2011-01-28",
        "title": "traits",
        "summary": "Observable typed attributes for Python classes",
        "tags": [
            "types",
            "python",
            "observer-pattern",
            "dataclasses",
            "runtime-typechecking",
            "gui",
            "attributes"
        ]
    },
    "https://github.com/google-research/rliable": {
        "extra-tags": [],
        "date": "2021-08-20",
        "title": "rliable",
        "summary": "[NeurIPS'21 Outstanding Paper] Library for reliable evaluation on RL and ML benchmarks, even with only a handful of seeds.",
        "tags": [
            "reinforcement-learning",
            "google",
            "evaluation-metrics",
            "machine-learning",
            "jupyter notebook",
            "benchmarking",
            "rl"
        ]
    },
    "https://github.com/mathlibrary/usimd": {
        "extra-tags": [],
        "date": "2020-09-08",
        "title": "usimd",
        "summary": "Cross platform portable accelerate math library using universal intrinsics.",
        "tags": [
            "c"
        ]
    },
    "https://github.com/filipdutescu/modern-cpp-template": {
        "extra-tags": [],
        "date": "2020-05-03",
        "title": "modern-cpp-template",
        "summary": "A template for modern C++ projects using CMake, Clang-Format, CI, unit testing and more, with support for downstream inclusion.",
        "tags": [
            "ccache",
            "cmake-template",
            "open-source",
            "code-coverage",
            "github-action",
            "cpp",
            "ci",
            "cmake",
            "gtest",
            "template",
            "cmakelists",
            "codecov",
            "static-analysis",
            "google-test",
            "package-manager",
            "project-template",
            "clang-format",
            "github-actions",
            "continuous-integration",
            "cmake-module"
        ]
    },
    "https://github.com/RustPython/RustPython": {
        "extra-tags": [],
        "date": "2018-05-28",
        "title": "RustPython",
        "summary": "A Python Interpreter written in Rust",
        "tags": [
            "python3",
            "compiler",
            "wasm",
            "rust",
            "hacktoberfest",
            "python-language",
            "language",
            "jit",
            "interpreter"
        ]
    },
    "https://github.com/marzer/poxy": {
        "extra-tags": [
            "documentation",
            "c++"
        ],
        "date": "2021-03-22",
        "title": "poxy",
        "summary": "Documentation generator for C++",
        "tags": [
            "doxygen",
            "cpp",
            "python",
            "c-plus-plus"
        ]
    },
    "https://github.com/MaxHalford/yamp": {
        "extra-tags": [
            "mkdocs",
            "parser"
        ],
        "date": "2021-08-21",
        "title": "yamp",
        "summary": "Yet Another MkDocs Parser",
        "tags": [
            "python"
        ]
    },
    "https://github.com/HomebrewNLP/revlib": {
        "extra-tags": [],
        "date": "2021-08-17",
        "title": "revlib",
        "summary": "Simple and efficient RevNet-Library for PyTorch with XLA and DeepSpeed support and parameter offload",
        "tags": [
            "revnet",
            "python",
            "momentumnet",
            "xla",
            "deepspeed",
            "pytorch",
            "deep-learning",
            "tpu"
        ]
    },
    "https://github.com/toshas/torch-discounted-cumsum": {
        "extra-tags": [],
        "date": "2020-12-30",
        "title": "torch-discounted-cumsum",
        "summary": "Fast Discounted Cumulative Sums in PyTorch",
        "tags": [
            "python",
            "reinforcement-learning",
            "reinforce",
            "discounted-cumulative-sum",
            "pytorch",
            "rl"
        ]
    },
    "https://github.com/omardrwch/rlly": {
        "extra-tags": [],
        "date": "2020-03-02",
        "title": "rlly",
        "summary": "A C++ library for reinforcement learning environments",
        "tags": [
            "c++",
            "reinforcement-learning-environments",
            "reinforcement-learning"
        ]
    },
    "https://github.com/Lightning-AI/metrics": {
        "extra-tags": [],
        "date": "2020-12-22",
        "title": "metrics",
        "summary": "Machine learning metrics for distributed, scalable PyTorch applications.",
        "tags": [
            "python",
            "metrics",
            "machine-learning",
            "data-science",
            "pytorch",
            "deep-learning",
            "analyses"
        ]
    },
    "https://github.com/google/googletest": {
        "extra-tags": [],
        "date": "2015-07-28",
        "title": "googletest",
        "summary": "GoogleTest - Google Testing and Mocking Framework",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/NVIDIA/cuda-python": {
        "extra-tags": [],
        "date": "2021-06-28",
        "title": "cuda-python",
        "summary": "CUDA Python Low-level Bindings",
        "tags": [
            "python"
        ]
    },
    "https://github.com/raphaelsty/textokb": {
        "extra-tags": [
            "knowledge",
            "text"
        ],
        "date": "2021-08-11",
        "title": "textokb",
        "summary": "Extract knowledge from raw text",
        "tags": [
            "relation-extraction",
            "python",
            "transformer",
            "luke",
            "spacy",
            "opennre"
        ]
    },
    "https://github.com/lezcano/geotorch": {
        "extra-tags": [
            "optimization"
        ],
        "date": "2020-02-19",
        "title": "geotorch",
        "summary": "Constrained optimization toolkit for PyTorch",
        "tags": [
            "constrained-optimization",
            "positive-definite-matrices",
            "python",
            "manifold-optimization",
            "invertible-neural-networks",
            "positive-semi-definite",
            "orthogonality",
            "pytorch",
            "low-rank"
        ]
    },
    "https://github.com/Huage001/PaintTransformer": {
        "extra-tags": [],
        "date": "2021-08-09",
        "title": "PaintTransformer",
        "summary": "Officially unofficial re-implementation of paper: Paint Transformer: Feed Forward Neural Painting with Stroke Prediction, ICCV 2021. ",
        "tags": [
            "python"
        ]
    },
    "https://github.com/PrithivirajDamodaran/Styleformer": {
        "extra-tags": [],
        "date": "2021-06-12",
        "title": "Styleformer",
        "summary": "A Neural Language Style Transfer framework to transfer natural language text smoothly between fine-grained language styles like formal/casual, active/passive, and many more. Created by Prithiviraj Damodaran. Open to pull requests and other forms of collaboration.",
        "tags": [
            "informal-sentences",
            "python",
            "passive",
            "slang",
            "style-transfer",
            "text-style-transfer",
            "nlp",
            "formal-languages",
            "text-style-transfer-benchmark",
            "text-style",
            "active"
        ]
    },
    "https://github.com/lucas-emery/rocket-league-gym": {
        "extra-tags": [],
        "date": "2020-10-15",
        "title": "rocket-league-gym",
        "summary": "A Gym-like environment for Reinforcement Learning in Rocket League",
        "tags": [
            "python",
            "reinforcement-learning",
            "gym-environment",
            "hacktoberfest",
            "rocket-league"
        ]
    },
    "https://github.com/deepmind/meltingpot": {
        "extra-tags": [],
        "date": "2021-07-16",
        "title": "meltingpot",
        "summary": "A suite of test scenarios for multi-agent reinforcement learning.",
        "tags": [
            "multiagent-reinforcement-learning",
            "python"
        ]
    },
    "https://github.com/MehdiZouitine/Learning-Disentangled-Representations-via-Mutual-Information-Estimation": {
        "extra-tags": [],
        "date": "2021-07-18",
        "title": "Learning-Disentangled-Representations-via-Mutual-Information-Estimation",
        "summary": "Pytorch implementation of Learning Disentangled Representations via Mutual Information Estimation (ECCV 2020)",
        "tags": [
            "python",
            "generative-adversarial-network",
            "representation-learning",
            "mutual-information",
            "disentanglement",
            "pytorch",
            "deep-learning",
            "gan",
            "disentangled-representations"
        ]
    },
    "https://github.com/tfeldmann/organize": {
        "extra-tags": [
            "automation",
            "tool"
        ],
        "date": "2017-09-20",
        "title": "organize",
        "summary": "The file management automation tool.",
        "tags": [
            "python3",
            "rule-based",
            "python",
            "document-management",
            "platform-independent",
            "command-line-tool",
            "file-management",
            "automatic"
        ]
    },
    "https://github.com/nektos/act": {
        "extra-tags": [
            "github",
            "actions"
        ],
        "date": "2019-01-02",
        "title": "act",
        "summary": "Run your GitHub Actions locally \ud83d\ude80",
        "tags": [
            "ci",
            "go",
            "devops",
            "golang",
            "github-actions"
        ]
    },
    "https://github.com/google/brax": {
        "extra-tags": [],
        "date": "2021-06-02",
        "title": "brax",
        "summary": "Massively parallel rigidbody physics simulation on accelerator hardware.",
        "tags": [
            "reinforcement-learning",
            "jax",
            "physics-simulation",
            "jupyter notebook",
            "robotics"
        ]
    },
    "https://github.com/arnemertz/docker4c": {
        "extra-tags": [],
        "date": "2021-05-21",
        "title": "docker4c",
        "summary": "Docker container with compilers and tooling for basic C++ projects",
        "tags": [
            "docker",
            "cpp",
            "dockerfile"
        ]
    },
    "https://github.com/hosseinmoein/DataFrame": {
        "extra-tags": [],
        "date": "2017-10-28",
        "title": "DataFrame",
        "summary": "C++ DataFrame for statistical, Financial, and ML analysis -- in modern C++ using native types and contiguous memory storage",
        "tags": [
            "heterogeneous-data",
            "statistical",
            "financial-engineering",
            "machine-learning",
            "cpp",
            "dataframe",
            "data-analysis",
            "financial-data-analysis",
            "statistical-analysis",
            "datascience",
            "efficient-implementations",
            "data-science",
            "tensorboard",
            "trading-strategies",
            "tensor",
            "numerical-analysis",
            "trading-algorithms",
            "multidimensional-data",
            "c++",
            "large-data"
        ]
    },
    "https://github.com/wzchen/probability_cheatsheet": {
        "extra-tags": [
            "page",
            "cheatsheet"
        ],
        "date": "2014-07-13",
        "title": "probability_cheatsheet",
        "summary": "A comprehensive 10-page probability cheatsheet that covers a semester's worth of introduction to probability.",
        "tags": [
            "tex"
        ]
    },
    "https://github.com/actions/cache": {
        "extra-tags": [],
        "date": "2019-10-16",
        "title": "cache",
        "summary": "Cache dependencies and build outputs in GitHub Actions",
        "tags": [
            "typescript"
        ]
    },
    "https://github.com/causal-rl-anonymous/causal-rl": {
        "extra-tags": [
            "rl"
        ],
        "date": "2021-05-28",
        "title": "causal-rl",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/ma3oun/torch-lego": {
        "extra-tags": [],
        "date": "2021-07-16",
        "title": "torch-lego",
        "summary": "Build pytorch modules using yaml descriptions",
        "tags": [
            "python"
        ]
    },
    "https://github.com/learning-at-home/hivemind": {
        "extra-tags": [],
        "date": "2020-02-27",
        "title": "hivemind",
        "summary": "Decentralized deep learning in PyTorch. Built to train models on thousands of volunteers across the world.",
        "tags": [
            "python",
            "neural-networks",
            "asynchronous-programming",
            "machine-learning",
            "dht",
            "hivemind",
            "volunteer-computing",
            "asyncio",
            "distributed-systems",
            "deep-learning",
            "mixture-of-experts",
            "pytorch",
            "distributed-training"
        ]
    },
    "https://github.com/AlexIoannides/kubernetes-mlops": {
        "extra-tags": [],
        "date": "2019-01-28",
        "title": "kubernetes-mlops",
        "summary": "MLOps tutorial using Python, Docker and Kubernetes.",
        "tags": [
            "cloud-platform",
            "docker",
            "python",
            "gcp",
            "kubernetes",
            "seldon",
            "machine-learning",
            "helm",
            "flask",
            "mlops",
            "seldon-core"
        ]
    },
    "https://github.com/faif/python-patterns": {
        "extra-tags": [],
        "date": "2012-06-06",
        "title": "python-patterns",
        "summary": "A collection of design patterns/idioms in Python",
        "tags": [
            "idioms",
            "design-patterns",
            "python"
        ]
    },
    "https://github.com/marlbenchmark/on-policy": {
        "extra-tags": [],
        "date": "2021-02-23",
        "title": "on-policy",
        "summary": "This is the official implementation of Multi-Agent PPO (MAPPO).",
        "tags": [
            "algorithms",
            "hanabi",
            "python",
            "smac",
            "starcraftii",
            "multi-agent",
            "mappo",
            "mpes",
            "ppo"
        ]
    },
    "https://github.com/vaaaaanquish/Awesome-Rust-MachineLearning": {
        "extra-tags": [],
        "date": "2021-02-23",
        "title": "Awesome-Rust-MachineLearning",
        "summary": "This repository is a list of machine learning libraries written in Rust. It's a compilation of GitHub repositories, blogs, books, movies, discussions, papers, etc. \ud83e\udd80",
        "tags": [
            "natural-language-processing",
            "machine-learning-library",
            "awasome",
            "rust",
            "javascript",
            "machine-learning",
            "image-processing",
            "deep-learning",
            "rust-library"
        ]
    },
    "https://github.com/IsmaelMartinez/teams-for-linux": {
        "extra-tags": [],
        "date": "2018-10-03",
        "title": "teams-for-linux",
        "summary": "Unofficial Microsoft Teams for Linux client",
        "tags": [
            "electron",
            "javascript",
            "microsoft",
            "linux",
            "teams"
        ]
    },
    "https://github.com/mkdocstrings/mkdocstrings": {
        "extra-tags": [],
        "date": "2019-12-09",
        "title": "mkdocstrings",
        "summary": ":blue_book: Automatic documentation from sources, for MkDocs.",
        "tags": [
            "material-theme",
            "python",
            "autodoc",
            "mkdocs",
            "mkdocs-plugin",
            "mkdocstrings",
            "docstrings"
        ]
    },
    "https://github.com/rust-lang/book": {
        "extra-tags": [],
        "date": "2015-12-11",
        "title": "book",
        "summary": "The Rust Programming Language",
        "tags": [
            "rust",
            "book",
            "rust-programming-language",
            "mdbook"
        ]
    },
    "https://github.com/AntoineTheb/RNN-RL": {
        "extra-tags": [],
        "date": "2020-01-30",
        "title": "RNN-RL",
        "summary": "Experiments with reinforcement learning and recurrent neural networks",
        "tags": [
            "recurrent-neural-networks",
            "python",
            "reinforcement-learning",
            "pytorch"
        ]
    },
    "https://github.com/instadeepai/Mava": {
        "extra-tags": [],
        "date": "2021-03-30",
        "title": "Mava",
        "summary": "\ud83e\udd81 A library of multi-agent reinforcement learning systems and components",
        "tags": [
            "python",
            "reinforcement-learning",
            "marl",
            "multi-agent-systems",
            "jax",
            "research",
            "framework",
            "multi-agent-reinforcement-learning",
            "multiagent"
        ]
    },
    "https://github.com/kazukiosawa/asdl": {
        "extra-tags": [],
        "date": "2020-09-03",
        "title": "asdl",
        "summary": "ASDL: Automatic Second-order Differentiation Library for PyTorch",
        "tags": [
            "python"
        ]
    },
    "https://github.com/google-research/falken": {
        "extra-tags": [],
        "date": "2021-05-13",
        "title": "falken",
        "summary": "Falken provides developers with a service that allows them to train AI that can play their games",
        "tags": [
            "python",
            "unity3d",
            "games",
            "ml",
            "imitation-learning",
            "cpp"
        ]
    },
    "https://github.com/TezRomacH/python-package-template": {
        "extra-tags": [],
        "date": "2020-04-15",
        "title": "python-package-template",
        "summary": "\ud83d\ude80 Your next Python package needs a bleeding-edge project structure.",
        "tags": [
            "formatters",
            "cookiecutter",
            "python",
            "codestyle",
            "semantic-versions",
            "python-packages",
            "best-practices",
            "poetry",
            "makefile",
            "template"
        ]
    },
    "https://github.com/MarcoMeter/recurrent-ppo-truncated-bptt": {
        "extra-tags": [],
        "date": "2021-06-07",
        "title": "recurrent-ppo-truncated-bptt",
        "summary": "Baseline implementation of recurrent PPO using truncated BPTT",
        "tags": [
            "recurrent-neural-networks",
            "actor-critic",
            "recurrent",
            "recurrence",
            "pomdp",
            "deep-reinforcement-learning",
            "proximal-policy-optimization",
            "truncated",
            "policy-gradient",
            "jupyter notebook",
            "on-policy",
            "pytorch",
            "deep-learning",
            "lstm",
            "bptt",
            "gru",
            "ppo"
        ]
    },
    "https://github.com/NVlabs/Taylor_pruning": {
        "extra-tags": [
            "pruning",
            "pytorch"
        ],
        "date": "2019-03-29",
        "title": "Taylor_pruning",
        "summary": "Pruning Neural Networks with Taylor criterion in Pytorch",
        "tags": [
            "python"
        ]
    },
    "https://github.com/3outeille/jpeg-algorithm": {
        "extra-tags": [
            "explanation",
            "algorithm"
        ],
        "date": "2021-06-09",
        "title": "jpeg-algorithm",
        "summary": "Explanation & Implementation of JPEG algorithm.",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/3outeille/operational-research-project": {
        "extra-tags": [
            "research",
            "project"
        ],
        "date": "2021-05-18",
        "title": "operational-research-project",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/fel-thomas/numkdoc": {
        "extra-tags": [],
        "date": "2020-04-26",
        "title": "numkdoc",
        "summary": "Mkdoc autodoc from your numpy style docstrings",
        "tags": [
            "mkdocs",
            "mkdocs-plugin",
            "python",
            "numpydoc"
        ]
    },
    "https://github.com/enlite-ai/maze": {
        "extra-tags": [],
        "date": "2021-02-11",
        "title": "maze",
        "summary": "Maze Applied Reinforcement Learning Framework",
        "tags": [
            "decision-making",
            "python",
            "applied-machine-learning",
            "reinforcement-learning",
            "simulation",
            "framework",
            "machine-learning",
            "automation",
            "optimization",
            "data-science",
            "distributed",
            "deep-learning",
            "monitoring",
            "documentation"
        ]
    },
    "https://github.com/egoist/docute": {
        "extra-tags": [],
        "date": "2016-12-08",
        "title": "docute",
        "summary": "\ud83d\udcda Effortless documentation, done right.",
        "tags": [
            "documentation-tool",
            "javascript",
            "vue",
            "pr-welcome",
            "documentation",
            "docute",
            "gitbook"
        ]
    },
    "https://github.com/JakubVojvoda/design-patterns-python": {
        "extra-tags": [],
        "date": "2016-09-04",
        "title": "design-patterns-python",
        "summary": "Python Design Patterns",
        "tags": [
            "creational-pattern",
            "python",
            "object-oriented",
            "structural-pattern",
            "design-pattern",
            "behavioral-pattern"
        ]
    },
    "https://github.com/xLaszlo/datascience-fails": {
        "extra-tags": [],
        "date": "2020-08-30",
        "title": "datascience-fails",
        "summary": "Collection of articles listing reasons why data science projects fail.",
        "tags": []
    },
    "https://github.com/imirzadeh/CL-Gym": {
        "extra-tags": [],
        "date": "2021-04-20",
        "title": "CL-Gym",
        "summary": "CL-Gym: Full-Featured PyTorch Library for Continual Learning",
        "tags": [
            "python"
        ]
    },
    "https://github.com/neurodata/hyppo": {
        "extra-tags": [],
        "date": "2019-10-10",
        "title": "hyppo",
        "summary": "Python package for multivariate hypothesis testing",
        "tags": [
            "python",
            "independence",
            "hacktoberfest",
            "ksample-testing",
            "hypothesis-testing",
            "data-science"
        ]
    },
    "https://github.com/econchick/interrogate": {
        "extra-tags": [],
        "date": "2020-04-24",
        "title": "interrogate",
        "summary": "Explain yourself! Interrogate a codebase for docstring coverage.",
        "tags": [
            "python",
            "coverage",
            "hacktoberfest",
            "code-quality",
            "documentation"
        ]
    },
    "https://github.com/raphaelsty/intermarche": {
        "extra-tags": [],
        "date": "2021-06-19",
        "title": "intermarche",
        "summary": "4th place solution to datafactory challenge by Intermarch\u00e9.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/facebookresearch/AugLy": {
        "extra-tags": [],
        "date": "2021-06-09",
        "title": "AugLy",
        "summary": "A data augmentations library for audio, image, text, and video.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/vwxyzjn/gym-microrts-paper": {
        "extra-tags": [],
        "date": "2021-04-12",
        "title": "gym-microrts-paper",
        "summary": "The source code for the gym-microrts paper.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/jinxu06/gsubsampling": {
        "extra-tags": [
            "reference"
        ],
        "date": "2021-06-08",
        "title": "gsubsampling",
        "summary": "Reference implementation for \"Group Equivariant Subsampling\"",
        "tags": [
            "python"
        ]
    },
    "https://github.com/deepmind/tensor_annotations": {
        "extra-tags": [],
        "date": "2020-12-02",
        "title": "tensor_annotations",
        "summary": "Annotating tensor shapes using Python types",
        "tags": [
            "python"
        ]
    },
    "https://github.com/XuehaiPan/nvitop": {
        "extra-tags": [],
        "date": "2021-01-25",
        "title": "nvitop",
        "summary": "An interactive NVIDIA-GPU process viewer and beyond, the one-stop solution for GPU process management.",
        "tags": [
            "resource-monitor",
            "curses",
            "console",
            "nvidia-smi",
            "python",
            "gpu",
            "cuda",
            "nvml",
            "process-monitoring",
            "top",
            "gpu-monitoring",
            "command-line-tool",
            "nvidia",
            "monitoring",
            "monitoring-tool"
        ]
    },
    "https://github.com/frgfm/torch-scan": {
        "extra-tags": [],
        "date": "2020-03-16",
        "title": "torch-scan",
        "summary": "Seamless analysis of your PyTorch models (RAM usage, FLOPs, MACs, receptive field, etc.)",
        "tags": [
            "python",
            "benchmark",
            "pytorch-utils",
            "keras",
            "receptive-field",
            "deep-neural-networks",
            "flops-counter",
            "deep-learning",
            "pytorch",
            "flops",
            "summary"
        ]
    },
    "https://github.com/indylab/tabular_xdo": {
        "extra-tags": [
            "tabular-data"
        ],
        "date": "2021-02-25",
        "title": "tabular_xdo",
        "summary": "",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/indylab/nxdo": {
        "extra-tags": [],
        "date": "2021-03-10",
        "title": "nxdo",
        "summary": "Deep RL Code for XDO: A Double Oracle Algorithm for Extensive-Form Games",
        "tags": [
            "python"
        ]
    },
    "https://github.com/kyamagu/faiss-wheels": {
        "extra-tags": [
            "faiss",
            "wheel"
        ],
        "date": "2019-06-27",
        "title": "faiss-wheels",
        "summary": "Unofficial faiss wheel builder",
        "tags": [
            "nearest-neighbor-search",
            "python"
        ]
    },
    "https://github.com/lucidrains/segformer-pytorch": {
        "extra-tags": [],
        "date": "2021-06-06",
        "title": "segformer-pytorch",
        "summary": "Implementation of Segformer, Attention + MLP neural network for segmentation, in Pytorch",
        "tags": [
            "python",
            "image-segmentation",
            "attention-mechanism",
            "multilayer-perceptron",
            "segmentation",
            "deep-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/thuxugang/opus_fold": {
        "extra-tags": [],
        "date": "2020-02-23",
        "title": "opus_fold",
        "summary": "OPUS-Fold: An Open-Source Protein Folding Framework Based on Torsion-Angle Sampling",
        "tags": [
            "protein-secondary-structure",
            "protein-folding",
            "protein-strucutre-prediction",
            "protein-side-chain-modeling",
            "protein-torsion-angles",
            "protein-potential-functions",
            "protein-contact-map",
            "protein-scoring-functions",
            "opus-fold"
        ]
    },
    "https://github.com/brentyi/jaxlie": {
        "extra-tags": [],
        "date": "2020-11-28",
        "title": "jaxlie",
        "summary": "Rigid transforms + Lie groups, in JAX",
        "tags": [
            "geometry",
            "python",
            "jax",
            "lie-groups",
            "robotics",
            "computer-vision"
        ]
    },
    "https://github.com/kzl/decision-transformer": {
        "extra-tags": [],
        "date": "2021-06-02",
        "title": "decision-transformer",
        "summary": "Official codebase for Decision Transformer: Reinforcement Learning via Sequence Modeling.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/great-expectations/great_expectations": {
        "extra-tags": [
            "data"
        ],
        "date": "2017-09-11",
        "title": "great_expectations",
        "summary": "Always know what to expect from your data.",
        "tags": [
            "dataunittest",
            "pipeline",
            "exploratory-analysis",
            "data-unit-tests",
            "eda",
            "mlops",
            "data-profilers",
            "cleandata",
            "python",
            "data-quality",
            "dataquality",
            "datacleaning",
            "data-engineering",
            "data-science",
            "exploratory-data-analysis",
            "pipeline-testing",
            "data-profiling",
            "exploratorydataanalysis",
            "pipeline-debt",
            "datacleaner",
            "pipeline-tests"
        ]
    },
    "https://github.com/capitalone/DataProfiler": {
        "extra-tags": [],
        "date": "2020-11-09",
        "title": "DataProfiler",
        "summary": "What's in your data? Extract schema, statistics and entities from datasets",
        "tags": [
            "graph-data",
            "machine-learning",
            "npi",
            "pii",
            "tabular-data",
            "sensitive-data",
            "gdpr",
            "data-labels",
            "data-analysis",
            "nlp",
            "privacy",
            "network-data",
            "csv",
            "python",
            "data-science",
            "dataprofiling",
            "avro",
            "dataset",
            "security",
            "pandas"
        ]
    },
    "https://github.com/deepmind/android_env": {
        "extra-tags": [],
        "date": "2021-04-21",
        "title": "android_env",
        "summary": "RL research on Android devices.",
        "tags": [
            "reinforcement-learning",
            "python",
            "android"
        ]
    },
    "https://github.com/iterative/cml": {
        "extra-tags": [],
        "date": "2020-02-26",
        "title": "cml",
        "summary": "\u267e\ufe0f CML - Continuous Machine Learning | CI/CD for ML",
        "tags": [
            "ci",
            "cicd",
            "ci-cd",
            "gitlab-ci",
            "javascript",
            "continuous-integration",
            "dvc",
            "machine-learning",
            "hacktoberfest",
            "bitbucket-pipelines",
            "developer-tools",
            "data-science",
            "continuous-delivery",
            "github-actions"
        ]
    },
    "https://github.com/DeMoriarty/custom_matmul_kernels": {
        "extra-tags": [
            "matrix"
        ],
        "date": "2021-05-02",
        "title": "custom_matmul_kernels",
        "summary": "Customized matrix multiplication kernels",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/lucidrains/protein-bert-pytorch": {
        "extra-tags": [
            "bert",
            "pytorch"
        ],
        "date": "2021-05-26",
        "title": "protein-bert-pytorch",
        "summary": "Implementation of ProteinBERT in Pytorch",
        "tags": [
            "python",
            "protein-sequences",
            "deep-learning",
            "unsupervised-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/raphaelsty/perdu": {
        "extra-tags": [
            "code"
        ],
        "date": "2021-05-20",
        "title": "perdu",
        "summary": "Recycle your code \u267b\ufe0f",
        "tags": [
            "python3",
            "elasticsearch",
            "search-engine",
            "javascript",
            "flask",
            "vuejs"
        ]
    },
    "https://github.com/theislab/scarches": {
        "extra-tags": [
            "reference"
        ],
        "date": "2019-08-12",
        "title": "scarches",
        "summary": "Reference mapping for single-cell genomics",
        "tags": [
            "multiomics",
            "human-cell-atlas",
            "rna-seq-analysis",
            "data-integration",
            "multimodal-deep-learning",
            "single-cell",
            "single-cell-genomics",
            "jupyter notebook",
            "batch-correction",
            "scrna-seq",
            "deep-learning"
        ]
    },
    "https://github.com/FunctionLab/selene": {
        "extra-tags": [],
        "date": "2017-08-21",
        "title": "selene",
        "summary": "a framework for training sequence-level deep learning networks",
        "tags": [
            "sampling-methods",
            "genomic-data-analysis",
            "jupyter notebook",
            "deep-learning"
        ]
    },
    "https://github.com/deGrootLab/pmx": {
        "extra-tags": [
            "energy",
            "analysis"
        ],
        "date": "2017-06-09",
        "title": "pmx",
        "summary": "Toolkit for free-energy calculation setup/analysis and biomolecular structure handling",
        "tags": [
            "python"
        ]
    },
    "https://github.com/laszukdawid/ai-traineree": {
        "extra-tags": [],
        "date": "2020-05-28",
        "title": "ai-traineree",
        "summary": "PyTorch agents and tools for (Deep) Reinforcement Learning",
        "tags": [
            "agents",
            "python",
            "reinforcement-learning",
            "dqn-pytorch",
            "deep",
            "ddpg",
            "rainbow",
            "pytorch",
            "artificial-intelligence-algorithms",
            "ppo",
            "multi-agents"
        ]
    },
    "https://github.com/eseraygun/python-alignment": {
        "extra-tags": [],
        "date": "2012-11-11",
        "title": "python-alignment",
        "summary": "Native Python library for generic sequence alignment",
        "tags": [
            "python"
        ]
    },
    "https://github.com/lucidrains/g-mlp-pytorch": {
        "extra-tags": [
            "transformers",
            "pytorch"
        ],
        "date": "2021-05-18",
        "title": "g-mlp-pytorch",
        "summary": "Implementation of gMLP, an all-MLP replacement for Transformers, in Pytorch",
        "tags": [
            "multilayer-perceptron",
            "python",
            "deep-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/pytorch/nestedtensor": {
        "extra-tags": [
            "tools",
            "tensors"
        ],
        "date": "2019-10-23",
        "title": "nestedtensor",
        "summary": "[Prototype] Tools for the concurrent manipulation of variably sized Tensors.",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/rusty1s/pytorch_sparse": {
        "extra-tags": [],
        "date": "2018-07-28",
        "title": "pytorch_sparse",
        "summary": "PyTorch Extension Library of Optimized Autograd Sparse Matrix Operations",
        "tags": [
            "sparse",
            "python",
            "autograd",
            "sparse-matrices",
            "pytorch"
        ]
    },
    "https://github.com/openproblems-bio/openproblems": {
        "extra-tags": [
            "benchmarking",
            "single-cell"
        ],
        "date": "2020-06-19",
        "title": "openproblems",
        "summary": "Formalizing and benchmarking open problems in single-cell genomics",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/joansj/hat": {
        "extra-tags": [
            "attention",
            "task"
        ],
        "date": "2018-04-09",
        "title": "hat",
        "summary": "Overcoming catastrophic forgetting with hard attention to the task",
        "tags": [
            "python"
        ]
    },
    "https://github.com/pydantic/pydantic": {
        "extra-tags": [],
        "date": "2017-05-03",
        "title": "pydantic",
        "summary": "Data validation using Python type hints",
        "tags": [
            "pydantic",
            "python",
            "python310",
            "python38",
            "python39",
            "json-schema",
            "python311",
            "parsing",
            "python37",
            "validation",
            "hints"
        ]
    },
    "https://github.com/eugeneyan/applied-ml": {
        "extra-tags": [],
        "date": "2020-07-04",
        "title": "applied-ml",
        "summary": "\ud83d\udcda Papers & tech blogs by companies sharing their work on data science & machine learning in production.",
        "tags": [
            "natural-language-processing",
            "production",
            "applied-machine-learning",
            "data-quality",
            "reinforcement-learning",
            "recsys",
            "search",
            "machine-learning",
            "applied-data-science",
            "data-engineering",
            "data-science",
            "deep-learning",
            "data-discovery",
            "computer-vision"
        ]
    },
    "https://github.com/pytorch/functorch": {
        "extra-tags": [
            "jax"
        ],
        "date": "2021-04-19",
        "title": "functorch",
        "summary": "functorch is JAX-like composable function transforms for PyTorch.",
        "tags": [
            "gradients",
            "pytorch",
            "jupyter notebook",
            "hessians"
        ]
    },
    "https://github.com/frozenca/Ndim-Matrix": {
        "extra-tags": [],
        "date": "2021-05-01",
        "title": "Ndim-Matrix",
        "summary": "C++20 N-dimensional Matrix class for hobby project",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/cpplint/cpplint": {
        "extra-tags": [],
        "date": "2015-12-01",
        "title": "cpplint",
        "summary": "Static code checker for C++",
        "tags": [
            "lint",
            "cpp",
            "python",
            "linter"
        ]
    },
    "https://github.com/clvrai/awesome-rl-envs": {
        "extra-tags": [
            "awesome",
            "rl"
        ],
        "date": "2019-11-26",
        "title": "awesome-rl-envs",
        "summary": "",
        "tags": []
    },
    "https://github.com/lightonai/lairgpt": {
        "extra-tags": [],
        "date": "2021-05-03",
        "title": "lairgpt",
        "summary": "Inference code in Pytorch for GPT-like models, such as PAGnol, a family of models with up to 1.5B parameters, trained on datasets in French.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/beartype/beartype": {
        "extra-tags": [],
        "date": "2020-04-03",
        "title": "beartype",
        "summary": "Unbearably fast near-real-time runtime type-checking in pure Python.",
        "tags": [
            "python3",
            "runtime-typechecking",
            "python"
        ]
    },
    "https://github.com/deepmind/chex": {
        "extra-tags": [],
        "date": "2020-08-06",
        "title": "chex",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/GoogleCloudPlatform/ml-design-patterns": {
        "extra-tags": [],
        "date": "2020-03-17",
        "title": "ml-design-patterns",
        "summary": "Source code accompanying O'Reilly book: Machine Learning Design Patterns",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/facebookresearch/dino": {
        "extra-tags": [],
        "date": "2021-04-21",
        "title": "dino",
        "summary": "PyTorch code for Vision Transformers training with the Self-Supervised learning method DINO",
        "tags": [
            "python"
        ]
    },
    "https://github.com/n2cholas/shapecheck": {
        "extra-tags": [],
        "date": "2020-12-25",
        "title": "shapecheck",
        "summary": "Framework-agnostic library for checking array/tensor shapes at runtime.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/scikit-learn-contrib/skope-rules": {
        "extra-tags": [],
        "date": "2018-02-18",
        "title": "skope-rules",
        "summary": "machine learning with logical rules in Python",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/suriyadeepan/torchtest": {
        "extra-tags": [],
        "date": "2018-11-07",
        "title": "torchtest",
        "summary": "Unit Testing for pytorch, based on mltest",
        "tags": [
            "testing",
            "python",
            "machine-learning",
            "pytorch"
        ]
    },
    "https://github.com/ml-tooling/opyrator": {
        "extra-tags": [],
        "date": "2021-04-06",
        "title": "opyrator",
        "summary": "\ud83e\ude84 Turns your machine learning code into microservices with web API, interactive GUI, and more.",
        "tags": [
            "python-functions",
            "pydantic",
            "python",
            "faas",
            "machine-learning",
            "streamlit",
            "deployment",
            "fastapi",
            "microservices",
            "serverless",
            "functions",
            "type-hints"
        ]
    },
    "https://github.com/raphaelsty/twiver": {
        "extra-tags": [],
        "date": "2021-04-26",
        "title": "twiver",
        "summary": "An infinite stream connected to Twitter and focusing on Retweets forecasting for River. ",
        "tags": [
            "python"
        ]
    },
    "https://github.com/KaiyuYue/torchshard": {
        "extra-tags": [],
        "date": "2021-04-27",
        "title": "torchshard",
        "summary": "TorchShard: Slicing a PyTorch Tensor Into Parallel Shards.",
        "tags": [
            "shard-training",
            "python",
            "pytorch",
            "model-parallel"
        ]
    },
    "https://github.com/deepmind/jmp": {
        "extra-tags": [],
        "date": "2021-04-12",
        "title": "jmp",
        "summary": "JMP is a Mixed Precision library for JAX.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/dwromero/ckconv": {
        "extra-tags": [],
        "date": "2021-02-04",
        "title": "ckconv",
        "summary": "Code repository of the paper \"CKConv: Continuous Kernel Convolution For Sequential Data\" published at ICLR 2022. https://arxiv.org/abs/2102.02611",
        "tags": [
            "python"
        ]
    },
    "https://github.com/hkchengrex/Scribble-to-Mask": {
        "extra-tags": [
            "cvpr"
        ],
        "date": "2021-03-09",
        "title": "Scribble-to-Mask",
        "summary": "[CVPR 2021] MiVOS - Scribble to Mask module",
        "tags": [
            "python",
            "interactive-segmentation",
            "cvpr2021",
            "segmentation",
            "pytorch",
            "deep-learning",
            "computer-vision"
        ]
    },
    "https://github.com/astariul/encode-attend-navigate-pytorch": {
        "extra-tags": [],
        "date": "2021-02-14",
        "title": "encode-attend-navigate-pytorch",
        "summary": "Encode-attend-navigate unofficial Pytorch implementation",
        "tags": [
            "tsp",
            "python",
            "tsp-problem",
            "gpu",
            "machine-learning",
            "hacktoberfest",
            "notebook",
            "pytorch",
            "deep-learning",
            "colab",
            "tsp-solver",
            "rl"
        ]
    },
    "https://github.com/dam-grassman/unity-6eme-etage": {
        "extra-tags": [
            "unity",
            "find"
        ],
        "date": "2021-04-22",
        "title": "unity-6eme-etage",
        "summary": "FIrst attempt to use unity : find the guy in a work office",
        "tags": [
            "asp.net"
        ]
    },
    "https://github.com/diambra/arena": {
        "extra-tags": [],
        "date": "2021-01-26",
        "title": "arena",
        "summary": "DIAMBRA Arena: a New Reinforcement Learning Platform for Research and Experimentation",
        "tags": [
            "python",
            "reinforcement-learning",
            "video-games",
            "machine-learning",
            "deep-reinforcement-learning",
            "deep-learning",
            "esports",
            "competitions",
            "tournaments",
            "artificial-intelligence"
        ]
    },
    "https://github.com/mfinzi/equivariant-MLP": {
        "extra-tags": [
            "library"
        ],
        "date": "2020-09-11",
        "title": "equivariant-MLP",
        "summary": "A library for programmatically generating equivariant layers through constraint solving",
        "tags": [
            "jupyter notebook",
            "equivariance",
            "deep-learning"
        ]
    },
    "https://github.com/IntelLabs/bayesian-torch": {
        "extra-tags": [],
        "date": "2020-12-17",
        "title": "bayesian-torch",
        "summary": "A library for Bayesian neural network layers and uncertainty estimation in Deep Learning extending the core of PyTorch",
        "tags": [
            "uncertainty-estimation",
            "uncertainty-quantification",
            "python",
            "bayesian-inference",
            "stochastic-variational-inference",
            "bayesian-neural-networks",
            "bayesian-deep-learning",
            "bayesian-layers",
            "deep-neural-networks",
            "deep-learning",
            "pytorch",
            "uncertainty-neural-networks"
        ]
    },
    "https://github.com/deepfakes/faceswap": {
        "extra-tags": [],
        "date": "2017-12-19",
        "title": "faceswap",
        "summary": "Deepfakes Software For All",
        "tags": [
            "deepfakes",
            "python",
            "neural-networks",
            "face-swap",
            "faceswap",
            "myfakeapp",
            "deepface",
            "machine-learning",
            "neural-nets",
            "deep-face-swap",
            "openfaceswap",
            "deep-neural-networks",
            "deep-learning",
            "deeplearning",
            "fakeapp"
        ]
    },
    "https://github.com/toshas/torch-fidelity": {
        "extra-tags": [],
        "date": "2020-04-23",
        "title": "torch-fidelity",
        "summary": "High-fidelity performance metrics for generative models in PyTorch",
        "tags": [
            "evaluation",
            "kernel-inception-distance",
            "generative-model",
            "precision",
            "reproducible-research",
            "python",
            "metrics",
            "reproducibility",
            "perceptual-path-length",
            "inception-score",
            "pytorch",
            "frechet-inception-distance",
            "gan"
        ]
    },
    "https://github.com/laike9m/Cyberbrain": {
        "extra-tags": [],
        "date": "2020-03-23",
        "title": "Cyberbrain",
        "summary": "Python debugging, redefined.",
        "tags": [
            "debugging",
            "python"
        ]
    },
    "https://github.com/aguschin/idao_2021_finals": {
        "extra-tags": [],
        "date": "2021-03-19",
        "title": "idao_2021_finals",
        "summary": "Example solution for IDAO 2021 Finals",
        "tags": [
            "data-science-competition",
            "idao",
            "python",
            "ml-competitions"
        ]
    },
    "https://github.com/flashlight/flashlight": {
        "extra-tags": [],
        "date": "2018-12-11",
        "title": "flashlight",
        "summary": "A C++ standalone library for machine learning",
        "tags": [
            "neural-network",
            "ml",
            "autograd",
            "machine-learning",
            "flashlight",
            "deep-learning",
            "cpp",
            "c++"
        ]
    },
    "https://github.com/cclauss/GitHub-Action-for-pytest": {
        "extra-tags": [],
        "date": "2019-02-14",
        "title": "GitHub-Action-for-pytest",
        "summary": "A GitHub Action to run a pytest command when new code is pushed into your repo",
        "tags": [
            "dockerfile"
        ]
    },
    "https://github.com/takuseno/d3rlpy": {
        "extra-tags": [],
        "date": "2020-05-23",
        "title": "d3rlpy",
        "summary": "An offline deep reinforcement learning library",
        "tags": [
            "python",
            "deep-reinforcement-learning",
            "offline-rl",
            "pytorch",
            "deep-learning"
        ]
    },
    "https://github.com/gpoore/minted": {
        "extra-tags": [],
        "date": "2013-04-08",
        "title": "minted",
        "summary": "minted is a LaTeX package that provides syntax highlighting using the Pygments library. Highlighted source code can be customized using fancyvrb.",
        "tags": [
            "tex"
        ]
    },
    "https://github.com/facebookresearch/pytorchvideo": {
        "extra-tags": [],
        "date": "2021-03-09",
        "title": "pytorchvideo",
        "summary": "A deep learning library for video understanding research.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/airbusgeo/playground-docs": {
        "extra-tags": [
            "documentation"
        ],
        "date": "2017-12-14",
        "title": "playground-docs",
        "summary": "Documentation for Intelligence Playground",
        "tags": [
            "python"
        ]
    },
    "https://github.com/deepmind/distrax": {
        "extra-tags": [],
        "date": "2021-04-01",
        "title": "distrax",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/probcomp/PClean": {
        "extra-tags": [],
        "date": "2019-09-30",
        "title": "PClean",
        "summary": "A domain-specific probabilistic programming language for scalable Bayesian data cleaning",
        "tags": [
            "julia",
            "bayesian-inference",
            "data-cleaning",
            "data-cleansing",
            "probabilistic-programming",
            "probabilistic-graphical-models"
        ]
    },
    "https://github.com/pytorch/pytorch_sphinx_theme": {
        "extra-tags": [
            "pytorch",
            "theme"
        ],
        "date": "2018-09-18",
        "title": "pytorch_sphinx_theme",
        "summary": "PyTorch Sphinx Theme",
        "tags": [
            "css"
        ]
    },
    "https://github.com/aevtikheev/flake8-numpy-random": {
        "extra-tags": [
            "numpy"
        ],
        "date": "2021-04-11",
        "title": "flake8-numpy-random",
        "summary": "Plugin for Flake8 that forbids the usage of numpy.random()",
        "tags": [
            "python",
            "linter",
            "flake8-plugin",
            "flake8"
        ]
    },
    "https://github.com/EleutherAI/equivariance": {
        "extra-tags": [],
        "date": "2021-03-15",
        "title": "equivariance",
        "summary": "A framework for implementing equivariant DL",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/Sara-Ahmed/SiT": {
        "extra-tags": [],
        "date": "2021-04-06",
        "title": "SiT",
        "summary": "Self-supervised vIsion Transformer (SiT)",
        "tags": [
            "python"
        ]
    },
    "https://github.com/agronholm/typeguard": {
        "extra-tags": [
            "time"
        ],
        "date": "2015-12-27",
        "title": "typeguard",
        "summary": "Run-time type checker for Python",
        "tags": [
            "python"
        ]
    },
    "https://github.com/simoninithomas/MLAgents-Tanks": {
        "extra-tags": [],
        "date": "2021-04-09",
        "title": "MLAgents-Tanks",
        "summary": "A multi-agent environment using Unity ML-Agents Toolkit where two agents compete in a 1vs1 tank fight game",
        "tags": [
            "reinforcement-learning",
            "mlagents",
            "unity3d",
            "ai",
            "pytorch"
        ]
    },
    "https://github.com/RL-VS/rlvs2021": {
        "extra-tags": [],
        "date": "2021-02-22",
        "title": "rlvs2021",
        "summary": "",
        "tags": [
            "css"
        ]
    },
    "https://github.com/JelteF/PyLaTeX": {
        "extra-tags": [],
        "date": "2014-01-15",
        "title": "PyLaTeX",
        "summary": "A Python library for creating LaTeX files",
        "tags": [
            "python"
        ]
    },
    "https://github.com/GarkGarcia/tikztosvg": {
        "extra-tags": [],
        "date": "2020-04-21",
        "title": "tikztosvg",
        "summary": "Render TikZ diagrams to SVG",
        "tags": [
            "latex",
            "shell",
            "tikz",
            "svg",
            "tikz-figures"
        ]
    },
    "https://github.com/jkterry1/RLSS": {
        "extra-tags": [],
        "date": "2020-09-25",
        "title": "RLSS",
        "summary": "",
        "tags": []
    },
    "https://github.com/lucidrains/egnn-pytorch": {
        "extra-tags": [
            "graph",
            "pytorch"
        ],
        "date": "2021-02-26",
        "title": "egnn-pytorch",
        "summary": "Implementation of E(n)-Equivariant Graph Neural Networks, in Pytorch",
        "tags": [
            "python",
            "equivariance",
            "graph-neural-network",
            "deep-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/deepmind/reverb": {
        "extra-tags": [],
        "date": "2020-05-01",
        "title": "reverb",
        "summary": "Reverb is an efficient and easy-to-use data storage and transport system designed for machine learning research",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/godweiyang/NN-CUDA-Example": {
        "extra-tags": [],
        "date": "2021-03-18",
        "title": "NN-CUDA-Example",
        "summary": "Several simple examples for popular neural network toolkits calling custom CUDA operators.",
        "tags": [
            "neural-network",
            "python",
            "cuda",
            "tensorflow",
            "pytorch",
            "cpp"
        ]
    },
    "https://github.com/lucidrains/STAM-pytorch": {
        "extra-tags": [],
        "date": "2021-03-28",
        "title": "STAM-pytorch",
        "summary": "Implementation of STAM (Space Time Attention Model), a pure and simple attention model that reaches SOTA for video classification",
        "tags": [
            "video-classification",
            "python",
            "attention-mechanism",
            "transformers",
            "deep-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/lucidrains/halonet-pytorch": {
        "extra-tags": [],
        "date": "2021-03-24",
        "title": "halonet-pytorch",
        "summary": "Implementation of the \ud83d\ude07 Attention layer from the paper, Scaling Local Self-Attention For Parameter Efficient Visual Backbones",
        "tags": [
            "python",
            "attention-mechanism",
            "vision",
            "deep-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/EleutherAI/gpt-neo": {
        "extra-tags": [],
        "date": "2020-07-05",
        "title": "gpt-neo",
        "summary": "An implementation of model parallel GPT-2 and GPT-3-style models using the mesh-tensorflow library.",
        "tags": [
            "python",
            "transformers",
            "gpt-2",
            "gpt-3",
            "gpt",
            "language-model"
        ]
    },
    "https://github.com/lxaw/JoJoPoseEstimation": {
        "extra-tags": [],
        "date": "2021-03-13",
        "title": "JoJoPoseEstimation",
        "summary": "Using OpenCV and OpenPose to recognize reference poses.",
        "tags": [
            "opencv",
            "computer-vision",
            "openpose",
            "python"
        ]
    },
    "https://github.com/grantjenks/python-sortedcontainers": {
        "extra-tags": [],
        "date": "2014-02-24",
        "title": "python-sortedcontainers",
        "summary": "Python Sorted Container Types: Sorted List, Sorted Dict, and Sorted Set",
        "tags": [
            "set",
            "dict",
            "python",
            "data-types",
            "list",
            "sorted"
        ]
    },
    "https://github.com/rlberry-py/rlberry": {
        "extra-tags": [],
        "date": "2020-10-15",
        "title": "rlberry",
        "summary": "An easy-to-use reinforcement learning library for research and education.",
        "tags": [
            "python",
            "reinforcement-learning-environments",
            "reinforcement-learning",
            "multi-armed-bandits",
            "reinforcement-learning-algorithms"
        ]
    },
    "https://github.com/swdotcom/swdc-vscode-musictime": {
        "extra-tags": [],
        "date": "2019-11-26",
        "title": "swdc-vscode-musictime",
        "summary": "A VS Code extension to discover the most productive music to listen to as you code",
        "tags": [
            "music",
            "typescript",
            "ai-playlists",
            "spotify",
            "vscode",
            "visual-studio-code"
        ]
    },
    "https://github.com/d-li14/involution": {
        "extra-tags": [],
        "date": "2021-01-29",
        "title": "involution",
        "summary": "[CVPR 2021] Involution: Inverting the Inherence of Convolution for Visual Recognition, a brand new neural operator",
        "tags": [
            "image-classification",
            "python",
            "cvpr2021",
            "instance-segmentation",
            "involution",
            "operator",
            "pytorch",
            "semantic-segmentation",
            "pre-trained-model",
            "object-detection"
        ]
    },
    "https://github.com/facebookresearch/vissl": {
        "extra-tags": [],
        "date": "2020-04-09",
        "title": "vissl",
        "summary": "VISSL is FAIR's library of extensible, modular and scalable components for SOTA Self-Supervised Learning with images.",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/zhaoxin94/awesome-domain-adaptation": {
        "extra-tags": [],
        "date": "2018-05-13",
        "title": "awesome-domain-adaptation",
        "summary": "A collection of AWESOME things about domian adaptation",
        "tags": [
            "few-shot-learning",
            "image-translation",
            "adversarial-learning",
            "domain-adaptation",
            "zero-shot-learning",
            "optimal-transport",
            "awesome-list",
            "paper",
            "transfer-learning"
        ]
    },
    "https://github.com/lucidrains/transformer-in-transformer": {
        "extra-tags": [],
        "date": "2021-03-02",
        "title": "transformer-in-transformer",
        "summary": "Implementation of Transformer in Transformer, pixel level attention paired with patch level attention for image classification, in Pytorch",
        "tags": [
            "image-classification",
            "python",
            "transformers",
            "deep-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/fastai/fastcore": {
        "extra-tags": [],
        "date": "2019-12-02",
        "title": "fastcore",
        "summary": "Python supercharged for the fastai library",
        "tags": [
            "fastai",
            "python",
            "functional-programming",
            "parallel-processing",
            "data-structures",
            "developer-tools",
            "dispatch",
            "jupyter notebook",
            "documentation-generator",
            "languages"
        ]
    },
    "https://github.com/changhsinlee/pytest-mock-examples": {
        "extra-tags": [],
        "date": "2020-04-07",
        "title": "pytest-mock-examples",
        "summary": "Examples for the blog post on pytest-mock",
        "tags": [
            "python"
        ]
    },
    "https://github.com/QUVA-Lab/e2cnn_experiments": {
        "extra-tags": [
            "experiment",
            "cnns"
        ],
        "date": "2021-02-26",
        "title": "e2cnn_experiments",
        "summary": "Experiment for General E(2)-Equivariant Steerable CNNs",
        "tags": [
            "python"
        ]
    },
    "https://github.com/sco1/flake8-annotations": {
        "extra-tags": [],
        "date": "2019-08-04",
        "title": "flake8-annotations",
        "summary": "Flake8 Type Annotation Checking",
        "tags": [
            "type-annotations",
            "python3",
            "python",
            "flake8",
            "python312",
            "python38",
            "python39",
            "python311",
            "python310",
            "flake8-plugin"
        ]
    },
    "https://github.com/lucidrains/En-transformer": {
        "extra-tags": [],
        "date": "2021-02-27",
        "title": "En-transformer",
        "summary": "Implementation of E(n)-Transformer, which extends the ideas of Welling's E(n)-Equivariant Graph Neural Network to attention",
        "tags": [
            "equivariance",
            "python",
            "transformer",
            "attention-mechanism",
            "deep-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/ischlag/fast-weight-transformers": {
        "extra-tags": [],
        "date": "2021-02-11",
        "title": "fast-weight-transformers",
        "summary": "Official code repository of the paper  Linear Transformers Are Secretly Fast Weight Programmers.",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/ays-dev/keras-transformer": {
        "extra-tags": [
            "keras",
            "transformer"
        ],
        "date": "2021-02-18",
        "title": "keras-transformer",
        "summary": "Transformer",
        "tags": [
            "python"
        ]
    },
    "https://github.com/python-poetry/poetry": {
        "extra-tags": [],
        "date": "2018-02-28",
        "title": "poetry",
        "summary": "Python packaging and dependency management made easy",
        "tags": [
            "python",
            "package-manager",
            "poetry",
            "dependency-manager",
            "packaging"
        ]
    },
    "https://github.com/pokaxpoka/sunrise": {
        "extra-tags": [],
        "date": "2020-07-02",
        "title": "sunrise",
        "summary": "SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning",
        "tags": [
            "model-free",
            "dm-control",
            "python",
            "off-policy",
            "reinforcement-learning",
            "deep-q-learning",
            "codebase",
            "sac",
            "soft-actor-critic",
            "mujoco",
            "deep-q-network",
            "deep-reinforcement-learning",
            "rainbow",
            "deep-neural-networks",
            "deep-learning",
            "rl"
        ]
    },
    "https://github.com/lucidrains/feedback-transformer-pytorch": {
        "extra-tags": [
            "pytorch"
        ],
        "date": "2021-02-02",
        "title": "feedback-transformer-pytorch",
        "summary": "Implementation of Feedback Transformer in Pytorch",
        "tags": [
            "artifiical-intelligence",
            "python",
            "transformer",
            "attention-mechanism",
            "deep-learning",
            "memory"
        ]
    },
    "https://github.com/openai/gym3": {
        "extra-tags": [],
        "date": "2020-06-03",
        "title": "gym3",
        "summary": "Vectorized interface for reinforcement learning environments",
        "tags": [
            "python"
        ]
    },
    "https://github.com/QUVA-Lab/e2cnn": {
        "extra-tags": [],
        "date": "2019-11-20",
        "title": "e2cnn",
        "summary": "E(2)-Equivariant CNNs Library for Pytorch",
        "tags": [
            "equivariance",
            "python",
            "cnns",
            "pytorch",
            "equivariant-network",
            "group-convolution"
        ]
    },
    "https://github.com/asteroid-team/torch-audiomentations": {
        "extra-tags": [],
        "date": "2020-06-22",
        "title": "torch-audiomentations",
        "summary": "Fast audio data augmentation in PyTorch. Inspired by audiomentations. Useful for deep learning.",
        "tags": [
            "audio",
            "music",
            "dsp",
            "differentiable-data-augmentation",
            "audio-data-augmentation",
            "python",
            "audio-effects",
            "sound-processing",
            "machine-learning",
            "waveform",
            "augmentation",
            "sound",
            "pytorch",
            "deep-learning",
            "data-augmentation"
        ]
    },
    "https://github.com/protossw512/AdaptiveWingLoss": {
        "extra-tags": [],
        "date": "2019-08-13",
        "title": "AdaptiveWingLoss",
        "summary": "[ICCV 2019] Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression - Official Implementation",
        "tags": [
            "python"
        ]
    },
    "https://github.com/fabienvauchelles/qscore": {
        "extra-tags": [],
        "date": "2018-05-11",
        "title": "qscore",
        "summary": "Quick Scoring Platform for Data Science and Artificial Intelligence",
        "tags": [
            "javascript"
        ]
    },
    "https://github.com/andyljones/reinforcement-learning-discord-wiki": {
        "extra-tags": [
            "reinforcement-learning",
            "rl"
        ],
        "date": "2020-10-20",
        "title": "reinforcement-learning-discord-wiki",
        "summary": "The RL discord wiki",
        "tags": []
    },
    "https://github.com/VITA-Group/TransGAN": {
        "extra-tags": [],
        "date": "2021-02-10",
        "title": "TransGAN",
        "summary": "[NeurIPS\u20182021] \"TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up\", Yifan Jiang, Shiyu Chang, Zhangyang Wang",
        "tags": [
            "python",
            "transformer",
            "transformer-models",
            "transformer-encoder",
            "pytorch",
            "gan"
        ]
    },
    "https://github.com/vballoli/nfnets-pytorch": {
        "extra-tags": [],
        "date": "2021-02-13",
        "title": "nfnets-pytorch",
        "summary": "NFNets and Adaptive Gradient Clipping for SGD implemented in PyTorch. Find explanation at tourdeml.github.io/blog/",
        "tags": [
            "image-classification",
            "python",
            "nfnets",
            "sgd",
            "adaptive-gradient-clipping",
            "pytorch",
            "sota",
            "paper",
            "deepmind"
        ]
    },
    "https://github.com/lucidrains/TimeSformer-pytorch": {
        "extra-tags": [],
        "date": "2021-02-11",
        "title": "TimeSformer-pytorch",
        "summary": "Implementation of TimeSformer from Facebook AI, a pure attention-based solution for video classification",
        "tags": [
            "video-classification",
            "python",
            "attention-mechanism",
            "transformers",
            "deep-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/yfletberliac/adversarially-guided-actor-critic": {
        "extra-tags": [
            "actor-critic"
        ],
        "date": "2021-01-26",
        "title": "adversarially-guided-actor-critic",
        "summary": "AGAC: Adversarially Guided Actor-Critic",
        "tags": [
            "adversarially-guided-actor-critic",
            "tensorflow",
            "pytorch",
            "python"
        ]
    },
    "https://github.com/facebookresearch/CompilerGym": {
        "extra-tags": [],
        "date": "2020-11-11",
        "title": "CompilerGym",
        "summary": "Reinforcement learning environments for compiler and program optimization tasks",
        "tags": [
            "python"
        ]
    },
    "https://github.com/utterance/utterances": {
        "extra-tags": [],
        "date": "2017-04-23",
        "title": "utterances",
        "summary": ":crystal_ball: A lightweight comments widget built on GitHub issues",
        "tags": [
            "comments",
            "blog",
            "comments-widget",
            "github",
            "typescript",
            "utterances"
        ]
    },
    "https://github.com/pybrain/pybrain": {
        "extra-tags": [],
        "date": "2009-07-02",
        "title": "pybrain",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/oegedijk/explainerdashboard": {
        "extra-tags": [],
        "date": "2019-10-30",
        "title": "explainerdashboard",
        "summary": "Quickly build Explainable AI dashboards that show the inner workings of so-called \"blackbox\" machine learning models.",
        "tags": [
            "inner-workings",
            "dash",
            "permutation-importances",
            "dashboard",
            "model-predictions",
            "python",
            "interactive-plots",
            "xai-library",
            "shap-values",
            "plotly",
            "data-scientists",
            "shap",
            "explainer",
            "xai",
            "interactive-dashboards"
        ]
    },
    "https://github.com/lucidrains/bottleneck-transformer-pytorch": {
        "extra-tags": [
            "transformer",
            "pytorch"
        ],
        "date": "2021-01-28",
        "title": "bottleneck-transformer-pytorch",
        "summary": "Implementation of Bottleneck Transformer in Pytorch",
        "tags": [
            "image-classification",
            "python",
            "attention-mechanism",
            "transformers",
            "vision",
            "deep-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/Chen-Cai-OSU/awesome-equivariant-network": {
        "extra-tags": [],
        "date": "2021-01-13",
        "title": "awesome-equivariant-network",
        "summary": "Paper list for equivariant neural network",
        "tags": []
    },
    "https://github.com/ml-tooling/best-of-ml-python": {
        "extra-tags": [],
        "date": "2020-11-29",
        "title": "best-of-ml-python",
        "summary": "\ud83c\udfc6 A ranked list of awesome machine learning Python libraries. Updated weekly.",
        "tags": [
            "keras",
            "machine-learning",
            "data-visualization",
            "data-visualizations",
            "data-analysis",
            "tensorflow",
            "nlp",
            "automl",
            "gpt",
            "chatgpt",
            "python",
            "transformer",
            "scikit-learn",
            "data-science",
            "deep-learning",
            "gpt-3",
            "jax",
            "pytorch",
            "ml"
        ]
    },
    "https://github.com/openai/CLIP": {
        "extra-tags": [],
        "date": "2020-12-16",
        "title": "CLIP",
        "summary": "CLIP (Contrastive Language-Image Pretraining),  Predict the most relevant text snippet given an image",
        "tags": [
            "jupyter notebook",
            "machine-learning",
            "deep-learning"
        ]
    },
    "https://github.com/lucidrains/se3-transformer-pytorch": {
        "extra-tags": [],
        "date": "2021-01-09",
        "title": "se3-transformer-pytorch",
        "summary": "Implementation of SE3-Transformers for Equivariant Self-Attention, in Pytorch. This specific repository is geared towards integration with eventual Alphafold2 replication.",
        "tags": [
            "equivariance",
            "python",
            "transformer",
            "attention-mechanism",
            "deep-learning",
            "se3",
            "artificial-intelligence"
        ]
    },
    "https://github.com/edent/SuperTinyIcons": {
        "extra-tags": [],
        "date": "2017-04-13",
        "title": "SuperTinyIcons",
        "summary": "Under 1KB each! Super Tiny Icons are miniscule SVG versions of your favourite website and app logos",
        "tags": [
            "tiny-social-icons",
            "javascript",
            "hacktoberfest",
            "svg-icons",
            "svg",
            "social-media",
            "logo"
        ]
    },
    "https://github.com/EndlessSora/focal-frequency-loss": {
        "extra-tags": [],
        "date": "2020-12-22",
        "title": "focal-frequency-loss",
        "summary": "[ICCV 2021] Focal Frequency Loss for Image Reconstruction and Synthesis",
        "tags": [
            "loss-function",
            "spade",
            "complementary",
            "frequency-analysis",
            "generative-adversarial-network",
            "iccv2021",
            "variational-autoencoder",
            "image-reconstruction",
            "loss",
            "image-synthesis",
            "python",
            "autoencoder",
            "frequency-domain",
            "generative-models",
            "generic",
            "image-generation",
            "stylegan2",
            "pix2pix",
            "gan"
        ]
    },
    "https://github.com/aquadzn/learn-x-by-doing-y": {
        "extra-tags": [],
        "date": "2021-01-03",
        "title": "learn-x-by-doing-y",
        "summary": "\ud83d\udee0\ufe0f Learn a technology X by doing a project  - Search engine of project-based learning",
        "tags": [
            "begineer",
            "python",
            "project",
            "algolia",
            "learning",
            "tutorial",
            "project-based-learning",
            "programming",
            "awesome-list"
        ]
    },
    "https://github.com/CompVis/taming-transformers": {
        "extra-tags": [],
        "date": "2020-12-17",
        "title": "taming-transformers",
        "summary": "Taming Transformers for High-Resolution Image Synthesis",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/janfreyberg/pytorch-revgrad": {
        "extra-tags": [],
        "date": "2019-03-30",
        "title": "pytorch-revgrad",
        "summary": "A minimal pytorch package implementing a gradient reversal layer.",
        "tags": [
            "pytorch",
            "python",
            "gradient-reversal",
            "domain-adaptation"
        ]
    },
    "https://github.com/lucidrains/lie-transformer-pytorch": {
        "extra-tags": [],
        "date": "2020-12-22",
        "title": "lie-transformer-pytorch",
        "summary": "Implementation of Lie Transformer, Equivariant Self-Attention, in Pytorch",
        "tags": [
            "equivariance",
            "python",
            "transformer",
            "attention-mechanism",
            "deep-learning",
            "se3",
            "artificial-intelligence"
        ]
    },
    "https://github.com/EleutherAI/the-pile": {
        "extra-tags": [],
        "date": "2020-08-26",
        "title": "the-pile",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/Mayukhdeb/torch-dreams": {
        "extra-tags": [],
        "date": "2020-08-28",
        "title": "torch-dreams",
        "summary": "Making neural networks more interpretable, for research and art :mag_right: :computer: :brain: :art:",
        "tags": [
            "python",
            "deep-dream",
            "feature-visualization",
            "pytorch",
            "deep-learning",
            "colab"
        ]
    },
    "https://github.com/yinboc/liif": {
        "extra-tags": [],
        "date": "2020-12-16",
        "title": "liif",
        "summary": "Learning Continuous Image Representation with Local Implicit Image Function, in CVPR 2021 (Oral)",
        "tags": [
            "python",
            "super-resolution",
            "machine-learning",
            "pytorch",
            "implicit-neural-representation"
        ]
    },
    "https://github.com/n2cholas/awesome-jax": {
        "extra-tags": [],
        "date": "2020-12-20",
        "title": "awesome-jax",
        "summary": "JAX - A curated list of resources https://github.com/google/jax",
        "tags": [
            "neural-network",
            "jax",
            "autograd",
            "machine-learning",
            "numpy",
            "xla",
            "deep-learning",
            "awesome",
            "awesome-list"
        ]
    },
    "https://github.com/Stonesjtu/pytorch_memlab": {
        "extra-tags": [],
        "date": "2019-05-24",
        "title": "pytorch_memlab",
        "summary": "Profiling and inspecting memory in pytorch",
        "tags": [
            "pytorch",
            "memory-profiler",
            "python",
            "cuda-memory"
        ]
    },
    "https://github.com/lucidrains/adjacent-attention-network": {
        "extra-tags": [],
        "date": "2020-12-10",
        "title": "adjacent-attention-network",
        "summary": "Graph neural network message passing reframed as a Transformer with local attention",
        "tags": [
            "python",
            "transformer",
            "attention-mechanism",
            "graph-neural-networks",
            "deep-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/IgorSusmelj/pytorch-styleguide": {
        "extra-tags": [],
        "date": "2019-04-14",
        "title": "pytorch-styleguide",
        "summary": "An unofficial styleguide and best practices summary for PyTorch",
        "tags": [
            "best-practices",
            "styleguide",
            "pytorch",
            "python"
        ]
    },
    "https://github.com/simoninithomas/ml-agents-snowball-fight": {
        "extra-tags": [],
        "date": "2020-12-09",
        "title": "ml-agents-snowball-fight",
        "summary": "A multi-agent environment using Unity ML-Agents Toolkit",
        "tags": [
            "reinforcement-learning",
            "mlagents",
            "unity3d",
            "deep-reinforcement-learning",
            "ai",
            "unity",
            "deep-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/GATECH-EIC/ShiftAddNet": {
        "extra-tags": [
            "hardware",
            "deep"
        ],
        "date": "2020-10-04",
        "title": "ShiftAddNet",
        "summary": "[NeurIPS 2020] ShiftAddNet: A Hardware-Inspired Deep Network",
        "tags": [
            "python"
        ]
    },
    "https://github.com/huawei-noah/AdderNet": {
        "extra-tags": [],
        "date": "2020-02-25",
        "title": "AdderNet",
        "summary": "Code for paper \" AdderNet: Do We Really Need Multiplications in Deep Learning?\"",
        "tags": [
            "python",
            "convolutional-neural-networks",
            "cvpr2020",
            "pytorch",
            "efficient-inference",
            "imagenet"
        ]
    },
    "https://github.com/isaackrementsov/agan": {
        "extra-tags": [
            "gan",
            "art"
        ],
        "date": "2020-10-26",
        "title": "agan",
        "summary": "GAN to create abstract art",
        "tags": [
            "python"
        ]
    },
    "https://github.com/banditml/offline-policy-evaluation": {
        "extra-tags": [],
        "date": "2020-03-10",
        "title": "offline-policy-evaluation",
        "summary": "Implementations and examples of common offline policy evaluation methods in Python.",
        "tags": [
            "counterfactual-learning",
            "importance-sampling",
            "doubly-robust",
            "python",
            "offline-policy-evaluation",
            "off-policy-evaluation",
            "counterfactual-policy-evaluation"
        ]
    },
    "https://github.com/fastai/tweetrel": {
        "extra-tags": [],
        "date": "2020-12-02",
        "title": "tweetrel",
        "summary": "Use GitHub Actions to send a tweet when you make a new release",
        "tags": [
            "release-automation",
            "twitter",
            "github",
            "jupyter notebook",
            "twitter-api",
            "nbdev",
            "github-actions"
        ]
    },
    "https://github.com/mszell/introdatasci": {
        "extra-tags": [],
        "date": "2019-12-04",
        "title": "introdatasci",
        "summary": "Course materials for: Introduction to Data Science and Programming",
        "tags": [
            "python",
            "teaching-materials",
            "jupyter notebook",
            "programming",
            "data-science"
        ]
    },
    "https://github.com/tawnkramer/gym-donkeycar": {
        "extra-tags": [],
        "date": "2018-10-13",
        "title": "gym-donkeycar",
        "summary": "OpenAI gym environment for donkeycar simulator",
        "tags": [
            "python"
        ]
    },
    "https://github.com/medipixel/rl_algorithms": {
        "extra-tags": [
            "rl",
            "algorithms"
        ],
        "date": "2018-12-10",
        "title": "rl_algorithms",
        "summary": "Structural implementation of RL key algorithms",
        "tags": [
            "gym",
            "python3",
            "python",
            "reinforcement-learning",
            "dqn",
            "policy-gradient",
            "pytorch",
            "deep-learning"
        ]
    },
    "https://github.com/CoinCheung/pytorch-loss": {
        "extra-tags": [],
        "date": "2019-04-10",
        "title": "pytorch-loss",
        "summary": "label-smooth, amsoftmax, partial-fc, focal-loss, triplet-loss, lovasz-softmax. Maybe useful ",
        "tags": [
            "python",
            "amsoftmax",
            "label-smoothing",
            "triplet-loss",
            "cuda",
            "focal-loss",
            "dice-loss",
            "partial-fc",
            "lovasz-softmax",
            "pytorch",
            "ema",
            "mish"
        ]
    },
    "https://github.com/implus/GFocalV2": {
        "extra-tags": [],
        "date": "2020-09-04",
        "title": "GFocalV2",
        "summary": "Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection, CVPR2021",
        "tags": [
            "dense-object-detection",
            "gfl",
            "python",
            "gflv2",
            "one-stage-detector",
            "cvpr2021",
            "localization-quality",
            "distribution-statistics",
            "computer-vision",
            "cvpr21",
            "object-detection",
            "detection"
        ]
    },
    "https://github.com/CoffeeBeforeArch/cuda_programming": {
        "extra-tags": [],
        "date": "2019-02-13",
        "title": "cuda_programming",
        "summary": "Code from the \"CUDA Crash Course\" YouTube series by CoffeeBeforeArch",
        "tags": [
            "cuda"
        ]
    },
    "https://github.com/NVIDIA/nvidia-container-runtime": {
        "extra-tags": [
            "nvidia",
            "container"
        ],
        "date": "2017-09-05",
        "title": "nvidia-container-runtime",
        "summary": "NVIDIA container runtime",
        "tags": [
            "makefile"
        ]
    },
    "https://github.com/hsvgbkhgbv/SQDDPG": {
        "extra-tags": [],
        "date": "2019-01-17",
        "title": "SQDDPG",
        "summary": "This is a framework for the research on multi-agent reinforcement learning and the implementation of the experiments in the paper titled by ''Shapley Q-value: A Local Reward Approach to Solve Global Reward Games''.",
        "tags": [
            "python",
            "reinforcement-learning",
            "multi-agent-rl",
            "marl",
            "multiagent-reinforcement-learning",
            "shapley-q-value",
            "framework",
            "multi-agent-reinforcement-learning",
            "policy-gradient",
            "sqddpg",
            "openai-gym",
            "pytorch"
        ]
    },
    "https://github.com/jiupinjia/stylized-neural-painting": {
        "extra-tags": [],
        "date": "2020-11-16",
        "title": "stylized-neural-painting",
        "summary": "Official Pytorch implementation of the preprint paper \"Stylized Neural Painting\", in CVPR 2021.",
        "tags": [
            "stroke-parameters",
            "python",
            "style-transfer",
            "painting-translation",
            "neural-rendering"
        ]
    },
    "https://github.com/abhishekkrthakur/tez": {
        "extra-tags": [],
        "date": "2020-11-13",
        "title": "tez",
        "summary": "Tez is a super-simple and lightweight Trainer for PyTorch. It also comes with many utils that you can use to tackle over 90% of deep learning projects in PyTorch.",
        "tags": [
            "python",
            "neural-networks",
            "tez",
            "pytorch",
            "deep-neural-networks",
            "deep-learning"
        ]
    },
    "https://github.com/NVIDIA/libnvidia-container": {
        "extra-tags": [],
        "date": "2017-04-14",
        "title": "libnvidia-container",
        "summary": "NVIDIA container runtime library",
        "tags": [
            "c"
        ]
    },
    "https://github.com/NVIDIA/nvidia-docker": {
        "extra-tags": [],
        "date": "2015-11-04",
        "title": "nvidia-docker",
        "summary": "Build and run Docker containers leveraging NVIDIA GPUs",
        "tags": [
            "docker",
            "gpu",
            "cuda",
            "makefile",
            "nvidia-docker"
        ]
    },
    "https://github.com/anibali/docker-pytorch": {
        "extra-tags": [],
        "date": "2017-05-22",
        "title": "docker-pytorch",
        "summary": "A Docker image for PyTorch",
        "tags": [
            "docker-image",
            "docker",
            "cuda",
            "pytorch",
            "dockerfile"
        ]
    },
    "https://github.com/ccthien/MetalWarfareML": {
        "extra-tags": [
            "game",
            "ml-agents"
        ],
        "date": "2018-01-28",
        "title": "MetalWarfareML",
        "summary": "Metal Warfare game for ML-Agents challenge",
        "tags": [
            "python"
        ]
    },
    "https://github.com/deepmind/dm_hard_eight": {
        "extra-tags": [],
        "date": "2020-04-23",
        "title": "dm_hard_eight",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/deepmind/dm_memorytasks": {
        "extra-tags": [],
        "date": "2019-12-03",
        "title": "dm_memorytasks",
        "summary": "A set of 13 diverse machine-learning tasks that require memory to solve.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/3b1b/manim": {
        "extra-tags": [],
        "date": "2015-03-22",
        "title": "manim",
        "summary": "Animation engine for explanatory math videos",
        "tags": [
            "python",
            "3b1b-videos",
            "animation",
            "explanatory-math-videos"
        ]
    },
    "https://github.com/Raschka-research-group/coral-pytorch": {
        "extra-tags": [],
        "date": "2020-11-07",
        "title": "coral-pytorch",
        "summary": "CORAL and CORN implementations for ordinal regression with deep neural networks.",
        "tags": [
            "ordinal-regression",
            "ordinal-classification",
            "python",
            "deep-learning"
        ]
    },
    "https://github.com/epfml/ML_course": {
        "extra-tags": [
            "machine",
            "learning"
        ],
        "date": "2016-07-13",
        "title": "ML_course",
        "summary": "EPFL Machine Learning Course, Fall 2021",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/sbadirli/GrowNet": {
        "extra-tags": [],
        "date": "2020-05-11",
        "title": "GrowNet",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/lilanxiao/Rotated_IoU": {
        "extra-tags": [
            "differentiable"
        ],
        "date": "2020-08-11",
        "title": "Rotated_IoU",
        "summary": "Differentiable IoU of rotated bounding boxes using Pytorch",
        "tags": [
            "python",
            "rotated-boxes-iou",
            "pytorch",
            "iou-loss",
            "object-detection",
            "differentiable-iou"
        ]
    },
    "https://github.com/ternaus/cloths_segmentation": {
        "extra-tags": [
            "code",
            "segmentation"
        ],
        "date": "2020-10-29",
        "title": "cloths_segmentation",
        "summary": "Code for binary segmentation of cloths",
        "tags": [
            "computer-vision",
            "image-segmentation",
            "python",
            "deep-learning"
        ]
    },
    "https://github.com/openai/phasic-policy-gradient": {
        "extra-tags": [],
        "date": "2020-09-02",
        "title": "phasic-policy-gradient",
        "summary": "Code for the paper \"Phasic Policy Gradient\"",
        "tags": [
            "python"
        ]
    },
    "https://github.com/rosewang2008/gym-cooking": {
        "extra-tags": [],
        "date": "2020-11-02",
        "title": "gym-cooking",
        "summary": "gym-cooking: Code for \"Too many cooks: Bayesian inference for coordinating multi-agent collaboration\", Winner of the CogSci 2020 Computational Modeling Prize in High Cognition, and a NeurIPS 2020 CoopAI Workshop Best Paper.",
        "tags": [
            "reinforcement-learning",
            "multiagent-reinforcement-learning",
            "python",
            "machine-learning"
        ]
    },
    "https://github.com/magicleap/SuperGluePretrainedNetwork": {
        "extra-tags": [],
        "date": "2020-03-17",
        "title": "SuperGluePretrainedNetwork",
        "summary": "SuperGlue: Learning Feature Matching with Graph Neural Networks (CVPR 2020, Oral)",
        "tags": [
            "pose-estimation",
            "python",
            "graph-neural-networks",
            "deep-learning",
            "feature-matching"
        ]
    },
    "https://github.com/raphaelsty/ckb": {
        "extra-tags": [
            "knowledge"
        ],
        "date": "2020-11-06",
        "title": "ckb",
        "summary": "Contextual knowledge bases",
        "tags": [
            "python",
            "wordnet",
            "freebase",
            "knowledge-base",
            "bert-embeddings",
            "link-prediction",
            "knowledge-graph",
            "rdf"
        ]
    },
    "https://github.com/ch3njust1n/papers_are_all_you_need": {
        "extra-tags": [],
        "date": "2020-11-06",
        "title": "papers_are_all_you_need",
        "summary": "Search and download accepted papers from machine learning conferences",
        "tags": [
            "conference",
            "python",
            "research-paper",
            "research",
            "machine-learning",
            "downloader",
            "artificial-intelligence"
        ]
    },
    "https://github.com/BY571/Soft-Actor-Critic-and-Extensions": {
        "extra-tags": [],
        "date": "2020-02-20",
        "title": "Soft-Actor-Critic-and-Extensions",
        "summary": "PyTorch implementation of Soft-Actor-Critic and Prioritized Experience Replay (PER) + Emphasizing Recent Experience (ERE) + Munchausen RL + D2RL and parallel Environments. ",
        "tags": [
            "actor-critic-algorithm",
            "continuous",
            "prioritized-experience-replay",
            "python",
            "reinforcement-learning",
            "sac",
            "multi-environment",
            "soft-actor-critic",
            "d2rl",
            "munchausen-reinforcement-learning",
            "munchausen",
            "pytorch",
            "emphasizing-recent-experience",
            "parallel-computing",
            "reinforcement-learning-algorithms"
        ]
    },
    "https://github.com/albumentations-team/autoalbument": {
        "extra-tags": [],
        "date": "2020-09-28",
        "title": "autoalbument",
        "summary": "AutoML for image augmentation. AutoAlbument uses the Faster AutoAugment algorithm to find optimal augmentation policies. Documentation - https://albumentations.ai/docs/autoalbument/",
        "tags": [
            "image-augmentation",
            "python",
            "machine-learning",
            "augmentation",
            "automl",
            "pytorch",
            "deep-learning",
            "computer-vision",
            "automated-machine-learning"
        ]
    },
    "https://github.com/google-research/batch_rl": {
        "extra-tags": [],
        "date": "2019-07-25",
        "title": "batch_rl",
        "summary": "Offline Reinforcement Learning (aka Batch Reinforcement Learning) on Atari 2600 games",
        "tags": [
            "python"
        ]
    },
    "https://github.com/facebook/pyre-check": {
        "extra-tags": [],
        "date": "2017-11-10",
        "title": "pyre-check",
        "summary": "Performant type-checking for python.",
        "tags": [
            "python",
            "static-analysis",
            "taint-analysis",
            "typechecker",
            "abstract-interpretation",
            "program-analysis",
            "control-flow-analysis",
            "code-quality",
            "security",
            "type-check",
            "ocaml"
        ]
    },
    "https://github.com/WarBean/emp": {
        "extra-tags": [],
        "date": "2020-11-02",
        "title": "emp",
        "summary": "Easy Multiprocessing for Python",
        "tags": [
            "python"
        ]
    },
    "https://github.com/clcarwin/focal_loss_pytorch": {
        "extra-tags": [
            "pytorch",
            "loss"
        ],
        "date": "2017-08-12",
        "title": "focal_loss_pytorch",
        "summary": "A PyTorch Implementation of Focal Loss.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/queensferryme/hugo-theme-texify": {
        "extra-tags": [],
        "date": "2019-07-22",
        "title": "hugo-theme-texify",
        "summary": "A minimal, latex-style hugo theme for personal blogging",
        "tags": [
            "latex",
            "html",
            "minimal",
            "responsive",
            "hugo-theme",
            "hugo",
            "hugo-blog"
        ]
    },
    "https://github.com/kaisugi/HugoTeX": {
        "extra-tags": [],
        "date": "2020-06-01",
        "title": "HugoTeX",
        "summary": "LaTeX style hugo theme",
        "tags": [
            "latex",
            "hugo-site",
            "latex-css",
            "css",
            "hugo-theme",
            "hugo",
            "hugo-blog"
        ]
    },
    "https://github.com/openai/multi-agent-emergence-environments": {
        "extra-tags": [],
        "date": "2019-08-12",
        "title": "multi-agent-emergence-environments",
        "summary": "Environment generation code for the paper \"Emergent Tool Use From Multi-Agent Autocurricula\"",
        "tags": [
            "python"
        ]
    },
    "https://github.com/chesterhow/tale": {
        "extra-tags": [],
        "date": "2017-03-10",
        "title": "tale",
        "summary": "Minimal Jekyll theme for storytellers",
        "tags": [
            "jekyll-theme",
            "jekyll",
            "scss"
        ]
    },
    "https://github.com/vincentdoerig/latex-css": {
        "extra-tags": [],
        "date": "2020-05-18",
        "title": "latex-css",
        "summary": "LaTeX.css is a CSS library that makes your website look like a LaTeX document",
        "tags": [
            "latex",
            "html",
            "latex-css",
            "css",
            "classless",
            "css-library",
            "classless-theme"
        ]
    },
    "https://github.com/ma3oun/hrn": {
        "extra-tags": [],
        "date": "2020-10-19",
        "title": "hrn",
        "summary": "Hash-routed Networks",
        "tags": [
            "python"
        ]
    },
    "https://github.com/ucla-rlcourse/competitive-rl": {
        "extra-tags": [],
        "date": "2020-03-30",
        "title": "competitive-rl",
        "summary": "A set of competitive environments for Reinforcement Learning research.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/sfujim/BCQ": {
        "extra-tags": [],
        "date": "2018-12-18",
        "title": "BCQ",
        "summary": "Author's PyTorch implementation of BCQ for continuous and discrete actions",
        "tags": [
            "python"
        ]
    },
    "https://github.com/toshikwa/rljax": {
        "extra-tags": [],
        "date": "2020-09-24",
        "title": "rljax",
        "summary": "A collection of RL algorithms written in JAX.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/plasma-umass/scalene": {
        "extra-tags": [],
        "date": "2019-12-17",
        "title": "scalene",
        "summary": "Scalene: a high-performance, high-precision CPU, GPU, and memory profiler for Python with AI-powered optimization proposals",
        "tags": [
            "memory-consumption",
            "performance-analysis",
            "python",
            "memory-allocation",
            "gpu",
            "gpu-programming",
            "javascript",
            "performance-cpu",
            "cpu-profiling",
            "python-profilers",
            "profiles-memory",
            "profiler",
            "profiling",
            "scalene",
            "cpu"
        ]
    },
    "https://github.com/ilevkivskyi/typing_inspect": {
        "extra-tags": [],
        "date": "2017-04-29",
        "title": "typing_inspect",
        "summary": "Runtime inspection utilities for Python typing module",
        "tags": [
            "python3",
            "python",
            "typing",
            "type-hints",
            "introspection"
        ]
    },
    "https://github.com/markshannon/faster-cpython": {
        "extra-tags": [
            "make",
            "cpython"
        ],
        "date": "2020-10-19",
        "title": "faster-cpython",
        "summary": "How to make CPython faster.",
        "tags": []
    },
    "https://github.com/Ha0Tang/DAGAN": {
        "extra-tags": [],
        "date": "2020-08-04",
        "title": "DAGAN",
        "summary": "[ACM MM 2020] Dual Attention GANs for Semantic Image Synthesis",
        "tags": [
            "python"
        ]
    },
    "https://github.com/lucidrains/phasic-policy-gradient": {
        "extra-tags": [],
        "date": "2020-09-27",
        "title": "phasic-policy-gradient",
        "summary": "An implementation of Phasic Policy Gradient, a proposed improvement of Proximal Policy Gradients, in Pytorch",
        "tags": [
            "reinforcement-learning",
            "python",
            "proximal-policy-optimization",
            "artificial-intelligence"
        ]
    },
    "https://github.com/trent-b/iterative-stratification": {
        "extra-tags": [],
        "date": "2018-02-04",
        "title": "iterative-stratification",
        "summary": "scikit-learn cross validators for iterative stratification of multilabel data",
        "tags": [
            "multilabel",
            "multilabel-classification",
            "python",
            "stratification",
            "scikit-learn",
            "cross-validation"
        ]
    },
    "https://github.com/vwxyzjn/invalid-action-masking": {
        "extra-tags": [],
        "date": "2020-06-18",
        "title": "invalid-action-masking",
        "summary": "Source Code for A Closer Look at Invalid Action Masking in Policy Gradient Algorithms",
        "tags": [
            "python"
        ]
    },
    "https://github.com/pola-rs/polars": {
        "extra-tags": [],
        "date": "2020-05-13",
        "title": "polars",
        "summary": "Fast multi-threaded, hybrid-out-of-core DataFrame library in Rust | Python | Node.js",
        "tags": [
            "dataframe-library",
            "dataframes",
            "arrow",
            "python",
            "rust",
            "dataframe",
            "out-of-core"
        ]
    },
    "https://github.com/deppen8/pandas-vet": {
        "extra-tags": [],
        "date": "2019-02-25",
        "title": "pandas-vet",
        "summary": "A plugin for Flake8 that checks pandas code",
        "tags": [
            "python"
        ]
    },
    "https://github.com/lucidrains/performer-pytorch": {
        "extra-tags": [],
        "date": "2020-10-03",
        "title": "performer-pytorch",
        "summary": "An implementation of Performer, a linear attention-based transformer, in Pytorch",
        "tags": [
            "python",
            "attention-mechanism",
            "transformers",
            "deep-learning",
            "attention",
            "artificial-intelligence"
        ]
    },
    "https://github.com/lucidrains/lambda-networks": {
        "extra-tags": [],
        "date": "2020-10-08",
        "title": "lambda-networks",
        "summary": "Implementation of LambdaNetworks, a new approach to image recognition that reaches SOTA with less compute",
        "tags": [
            "computer-vision",
            "python",
            "attention-mechanism",
            "deep-learning",
            "attention",
            "artificial-intelligence"
        ]
    },
    "https://github.com/anguelos/tormentor": {
        "extra-tags": [
            "pytorch",
            "augmentation"
        ],
        "date": "2020-03-06",
        "title": "tormentor",
        "summary": "Pytorch augmentation",
        "tags": [
            "python"
        ]
    },
    "https://github.com/lucidrains/byol-pytorch": {
        "extra-tags": [],
        "date": "2020-06-16",
        "title": "byol-pytorch",
        "summary": "Usable Implementation of \"Bootstrap Your Own Latent\" self-supervised learning, from Deepmind, in Pytorch",
        "tags": [
            "self-supervised-learning",
            "python",
            "deep-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/PetarV-/TikZ": {
        "extra-tags": [],
        "date": "2016-07-18",
        "title": "TikZ",
        "summary": "Complete collection of my PGF/TikZ figures.",
        "tags": [
            "latex",
            "tex",
            "pgf",
            "graphics",
            "tikz",
            "tikz-figures"
        ]
    },
    "https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail": {
        "extra-tags": [],
        "date": "2017-08-22",
        "title": "pytorch-a2c-ppo-acktr-gail",
        "summary": "PyTorch implementation of Advantage Actor Critic (A2C), Proximal Policy Optimization (PPO), Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (ACKTR) and Generative Adversarial Imitation Learning (GAIL).",
        "tags": [
            "actor-critic",
            "atari",
            "ale",
            "deep-reinforcement-learning",
            "natural-gradients",
            "roboschool",
            "acktr",
            "python",
            "reinforcement-learning",
            "advantage-actor-critic",
            "hessian",
            "deep-learning",
            "kronecker-factored-approximation",
            "a2c",
            "kfac",
            "continuous-control",
            "mujoco",
            "second-order",
            "pytorch",
            "proximal-policy-optimization",
            "ppo"
        ]
    },
    "https://github.com/lucidrains/vit-pytorch": {
        "extra-tags": [],
        "date": "2020-10-03",
        "title": "vit-pytorch",
        "summary": "Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch",
        "tags": [
            "image-classification",
            "python",
            "attention-mechanism",
            "transformers",
            "computer-vision",
            "artificial-intelligence"
        ]
    },
    "https://github.com/mxrch/GHunt": {
        "extra-tags": [
            "framework"
        ],
        "date": "2020-10-02",
        "title": "GHunt",
        "summary": "\ud83d\udd75\ufe0f\u200d\u2642\ufe0f Offensive Google framework.",
        "tags": [
            "osint",
            "python",
            "hideandsec",
            "google"
        ]
    },
    "https://github.com/Farama-Foundation/MicroRTS-Py": {
        "extra-tags": [],
        "date": "2019-04-03",
        "title": "MicroRTS-Py",
        "summary": "A simple and highly efficient RTS-game-inspired environment for reinforcement learning (formerly Gym-MicroRTS)",
        "tags": [
            "python"
        ]
    },
    "https://github.com/rapidsai/deeplearning": {
        "extra-tags": [
            "deeplearning"
        ],
        "date": "2019-06-11",
        "title": "deeplearning",
        "summary": "",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/deepmind/dqn_zoo": {
        "extra-tags": [],
        "date": "2020-09-22",
        "title": "dqn_zoo",
        "summary": "DQN Zoo is a collection of reference implementations of reinforcement learning agents developed at DeepMind based on the Deep Q-Network (DQN) agent.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/deepmind/open_spiel": {
        "extra-tags": [],
        "date": "2019-07-22",
        "title": "open_spiel",
        "summary": "OpenSpiel is a collection of environments and algorithms for research in general reinforcement learning and search/planning in games.",
        "tags": [
            "python",
            "reinforcement-learning",
            "multiagent",
            "games",
            "cpp",
            "c++"
        ]
    },
    "https://github.com/simontudge/replicator": {
        "extra-tags": [
            "game"
        ],
        "date": "2015-07-16",
        "title": "replicator",
        "summary": "Python module for easily studying the evolutionary dynamics of game theory",
        "tags": [
            "python"
        ]
    },
    "https://github.com/bruzat/SC2ReinforcementLearning": {
        "extra-tags": [
            "reinforcement",
            "learning"
        ],
        "date": "2019-02-05",
        "title": "SC2ReinforcementLearning",
        "summary": "Work on SC2 minigames with reinforcement learning",
        "tags": [
            "keras-tensorflow",
            "python",
            "pysc2-mini-games",
            "policy-gradient",
            "startcraft2",
            "pysc2",
            "ppo"
        ]
    },
    "https://github.com/RchalYang/torchrl": {
        "extra-tags": [],
        "date": "2018-12-22",
        "title": "torchrl",
        "summary": "Pytorch Implementation of Reinforcement Learning Algorithms ( Soft Actor Critic(SAC)/ DDPG / TD3 /DQN / A2C/ PPO / TRPO)",
        "tags": [
            "gym",
            "python",
            "reinforcement-learning",
            "sac",
            "mujoco",
            "algorithm",
            "ddpg",
            "dqn",
            "policy-agent",
            "rl-algorithms",
            "td3",
            "trpo",
            "pytorch",
            "ppo"
        ]
    },
    "https://github.com/raphaelsty/abayes": {
        "extra-tags": [],
        "date": "2020-09-08",
        "title": "abayes",
        "summary": "Autoregressive Bayesian linear model",
        "tags": [
            "python",
            "bayesian-inference",
            "online",
            "autoregressive",
            "machine-learning",
            "linear-regression",
            "bayesian",
            "bayes"
        ]
    },
    "https://github.com/senguptaumd/Background-Matting": {
        "extra-tags": [
            "background"
        ],
        "date": "2020-03-18",
        "title": "Background-Matting",
        "summary": "Background Matting: The World is Your Green Screen",
        "tags": [
            "python"
        ]
    },
    "https://github.com/younggyoseo/pytorch-nfsp": {
        "extra-tags": [],
        "date": "2018-11-29",
        "title": "pytorch-nfsp",
        "summary": "Implementation of Deep Reinforcement Learning from Self-Play in Imperfect-Information Games (Heinrich and Silver, 2016)",
        "tags": [
            "marl",
            "nfsp",
            "python"
        ]
    },
    "https://github.com/hardmaru/slimevolleygym": {
        "extra-tags": [],
        "date": "2020-06-08",
        "title": "slimevolleygym",
        "summary": "A simple OpenAI Gym environment for single and multi-agent reinforcement learning",
        "tags": [
            "python"
        ]
    },
    "https://github.com/koaning/human-learn": {
        "extra-tags": [],
        "date": "2020-07-11",
        "title": "human-learn",
        "summary": "Natural Intelligence is still a pretty good idea.",
        "tags": [
            "benchmark",
            "scikit-learn",
            "jupyter notebook",
            "machine-learning"
        ]
    },
    "https://github.com/nikhilbarhate99/PPO-PyTorch": {
        "extra-tags": [],
        "date": "2018-09-27",
        "title": "PPO-PyTorch",
        "summary": "Minimal implementation of clipped objective Proximal Policy Optimization (PPO) in PyTorch",
        "tags": [
            "python",
            "reinforcement-learning",
            "pytorch-tutorial",
            "deep-reinforcement-learning",
            "ppo-pytorch",
            "proximal-policy-optimization",
            "policy-gradient",
            "pytorch",
            "deep-learning",
            "ppo",
            "pytorch-implmention",
            "reinforcement-learning-algorithms"
        ]
    },
    "https://github.com/boschresearch/unetgan": {
        "extra-tags": [],
        "date": "2020-08-27",
        "title": "unetgan",
        "summary": "Official Implementation of the paper \"A U-Net Based Discriminator for Generative Adversarial Networks\" (CVPR 2020)",
        "tags": [
            "ffhq",
            "python",
            "generative-adversarial-network",
            "u-net",
            "unet-gan",
            "machine-learning",
            "image-generation",
            "cvpr2020",
            "biggan",
            "computer-vision",
            "gan",
            "bcai"
        ]
    },
    "https://github.com/ArnaudFickinger/gym-multigrid": {
        "extra-tags": [],
        "date": "2020-03-30",
        "title": "gym-multigrid",
        "summary": "Lightweight multi-agent gridworld Gym environment",
        "tags": [
            "gym",
            "python",
            "multiplayer-game",
            "multi-agent-systems",
            "gridworld-environment",
            "multiagent-reinforcement-learning",
            "multiagent-systems",
            "gridworld",
            "gym-environment",
            "multi-agent-reinforcement-learning",
            "multi-agent"
        ]
    },
    "https://github.com/lcswillems/torch-ac": {
        "extra-tags": [],
        "date": "2019-04-07",
        "title": "torch-ac",
        "summary": "Recurrent and multi-process PyTorch implementation of deep reinforcement Actor-Critic algorithms A2C and PPO",
        "tags": [
            "a3c",
            "reward-shaping",
            "recurrent-neural-networks",
            "actor-critic",
            "reinforcement-learning",
            "python",
            "recurrent",
            "a2c",
            "minigrid",
            "advantage-actor-critic",
            "multi-process",
            "deep-reinforcement-learning",
            "pytorch",
            "proximal-policy-optimization",
            "ppo"
        ]
    },
    "https://github.com/kwotsin/mimicry": {
        "extra-tags": [],
        "date": "2020-03-31",
        "title": "mimicry",
        "summary": "[CVPR 2020 Workshop] A PyTorch GAN library that reproduces research results for popular GANs.",
        "tags": [
            "cgan",
            "sngan",
            "python",
            "reproducibility",
            "infomax-gan",
            "dcgan",
            "machine-learning",
            "sngan-projection",
            "ssgan",
            "cvpr2020",
            "cvpr20",
            "gans",
            "generative-models",
            "pytorch",
            "wgan-gp",
            "cvpr",
            "gan",
            "generative-adversarial-networks"
        ]
    },
    "https://github.com/sfujim/TD3": {
        "extra-tags": [],
        "date": "2018-02-22",
        "title": "TD3",
        "summary": "Author's PyTorch implementation of TD3 for OpenAI gym tasks",
        "tags": [
            "python"
        ]
    },
    "https://github.com/AlexiaJM/AdversarialConsistentScoreMatching": {
        "extra-tags": [],
        "date": "2020-09-09",
        "title": "AdversarialConsistentScoreMatching",
        "summary": "Code for paper \"Adversarial score matching and improved sampling for image generation\"",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/maxence-charriere/lofimusic": {
        "extra-tags": [],
        "date": "2020-09-07",
        "title": "lofimusic",
        "summary": "Lofimusic.app is an installable Progressive web app (PWA) that lists and displays famous YouTube Lo-Fi radios.",
        "tags": [
            "lofi",
            "golang",
            "pwa",
            "go"
        ]
    },
    "https://github.com/3outeille/CNNumpy": {
        "extra-tags": [],
        "date": "2019-12-05",
        "title": "CNNumpy",
        "summary": "A Numpy implementation of a Convolutional Neural Network: slow & fast (im2col/col2im).",
        "tags": [
            "python3",
            "col2im",
            "im2col",
            "python",
            "convolutional-neural-networks",
            "numpy",
            "cnn"
        ]
    },
    "https://github.com/3outeille/GANumpy": {
        "extra-tags": [
            "adversarial"
        ],
        "date": "2020-08-20",
        "title": "GANumpy",
        "summary": "A Numpy implementation of a Generative Adversarial Network.",
        "tags": [
            "python",
            "autodiff",
            "mnist",
            "numpy",
            "yaae",
            "deeplearning",
            "gan"
        ]
    },
    "https://github.com/luopeixiang/im2latex": {
        "extra-tags": [],
        "date": "2019-03-26",
        "title": "im2latex",
        "summary": "Pytorch implemention of Deep CNN Encoder + LSTM Decoder with Attention for Image to Latex",
        "tags": [
            "im2latex",
            "python",
            "show-and-tell",
            "encoder-decoder-model",
            "seq2seq",
            "pytorch",
            "imagecaptioning"
        ]
    },
    "https://github.com/quantumiracle/Popular-RL-Algorithms": {
        "extra-tags": [],
        "date": "2019-04-19",
        "title": "Popular-RL-Algorithms",
        "summary": "PyTorch implementation of Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3), Actor-Critic (AC/A2C), Proximal Policy Optimization (PPO), QT-Opt, PointNet..",
        "tags": [
            "state-of-the-art",
            "soft-actor-critic",
            "jupyter notebook",
            "reinforcement-learning"
        ]
    },
    "https://github.com/csrhddlam/axial-deeplab": {
        "extra-tags": [
            "pytorch"
        ],
        "date": "2020-07-09",
        "title": "axial-deeplab",
        "summary": "This is a PyTorch re-implementation of Axial-DeepLab (ECCV 2020 Spotlight)",
        "tags": [
            "python"
        ]
    },
    "https://github.com/google/objax": {
        "extra-tags": [
            "jax"
        ],
        "date": "2020-08-20",
        "title": "objax",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/epfml/OptML_course": {
        "extra-tags": [],
        "date": "2018-02-21",
        "title": "OptML_course",
        "summary": "EPFL Course - Optimization for Machine Learning - CS-439",
        "tags": [
            "tex"
        ]
    },
    "https://github.com/shariqiqbal2810/MAAC": {
        "extra-tags": [],
        "date": "2018-10-16",
        "title": "MAAC",
        "summary": "Code for \"Actor-Attention-Critic for Multi-Agent Reinforcement Learning\" ICML 2019",
        "tags": [
            "python"
        ]
    },
    "https://github.com/iamadamdev/bypass-paywalls-chrome": {
        "extra-tags": [],
        "date": "2018-04-07",
        "title": "bypass-paywalls-chrome",
        "summary": "Bypass Paywalls web browser extension for Chrome and Firefox.",
        "tags": [
            "chrome",
            "bypass",
            "bypass-paywalls",
            "paywall",
            "firefox-addon",
            "chrome-extension",
            "javascript",
            "firefox",
            "firefox-extension",
            "chrome-extensions"
        ]
    },
    "https://github.com/Farama-Foundation/PettingZoo": {
        "extra-tags": [],
        "date": "2020-01-20",
        "title": "PettingZoo",
        "summary": "A standard API for multi-agent reinforcement learning environments, with popular reference environments and related utilities",
        "tags": [
            "gym",
            "python",
            "reinforcement-learning",
            "multi-agent-reinforcement-learning",
            "gymnasium",
            "api"
        ]
    },
    "https://github.com/ml-jku/hopfield-layers": {
        "extra-tags": [],
        "date": "2020-07-09",
        "title": "hopfield-layers",
        "summary": "Hopfield Networks is All You Need",
        "tags": [
            "python"
        ]
    },
    "https://github.com/taliesinb/spieeltjie": {
        "extra-tags": [
            "experiments",
            "game"
        ],
        "date": "2019-08-13",
        "title": "spieeltjie",
        "summary": "Small lab for experiments with PSRO, game theoretic niching, etc.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/deepmind/dm_control": {
        "extra-tags": [],
        "date": "2017-12-29",
        "title": "dm_control",
        "summary": "DeepMind's software stack for physics-based simulation and Reinforcement Learning environments, using MuJoCo.",
        "tags": [
            "python",
            "reinforcement-learning",
            "neural-networks",
            "mujoco",
            "machine-learning",
            "physics-simulation",
            "deep-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/facebookresearch/swav": {
        "extra-tags": [
            "pytorch",
            "arxiv"
        ],
        "date": "2020-07-16",
        "title": "swav",
        "summary": "PyTorch implementation of SwAV https//arxiv.org/abs/2006.09882",
        "tags": [
            "python"
        ]
    },
    "https://github.com/SoyGema/Numpy_exploration": {
        "extra-tags": [],
        "date": "2018-11-19",
        "title": "Numpy_exploration",
        "summary": "Numpy library exploration . Implementations of algorithms in numpy",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/drvinceknight/Nashpy": {
        "extra-tags": [],
        "date": "2016-11-03",
        "title": "Nashpy",
        "summary": "A python library for 2 player games.",
        "tags": [
            "python",
            "algorithm",
            "equilibria",
            "nash",
            "computer-science",
            "mathematics",
            "game"
        ]
    },
    "https://github.com/deepmind/pycolab": {
        "extra-tags": [],
        "date": "2017-11-14",
        "title": "pycolab",
        "summary": "A highly-customisable gridworld game engine with some batteries included. Make your own gridworld games to test reinforcement learning agents!",
        "tags": [
            "python"
        ]
    },
    "https://github.com/openai/multiagent-particle-envs": {
        "extra-tags": [],
        "date": "2017-08-17",
        "title": "multiagent-particle-envs",
        "summary": "Code for a multi-agent particle environment used in the paper \"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments\"",
        "tags": [
            "paper",
            "python"
        ]
    },
    "https://github.com/google-research/fast-soft-sort": {
        "extra-tags": [],
        "date": "2020-06-08",
        "title": "fast-soft-sort",
        "summary": "Fast Differentiable Sorting and Ranking",
        "tags": [
            "python",
            "jax",
            "ranking",
            "sorting",
            "tensorflow",
            "differentiable",
            "pytorch"
        ]
    },
    "https://github.com/3outeille/Yaae": {
        "extra-tags": [],
        "date": "2020-05-27",
        "title": "Yaae",
        "summary": "Yaae: Yet another autodiff engine (written in Numpy).",
        "tags": [
            "autodifferentiation",
            "autograd",
            "python",
            "numpy"
        ]
    },
    "https://github.com/deel-ai/deel-lip": {
        "extra-tags": [],
        "date": "2020-05-11",
        "title": "deel-lip",
        "summary": "Tensorflow 2 implementation of k-Lipschitz layers.",
        "tags": [
            "keras-tensorflow",
            "python",
            "lipschitz",
            "keras",
            "wasserstein-distance-estimation",
            "lipschitz-regularization",
            "tensorflow",
            "lipschitz-functions",
            "spectral-normalization",
            "tensorflow2"
        ]
    },
    "https://github.com/facebookresearch/detr": {
        "extra-tags": [
            "detection",
            "transformers"
        ],
        "date": "2020-05-26",
        "title": "detr",
        "summary": "End-to-End Object Detection with Transformers",
        "tags": [
            "python"
        ]
    },
    "https://github.com/nalepae/pandarallel": {
        "extra-tags": [],
        "date": "2019-03-10",
        "title": "pandarallel",
        "summary": "A simple and efficient tool to parallelize Pandas operations on all available\u00a0CPUs",
        "tags": [
            "pandas",
            "parallel",
            "python"
        ]
    },
    "https://github.com/koulanurag/ma-gym": {
        "extra-tags": [],
        "date": "2019-06-10",
        "title": "ma-gym",
        "summary": "A collection of multi agent environments based on OpenAI gym.",
        "tags": [
            "gym",
            "collaborative",
            "python",
            "reinforcement-learning",
            "openai-gym",
            "multi-agent",
            "environment"
        ]
    },
    "https://github.com/anopara/genetic-drawing": {
        "extra-tags": [],
        "date": "2020-06-05",
        "title": "genetic-drawing",
        "summary": "A genetic algorithm toy project for drawing",
        "tags": [
            "python"
        ]
    },
    "https://github.com/davidrzs/latexcss": {
        "extra-tags": [],
        "date": "2017-05-29",
        "title": "latexcss",
        "summary": "Nearly Classless CSS file to give html a latex-like look",
        "tags": [
            "latex",
            "classless-theme",
            "css"
        ]
    },
    "https://github.com/numpy/numpy-stubs": {
        "extra-tags": [
            "typing",
            "numpy"
        ],
        "date": "2017-11-22",
        "title": "numpy-stubs",
        "summary": "Experimental typing stubs for NumPy",
        "tags": [
            "python"
        ]
    },
    "https://github.com/masesk/process-google-dataset": {
        "extra-tags": [],
        "date": "2020-05-14",
        "title": "process-google-dataset",
        "summary": "Process Google Dataset is a tool to download and process images for neural networks from a Google Image Search using a Chrome extension and a simple Python code.",
        "tags": [
            "neural-network",
            "neural-networks",
            "javascript",
            "machine-learning",
            "ai",
            "image-processing",
            "google-image-search",
            "image-recognition",
            "object-detection"
        ]
    },
    "https://github.com/wookayin/gpustat": {
        "extra-tags": [],
        "date": "2016-04-24",
        "title": "gpustat",
        "summary": "\ud83d\udcca A simple command-line utility for querying and monitoring GPU status",
        "tags": [
            "python",
            "nvidia-smi",
            "gpustat",
            "command-line",
            "gpu",
            "monitoring"
        ]
    },
    "https://github.com/MatthewZMD/.emacs.d": {
        "extra-tags": [],
        "date": "2019-02-05",
        "title": ".emacs.d",
        "summary": "M-EMACS, a full-featured GNU Emacs configuration distribution",
        "tags": [
            "m-emacs",
            "emacs",
            "emacs-configuration",
            "emacs lisp",
            "emacs-lisp"
        ]
    },
    "https://github.com/fat-dash/fat-dash": {
        "extra-tags": [
            "dash",
            "dashboard"
        ],
        "date": "2019-02-19",
        "title": "fat-dash",
        "summary": "Dashboard for FPS Aim Trainer",
        "tags": []
    },
    "https://github.com/SymJAX/SymJAX": {
        "extra-tags": [
            "documentation"
        ],
        "date": "2019-09-04",
        "title": "SymJAX",
        "summary": "Documentation:",
        "tags": [
            "python",
            "jax",
            "numpy",
            "dataset",
            "tensorflow",
            "theano",
            "deep-neural-networks",
            "lasagne",
            "deep-learning"
        ]
    },
    "https://github.com/french-ai/reinforcement": {
        "extra-tags": [],
        "date": "2020-05-16",
        "title": "reinforcement",
        "summary": "Reinforcement learning modular with pytorch",
        "tags": [
            "gym",
            "double-dqn",
            "agent",
            "reinforcement-learning",
            "dqn-pytorch",
            "dueling-dqn",
            "dqn",
            "jupyter notebook",
            "pytorch",
            "tensorboard"
        ]
    },
    "https://github.com/colah/colah.github.io": {
        "extra-tags": [
            "github"
        ],
        "date": "2014-03-31",
        "title": "colah.github.io",
        "summary": "",
        "tags": [
            "html"
        ]
    },
    "https://github.com/PhoenixDL/rising": {
        "extra-tags": [],
        "date": "2019-11-17",
        "title": "rising",
        "summary": "Provides everything needed for high performance data loading and augmentation in pytorch.",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/dair-ai/ml-visuals": {
        "extra-tags": [
            "ml",
            "templates"
        ],
        "date": "2020-05-17",
        "title": "ml-visuals",
        "summary": "\ud83c\udfa8 ML Visuals contains figures and templates which you can reuse and customize to improve your scientific writing.",
        "tags": [
            "natural-language-processing",
            "design",
            "machine-learning",
            "deep-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/subeeshvasu/Awesome-Learning-with-Label-Noise": {
        "extra-tags": [],
        "date": "2019-07-17",
        "title": "Awesome-Learning-with-Label-Noise",
        "summary": "A curated list of resources for Learning with Noisy Labels",
        "tags": [
            "noisy-labels",
            "unreliable-labels",
            "noisy-data",
            "deep-neural-networks",
            "label-noise",
            "robust-learning"
        ]
    },
    "https://github.com/thib-s/contour-asciicam": {
        "extra-tags": [
            "art",
            "image"
        ],
        "date": "2019-07-03",
        "title": "contour-asciicam",
        "summary": "yet an other image to assci art converter, however this one extract the contours of the image",
        "tags": [
            "python"
        ]
    },
    "https://github.com/raphaelsty/M5-Forecasting-Accuracy": {
        "extra-tags": [],
        "date": "2020-03-30",
        "title": "M5-Forecasting-Accuracy",
        "summary": "Deploying machine learning easily",
        "tags": [
            "online-learning",
            "machine-learning",
            "kaggle",
            "flask",
            "digitalocean",
            "deployment",
            "jupyter notebook",
            "kaggle-solution"
        ]
    },
    "https://github.com/Baekalfen/PyBoy": {
        "extra-tags": [],
        "date": "2015-05-29",
        "title": "PyBoy",
        "summary": "Game Boy emulator written in Python",
        "tags": [
            "python",
            "gameboy",
            "gameboy-emulator",
            "cython",
            "emulator",
            "pypy",
            "gameboy-emulator-library"
        ]
    },
    "https://github.com/kornia/kornia": {
        "extra-tags": [],
        "date": "2018-08-22",
        "title": "kornia",
        "summary": "Open Source Differentiable Computer Vision Library",
        "tags": [
            "neural-network",
            "python",
            "machine-learning",
            "image-processing",
            "pytorch",
            "deep-learning",
            "computer-vision",
            "artificial-intelligence"
        ]
    },
    "https://github.com/bonlime/pytorch-tools": {
        "extra-tags": [],
        "date": "2019-08-12",
        "title": "pytorch-tools",
        "summary": "Tool box for PyTorch ",
        "tags": [
            "python"
        ]
    },
    "https://github.com/linjx-ustc1106/TuiGAN-PyTorch": {
        "extra-tags": [],
        "date": "2020-04-09",
        "title": "TuiGAN-PyTorch",
        "summary": "PyTorch Implementation of ECCV 2020 Spotlight \"TuiGAN: Learning Versatile Image-to-Image Translation with Two Unpaired Images\"",
        "tags": [
            "python"
        ]
    },
    "https://github.com/firmai/deltapy": {
        "extra-tags": [
            "data"
        ],
        "date": "2020-04-08",
        "title": "deltapy",
        "summary": "DeltaPy - Tabular Data Augmentation (by @firmai)",
        "tags": [
            "feature-extraction",
            "machine-learning",
            "time-series",
            "feature-engineering",
            "augmentation",
            "jupyter notebook",
            "data-science",
            "tabular-data",
            "data-augmentation",
            "finance"
        ]
    },
    "https://github.com/zhanghang1989/ResNeSt": {
        "extra-tags": [
            "attention"
        ],
        "date": "2020-03-15",
        "title": "ResNeSt",
        "summary": "ResNeSt: Split-Attention Networks",
        "tags": [
            "resnest",
            "resnet",
            "python",
            "split-attention-networks",
            "pytorch",
            "deep-learning",
            "detectron-models"
        ]
    },
    "https://github.com/dhaitz/mplcyberpunk": {
        "extra-tags": [],
        "date": "2020-03-29",
        "title": "mplcyberpunk",
        "summary": "\"Cyberpunk style\" for matplotlib plots",
        "tags": [
            "dataviz",
            "python",
            "matplotlib",
            "plotting",
            "visualization"
        ]
    },
    "https://github.com/Unity-Technologies/ml-agents": {
        "extra-tags": [],
        "date": "2017-09-08",
        "title": "ml-agents",
        "summary": "The Unity Machine Learning Agents Toolkit (ML-Agents) is an open-source project that enables games and simulations to serve as environments for training intelligent agents using deep reinforcement learning and imitation learning.",
        "tags": [
            "reinforcement-learning",
            "neural-networks",
            "unity3d",
            "machine-learning",
            "deep-reinforcement-learning",
            "unity",
            "c#",
            "deep-learning"
        ]
    },
    "https://github.com/simoninithomas/the_mayan_adventure": {
        "extra-tags": [],
        "date": "2020-03-23",
        "title": "the_mayan_adventure",
        "summary": "The Mayan Adventure is an open-source reinforcement learning environment for Unity ML-Agents.  In this environment, you train your agent (Indie) to find the golden statue in this dangerous environment full of traps.",
        "tags": [
            "shaderlab",
            "reinforcement-learning",
            "unity3d",
            "deep-reinforcement-learning",
            "unity",
            "ml-agents"
        ]
    },
    "https://github.com/DeepVoltaire/AutoAugment": {
        "extra-tags": [],
        "date": "2018-06-05",
        "title": "AutoAugment",
        "summary": "Unofficial implementation of the ImageNet, CIFAR 10 and SVHN Augmentation Policies learned by AutoAugment using pillow",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/giddyyupp/ganilla": {
        "extra-tags": [],
        "date": "2018-12-22",
        "title": "ganilla",
        "summary": "Official Pytorch implementation of GANILLA",
        "tags": [
            "python",
            "generative-adversarial-network",
            "image-to-image-translation",
            "illustration-drawing",
            "illustration",
            "gans",
            "pytorch",
            "deep-learning",
            "illustrator",
            "illustrations",
            "gan"
        ]
    },
    "https://github.com/clarete/forbiddenfruit": {
        "extra-tags": [],
        "date": "2013-04-03",
        "title": "forbiddenfruit",
        "summary": "Patch built-in python objects",
        "tags": [
            "monkey-patching",
            "python"
        ]
    },
    "https://github.com/BayesWatch/cinic-10": {
        "extra-tags": [],
        "date": "2018-09-21",
        "title": "cinic-10",
        "summary": "A drop-in replacement for CIFAR-10.",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/Lightning-AI/lightning": {
        "extra-tags": [],
        "date": "2019-03-31",
        "title": "lightning",
        "summary": "Deep learning framework to train, deploy, and ship AI products Lightning fast.",
        "tags": [
            "python",
            "machine-learning",
            "ai",
            "data-science",
            "pytorch",
            "deep-learning",
            "artificial-intelligence"
        ]
    },
    "https://github.com/Hadjubuntu/sweet-rl": {
        "extra-tags": [],
        "date": "2019-11-09",
        "title": "sweet-rl",
        "summary": "The sweetest Reinforcement Learning framework",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/google-research/augmix": {
        "extra-tags": [],
        "date": "2019-12-05",
        "title": "augmix",
        "summary": "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty",
        "tags": [
            "python"
        ]
    },
    "https://github.com/TotallyNotChase/glitch-this": {
        "extra-tags": [],
        "date": "2020-02-15",
        "title": "glitch-this",
        "summary": ":camera: Glitchify images and GIF - with highly customizable options! ",
        "tags": [
            "commandline-tool",
            "glitchify-images",
            "image-manipulation",
            "glitched-gifs",
            "python",
            "glitching-intensity",
            "scan-lines",
            "glitch-effect",
            "glitch-art",
            "glitching",
            "gifs",
            "glitched-images"
        ]
    },
    "https://github.com/rois-codh/kaokore": {
        "extra-tags": [
            "dataset",
            "collection"
        ],
        "date": "2020-01-16",
        "title": "kaokore",
        "summary": "Dataset for the Collection of Facial Expressions from Japanese Artwork",
        "tags": [
            "python"
        ]
    },
    "https://github.com/remykarem/mixed-naive-bayes": {
        "extra-tags": [],
        "date": "2019-10-05",
        "title": "mixed-naive-bayes",
        "summary": "Naive Bayes with support for categorical and continuous data",
        "tags": [
            "categorical-data",
            "naive-bayes-algorithm",
            "machine-learning",
            "python"
        ]
    },
    "https://github.com/dheera/mnist-clock": {
        "extra-tags": [
            "mnist"
        ],
        "date": "2018-08-19",
        "title": "mnist-clock",
        "summary": "A clock that displays digits using randomly selected MNIST digits.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/deepmind/dm-haiku": {
        "extra-tags": [
            "library"
        ],
        "date": "2020-02-18",
        "title": "dm-haiku",
        "summary": "JAX-based neural network library",
        "tags": [
            "python",
            "neural-networks",
            "jax",
            "machine-learning",
            "deep-neural-networks",
            "deep-learning"
        ]
    },
    "https://github.com/deepmind/rlax": {
        "extra-tags": [
            "flax"
        ],
        "date": "2020-02-18",
        "title": "rlax",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/MarcSkovMadsen/awesome-streamlit": {
        "extra-tags": [],
        "date": "2019-10-06",
        "title": "awesome-streamlit",
        "summary": "The purpose of this project is to share knowledge on how awesome Streamlit is and can be",
        "tags": [
            "innovation",
            "python",
            "analytics",
            "exploration",
            "html",
            "apps",
            "machine-learning",
            "streamlit",
            "data",
            "trading",
            "data-science",
            "mathematics",
            "fun",
            "models",
            "awesome-list",
            "finance"
        ]
    },
    "https://github.com/python/typing": {
        "extra-tags": [],
        "date": "2014-09-29",
        "title": "typing",
        "summary": "Python static typing home. Hosts the documentation and a user help forum.",
        "tags": [
            "gradual-typing",
            "types",
            "python",
            "typing",
            "static-typing"
        ]
    },
    "https://github.com/jason718/awesome-self-supervised-learning": {
        "extra-tags": [],
        "date": "2018-02-10",
        "title": "awesome-self-supervised-learning",
        "summary": "A curated list of awesome self-supervised methods",
        "tags": [
            "natural-language-processing",
            "self-supervised",
            "reinforcement-learning",
            "machine-learning",
            "deep-learning",
            "robotics",
            "computer-vision"
        ]
    },
    "https://github.com/clovaai/assembled-cnn": {
        "extra-tags": [],
        "date": "2020-01-17",
        "title": "assembled-cnn",
        "summary": "Tensorflow implementation of \"Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network\"",
        "tags": [
            "image-classification",
            "python",
            "convolutional-neural-networks",
            "inference-throughput",
            "tensorflow",
            "robustness",
            "food-101",
            "deep-learning",
            "computer-vision",
            "imagenet",
            "mce",
            "transfer-learning"
        ]
    },
    "https://github.com/rust-lang/rust": {
        "extra-tags": [],
        "date": "2010-06-16",
        "title": "rust",
        "summary": "Empowering everyone to build reliable and efficient software.",
        "tags": [
            "hacktoberfest",
            "rust",
            "compiler",
            "language"
        ]
    },
    "https://github.com/guillaume-be/rust-bert": {
        "extra-tags": [],
        "date": "2020-01-25",
        "title": "rust-bert",
        "summary": "Rust native ready-to-use NLP pipelines and transformer-based models (BERT, DistilBERT, GPT2,...)",
        "tags": [
            "language-generation",
            "transformer",
            "rust",
            "question-answering",
            "translation",
            "machine-learning",
            "bart",
            "electra",
            "ner",
            "roberta",
            "rust-lang",
            "gpt-2",
            "nlp",
            "sentiment-analysis",
            "deep-learning",
            "bert",
            "gpt"
        ]
    },
    "https://github.com/LaurentMazare/tch-rs": {
        "extra-tags": [],
        "date": "2019-02-16",
        "title": "tch-rs",
        "summary": "Rust bindings for the C++ api of PyTorch.",
        "tags": [
            "neural-network",
            "rust",
            "machine-learning",
            "pytorch",
            "deep-learning"
        ]
    },
    "https://github.com/pytorchbearer/torchbearer": {
        "extra-tags": [
            "library"
        ],
        "date": "2018-03-12",
        "title": "torchbearer",
        "summary": "torchbearer: A model fitting library for PyTorch",
        "tags": [
            "python3",
            "python",
            "model-fitting",
            "differentiable-programming",
            "machine-learning",
            "pytorch",
            "deep-learning"
        ]
    },
    "https://github.com/huggingface/pytorch-image-models": {
        "extra-tags": [],
        "date": "2019-02-02",
        "title": "pytorch-image-models",
        "summary": "PyTorch image models, scripts, pretrained weights -- ResNet, ResNeXT, EfficientNet, EfficientNetV2, NFNet, Vision Transformer, MixNet, MobileNet-V3/V2, RegNet, DPN, CSPNet, and more",
        "tags": [
            "normalization-free-training",
            "mobile-deep-learning",
            "dual-path-networks",
            "resnet",
            "mixnet",
            "augmix",
            "cnn-classification",
            "pretrained-weights",
            "randaugment",
            "python",
            "mobilenet-v2",
            "mnasnet",
            "nfnets",
            "pretrained-models",
            "imagenet-classifier",
            "distributed-training",
            "mobilenetv3",
            "vision-transformer-models",
            "efficientnet-training",
            "pytorch",
            "efficientnet"
        ]
    },
    "https://github.com/gyli/PyWaffle": {
        "extra-tags": [],
        "date": "2017-11-14",
        "title": "PyWaffle",
        "summary": "\ud83e\uddc7 Make Waffle Charts in Python.",
        "tags": [
            "python",
            "waffle",
            "matplotlib",
            "charts",
            "data-visualization",
            "waffle-charts",
            "visualization"
        ]
    },
    "https://github.com/pschanely/CrossHair": {
        "extra-tags": [],
        "date": "2017-08-29",
        "title": "CrossHair",
        "summary": "An analysis tool for Python that blurs the line between testing and type systems.",
        "tags": [
            "testing",
            "type-systems",
            "contracts",
            "python",
            "fuzzing",
            "concolic-execution",
            "static-analysis",
            "testing-framework",
            "dynamic-analysis",
            "hacktoberfest",
            "symbolic-execution",
            "z3"
        ]
    },
    "https://github.com/ufoym/deepo": {
        "extra-tags": [],
        "date": "2017-10-27",
        "title": "deepo",
        "summary": "Setup and customize deep learning environment in seconds.",
        "tags": [
            "dockerfile-generator",
            "docker-image",
            "python",
            "caffe2",
            "sonnet",
            "onnx",
            "keras",
            "mxnet",
            "chainer",
            "jupyter",
            "tensorflow",
            "theano",
            "torch",
            "lasagne",
            "deep-learning",
            "pytorch",
            "caffe",
            "cntk"
        ]
    },
    "https://github.com/epfml/attention-cnn": {
        "extra-tags": [],
        "date": "2019-06-25",
        "title": "attention-cnn",
        "summary": "Source code for \"On the Relationship between Self-Attention and Convolutional Layers\"",
        "tags": [
            "python"
        ]
    },
    "https://github.com/lucashadfield/speck": {
        "extra-tags": [],
        "date": "2019-12-27",
        "title": "speck",
        "summary": "line art image renderer",
        "tags": [
            "speck",
            "line-art",
            "python",
            "matplotlib"
        ]
    },
    "https://github.com/jiecaoyu/XNOR-Net-PyTorch": {
        "extra-tags": [
            "pytorch"
        ],
        "date": "2017-09-06",
        "title": "XNOR-Net-PyTorch",
        "summary": "PyTorch Implementation of XNOR-Net",
        "tags": [
            "python"
        ]
    },
    "https://github.com/torchgan/torchgan": {
        "extra-tags": [],
        "date": "2018-09-20",
        "title": "torchgan",
        "summary": "Research Framework for easy and efficient training of GANs based on Pytorch",
        "tags": [
            "python3",
            "generative-model",
            "python",
            "neural-networks",
            "machine-learning",
            "gans",
            "pytorch",
            "deep-learning",
            "computer-vision",
            "generative-adversarial-networks"
        ]
    },
    "https://github.com/pytorch/xla": {
        "extra-tags": [],
        "date": "2018-11-05",
        "title": "xla",
        "summary": "Enabling PyTorch on Google TPU",
        "tags": [
            "compiler",
            "xla",
            "pytorch",
            "deep-learning",
            "c++"
        ]
    },
    "https://github.com/jsbroks/coco-annotator": {
        "extra-tags": [],
        "date": "2018-09-03",
        "title": "coco-annotator",
        "summary": ":pencil2: Web-based image segmentation tool for object detection, localization, and keypoints",
        "tags": [
            "computer-vision",
            "datasets",
            "image-segmentation",
            "image-labeling",
            "machine-learning",
            "coco-annotator",
            "coco",
            "label",
            "vue",
            "detection",
            "image-annotation",
            "deep-learning",
            "coco-format",
            "annotate-images"
        ]
    },
    "https://github.com/PistonY/torch-toolbox": {
        "extra-tags": [],
        "date": "2019-05-06",
        "title": "torch-toolbox",
        "summary": "\ud83d\udee0 Toolbox to extend PyTorch functionalities",
        "tags": [
            "metrics",
            "arcloss",
            "cosinewarmuplr",
            "lookahead",
            "lmdb",
            "swish",
            "teansform",
            "autoaugment",
            "cv2",
            "switchnorm",
            "initializers",
            "mixup",
            "labelsmoothing",
            "python",
            "cosloss",
            "toolbox",
            "pytorch",
            "l2softmax",
            "flops",
            "lmdb-dataset",
            "cutout"
        ]
    },
    "https://github.com/khornlund/pytorch-balanced-sampler": {
        "extra-tags": [
            "training"
        ],
        "date": "2019-09-21",
        "title": "pytorch-balanced-sampler",
        "summary": "PyTorch implementations of `BatchSampler` that under/over sample according to a chosen parameter alpha, in order to create a balanced training distribution.",
        "tags": [
            "python",
            "pytorch",
            "deep-learning"
        ]
    },
    "https://github.com/jekyll/minima": {
        "extra-tags": [
            "theme"
        ],
        "date": "2016-05-20",
        "title": "minima",
        "summary": "Minima is a one-size-fits-all Jekyll theme for writers.",
        "tags": [
            "jekyll-theme",
            "jekyll",
            "scss",
            "jekyll-themes"
        ]
    },
    "https://github.com/ajayyy/SponsorBlock": {
        "extra-tags": [],
        "date": "2019-07-10",
        "title": "SponsorBlock",
        "summary": "Skip YouTube video sponsors (browser extension)",
        "tags": [
            "adblock",
            "chrome",
            "adblocker",
            "chromium",
            "chrome-extension",
            "youtube-videos",
            "firefox",
            "hacktoberfest",
            "typescript",
            "firefox-extension",
            "sponsorblock",
            "opera",
            "youtube",
            "web-extension",
            "sponsored-segments"
        ]
    },
    "https://github.com/facebookresearch/mixup-cifar10": {
        "extra-tags": [
            "mixup",
            "risk"
        ],
        "date": "2018-02-19",
        "title": "mixup-cifar10",
        "summary": "mixup: Beyond Empirical Risk Minimization",
        "tags": [
            "python"
        ]
    },
    "https://github.com/qubvel/ttach": {
        "extra-tags": [],
        "date": "2019-10-01",
        "title": "ttach",
        "summary": "Image Test Time Augmentation with PyTorch!",
        "tags": [
            "keypoint-detection",
            "test-time-augmentation",
            "python",
            "tta-wrapper",
            "augmentation",
            "classification",
            "segmentation",
            "tta",
            "pytorch",
            "deep-learning",
            "computer-vision"
        ]
    },
    "https://github.com/rotinov/AGNES": {
        "extra-tags": [],
        "date": "2019-09-20",
        "title": "AGNES",
        "summary": "Flexible Reinforcement Learning Framework with PyTorch",
        "tags": [
            "python"
        ]
    },
    "https://github.com/opherlieber/rltime": {
        "extra-tags": [],
        "date": "2019-09-04",
        "title": "rltime",
        "summary": "RLtime is a reinforcement learning library focused on state-of-the-art q-learning algorithms and features",
        "tags": [
            "python"
        ]
    },
    "https://github.com/chrieke/awesome-satellite-imagery-datasets": {
        "extra-tags": [],
        "date": "2018-05-01",
        "title": "awesome-satellite-imagery-datasets",
        "summary": "\ud83d\udef0\ufe0f List of satellite image training datasets with annotations for computer vision and deep learning",
        "tags": [
            "remote-sensing",
            "machine-learning",
            "satellite-imagery",
            "instance-segmentation",
            "earth-observation",
            "deep-learning",
            "computer-vision",
            "object-detection"
        ]
    },
    "https://github.com/KaTeX/KaTeX": {
        "extra-tags": [],
        "date": "2013-07-05",
        "title": "KaTeX",
        "summary": "Fast math typesetting for the web.",
        "tags": [
            "latex",
            "math",
            "javascript",
            "katex",
            "math-typesetting"
        ]
    },
    "https://github.com/vpj/annotate": {
        "extra-tags": [],
        "date": "2019-08-25",
        "title": "annotate",
        "summary": "Annotate python source code",
        "tags": [
            "typescript"
        ]
    },
    "https://github.com/Santosh-Gupta/SpeedTorch": {
        "extra-tags": [],
        "date": "2019-09-07",
        "title": "SpeedTorch",
        "summary": "Library for faster pinned CPU <-> GPU transfer in Pytorch ",
        "tags": [
            "cupy",
            "machine-learning",
            "cuda-tensors",
            "pinned-cpu-tensors",
            "gpu-transfer",
            "cpu-gpu-transfer",
            "pytorch-tensors",
            "nlp",
            "cpu-pinned-tensors",
            "embeddings-trained",
            "python",
            "gpu",
            "cuda",
            "embeddings",
            "sparse-modeling",
            "cuda-variables",
            "pytorch-variables",
            "natural-language-processing",
            "sparse",
            "data-transfer",
            "pytorch"
        ]
    },
    "https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch": {
        "extra-tags": [],
        "date": "2018-09-07",
        "title": "Deep-Reinforcement-Learning-Algorithms-with-PyTorch",
        "summary": "PyTorch implementations of deep reinforcement learning algorithms and environments",
        "tags": [
            "python"
        ]
    },
    "https://github.com/huggingface/knockknock": {
        "extra-tags": [
            "training",
            "code"
        ],
        "date": "2019-03-20",
        "title": "knockknock",
        "summary": "\ud83d\udeaa\u270aKnock Knock: Get notified when your training ends with only two additional lines of code",
        "tags": [
            "natural-language-processing",
            "python",
            "neural-networks",
            "nlproc",
            "machine-learning",
            "cv",
            "nlp",
            "train",
            "deep-learning",
            "computer-vision",
            "python36"
        ]
    },
    "https://github.com/bestfitting/kaggle": {
        "extra-tags": [
            "kaggle"
        ],
        "date": "2017-07-23",
        "title": "kaggle",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/learnables/cherry": {
        "extra-tags": [],
        "date": "2018-11-30",
        "title": "cherry",
        "summary": "A PyTorch Library for Reinforcement Learning Research",
        "tags": [
            "python",
            "reinforcement-learning",
            "learning",
            "reinforcement",
            "pytorch",
            "rl"
        ]
    },
    "https://github.com/Kaixhin/Rainbow": {
        "extra-tags": [],
        "date": "2017-10-09",
        "title": "Rainbow",
        "summary": "Rainbow: Combining Improvements in Deep Reinforcement Learning",
        "tags": [
            "deep-reinforcement-learning",
            "python",
            "deep-learning"
        ]
    },
    "https://github.com/seungeunrho/minimalRL": {
        "extra-tags": [],
        "date": "2019-04-23",
        "title": "minimalRL",
        "summary": "Implementations of basic RL algorithms with minimal lines of codes! (pytorch based)",
        "tags": [
            "a3c",
            "python",
            "reinforcement-learning",
            "a2c",
            "sac",
            "ddpg",
            "machine-learning",
            "deep-reinforcement-learning",
            "dqn",
            "policy-gradients",
            "simple",
            "pytorch",
            "deep-learning",
            "acer",
            "ppo",
            "reinforce"
        ]
    },
    "https://github.com/astooke/rlpyt": {
        "extra-tags": [],
        "date": "2019-04-28",
        "title": "rlpyt",
        "summary": "Reinforcement Learning in PyTorch",
        "tags": [
            "python"
        ]
    },
    "https://github.com/vandit15/Class-balanced-loss-pytorch": {
        "extra-tags": [],
        "date": "2019-08-31",
        "title": "Class-balanced-loss-pytorch",
        "summary": "Pytorch implementation of the paper \"Class-Balanced Loss Based on Effective Number of Samples\"",
        "tags": [
            "cvpr2019",
            "python",
            "loss-functions",
            "pytorch",
            "deep-learning",
            "computer-vision"
        ]
    },
    "https://github.com/labmlai/labml": {
        "extra-tags": [],
        "date": "2018-11-16",
        "title": "labml",
        "summary": "\ud83d\udd0e Monitor deep learning model training and hardware usage from your mobile phone \ud83d\udcf1",
        "tags": [
            "keras-tensorflow",
            "fastai",
            "analytics",
            "keras",
            "machine-learning",
            "mobile",
            "tensorflow",
            "pytorch-lightning",
            "jupyter notebook",
            "pytorch",
            "deep-learning",
            "experiment",
            "tensorboard",
            "visualization",
            "tensorflow2"
        ]
    },
    "https://github.com/lutris/lutris": {
        "extra-tags": [
            "client"
        ],
        "date": "2013-10-08",
        "title": "lutris",
        "summary": "Lutris desktop client in Python / PyGObject",
        "tags": [
            "game-launcher",
            "gaming",
            "python"
        ]
    },
    "https://github.com/dkozlov/awesome-knowledge-distillation": {
        "extra-tags": [],
        "date": "2017-02-28",
        "title": "awesome-knowledge-distillation",
        "summary": "Awesome Knowledge Distillation",
        "tags": [
            "distillation",
            "knowledge-distillation",
            "knowledge-transfer",
            "model-distillation",
            "knowldge-distillation",
            "kd",
            "deep-learning",
            "teacher-student",
            "distillation-model",
            "model-compression",
            "co-training"
        ]
    },
    "https://github.com/victoresque/pytorch-template": {
        "extra-tags": [],
        "date": "2018-03-13",
        "title": "pytorch-template",
        "summary": "PyTorch deep learning projects made easy.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/sdv-dev/TGAN": {
        "extra-tags": [],
        "date": "2018-05-24",
        "title": "TGAN",
        "summary": "Generative adversarial training for generating synthetic  tabular data.",
        "tags": [
            "python",
            "generative-adversarial-network",
            "synthetic-data",
            "tabular-data",
            "synthesizing-tabular-data"
        ]
    },
    "https://github.com/LiyuanLucasLiu/RAdam": {
        "extra-tags": [
            "variance",
            "learning"
        ],
        "date": "2019-08-01",
        "title": "RAdam",
        "summary": "On the Variance of the Adaptive Learning Rate and Beyond",
        "tags": [
            "python",
            "optimizer",
            "adam-optimizer",
            "adam",
            "warmup"
        ]
    },
    "https://github.com/eriklindernoren/PyTorch-GAN": {
        "extra-tags": [],
        "date": "2018-04-21",
        "title": "PyTorch-GAN",
        "summary": "PyTorch implementations of Generative Adversarial Networks.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/wanglouis49/pytorch-weights_pruning": {
        "extra-tags": [
            "pruning"
        ],
        "date": "2017-11-15",
        "title": "pytorch-weights_pruning",
        "summary": "PyTorch Implementation of Weights Pruning",
        "tags": [
            "weights-pruning",
            "model-compression",
            "python",
            "pytorch"
        ]
    },
    "https://github.com/tugstugi/pytorch-saltnet": {
        "extra-tags": [],
        "date": "2018-10-20",
        "title": "pytorch-saltnet",
        "summary": "Kaggle | 9th place single model solution for TGS Salt Identification Challenge",
        "tags": [
            "unet",
            "python",
            "kaggle-competition",
            "convolutional-neural-networks",
            "unet-pytorch",
            "seismic-imaging",
            "segmentation",
            "cnn",
            "unet-image-segmentation",
            "pytorch",
            "deep-learning"
        ]
    },
    "https://github.com/williamFalcon/DeepRLHacks": {
        "extra-tags": [],
        "date": "2017-08-28",
        "title": "DeepRLHacks",
        "summary": "Hacks for training RL systems from John Schulman's lecture at Deep RL Bootcamp  (Aug 2017)",
        "tags": []
    },
    "https://github.com/pytorch/botorch": {
        "extra-tags": [],
        "date": "2018-07-30",
        "title": "botorch",
        "summary": "Bayesian optimization in PyTorch",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/ptrblck/pytorch_misc": {
        "extra-tags": [],
        "date": "2018-11-15",
        "title": "pytorch_misc",
        "summary": "Code snippets created for the PyTorch discussion board",
        "tags": [
            "python"
        ]
    },
    "https://github.com/Bjarten/early-stopping-pytorch": {
        "extra-tags": [],
        "date": "2018-12-29",
        "title": "early-stopping-pytorch",
        "summary": "Early stopping for PyTorch ",
        "tags": [
            "python",
            "pytorch-tutorial",
            "mnist",
            "stopping",
            "tutorial",
            "jupyter notebook",
            "early-stopping",
            "pytorch",
            "regularization",
            "early"
        ]
    },
    "https://github.com/entron/entity-embedding-rossmann": {
        "extra-tags": [],
        "date": "2015-12-17",
        "title": "entity-embedding-rossmann",
        "summary": "",
        "tags": [
            "entity-embedding",
            "kaggle",
            "categorical-data",
            "one-hot-encoding",
            "jupyter notebook",
            "categorical-features"
        ]
    },
    "https://github.com/ybabakhin/kaggle_salt_bes_phalanx": {
        "extra-tags": [
            "kaggle",
            "identification"
        ],
        "date": "2018-08-02",
        "title": "kaggle_salt_bes_phalanx",
        "summary": "Winning solution for the Kaggle TGS Salt Identification Challenge.",
        "tags": [
            "pseudo-labeling",
            "python",
            "image-segmentation",
            "u-net",
            "keras",
            "pytorch",
            "deep-learning",
            "semantic-segmentation"
        ]
    },
    "https://github.com/zuoxingdong/lagom": {
        "extra-tags": [],
        "date": "2017-12-21",
        "title": "lagom",
        "summary": "lagom: A PyTorch infrastructure for rapid prototyping of reinforcement learning algorithms.",
        "tags": [
            "research",
            "ddpg",
            "machine-learning",
            "td3",
            "jupyter notebook",
            "cem",
            "cmaes",
            "deep-reinforcement-learning",
            "artificial-intelligence",
            "python",
            "reinforcement-learning",
            "sac",
            "soft-actor-critic",
            "policy-gradient",
            "deep-deterministic-policy-gradient",
            "deep-learning",
            "evolution-strategies",
            "mujoco",
            "pytorch",
            "proximal-policy-optimization",
            "ppo"
        ]
    },
    "https://github.com/tkrabel/bamboolib": {
        "extra-tags": [],
        "date": "2019-05-29",
        "title": "bamboolib",
        "summary": "bamboolib - a GUI for pandas DataFrames",
        "tags": [
            "jupyter-notebook",
            "python",
            "jupyterlab",
            "jupyter notebook",
            "pandas",
            "pandas-dataframes"
        ]
    },
    "https://github.com/erachelson/RLclass": {
        "extra-tags": [
            "dataclass"
        ],
        "date": "2018-01-31",
        "title": "RLclass",
        "summary": "",
        "tags": [
            "jupyter notebook",
            "reinforcement-learning",
            "course-materials"
        ]
    },
    "https://github.com/aerdem4/lofo-importance": {
        "extra-tags": [],
        "date": "2019-01-14",
        "title": "lofo-importance",
        "summary": "Leave One Feature Out Importance",
        "tags": [
            "feature-selection",
            "python",
            "feature-importance",
            "machine-learning",
            "explainable-ai",
            "data-science"
        ]
    },
    "https://github.com/facebookresearch/kill-the-bits": {
        "extra-tags": [
            "code",
            "quantization"
        ],
        "date": "2019-05-21",
        "title": "kill-the-bits",
        "summary": "Code for: \"And the bit goes down: Revisiting the quantization of neural networks\"",
        "tags": [
            "python"
        ]
    },
    "https://github.com/facebookresearch/FixRes": {
        "extra-tags": [],
        "date": "2019-06-28",
        "title": "FixRes",
        "summary": "This repository reproduces the results of the paper: \"Fixing the train-test resolution discrepancy\" https://arxiv.org/abs/1906.06423",
        "tags": [
            "python"
        ]
    },
    "https://github.com/python/mypy": {
        "extra-tags": [],
        "date": "2012-12-07",
        "title": "mypy",
        "summary": "Optional static typing for Python",
        "tags": [
            "types",
            "python",
            "typechecker",
            "linter",
            "typing"
        ]
    },
    "https://github.com/dsgiitr/d2l-pytorch": {
        "extra-tags": [],
        "date": "2019-05-11",
        "title": "d2l-pytorch",
        "summary": "This project reproduces the book Dive Into Deep Learning (https://d2l.ai/), adapting the code from MXNet into PyTorch.",
        "tags": [
            "dive-into-deep-learning",
            "mxnet",
            "nlp",
            "d2l",
            "data-science",
            "pytorch",
            "deep-learning",
            "jupyter notebook",
            "computer-vision",
            "book",
            "pytorch-implmention"
        ]
    },
    "https://github.com/utkuozbulak/pytorch-cnn-visualizations": {
        "extra-tags": [],
        "date": "2017-10-21",
        "title": "pytorch-cnn-visualizations",
        "summary": "Pytorch implementation of convolutional neural network visualization techniques",
        "tags": [
            "cnn-visualization",
            "gradient-visualization",
            "python",
            "deep-dream",
            "guided-backpropagation",
            "cam",
            "smooth-grad",
            "segmentation",
            "guided-grad-cam",
            "saliency",
            "pytorch",
            "grad-cam",
            "gradient"
        ]
    },
    "https://github.com/unixpickle/obs-tower2": {
        "extra-tags": [
            "unity"
        ],
        "date": "2019-03-07",
        "title": "obs-tower2",
        "summary": "My solution to the Unity Obstacle Tower Challenge",
        "tags": [
            "python"
        ]
    },
    "https://github.com/facebookresearch/ReAgent": {
        "extra-tags": [],
        "date": "2017-07-27",
        "title": "ReAgent",
        "summary": "A platform for Reasoning systems (Reinforcement Learning, Contextual Bandits, etc.)",
        "tags": [
            "python"
        ]
    },
    "https://github.com/deepmind/dm_env": {
        "extra-tags": [],
        "date": "2019-07-08",
        "title": "dm_env",
        "summary": "A Python interface for reinforcement learning environments",
        "tags": [
            "python",
            "reinforcement-learning",
            "machine-learning",
            "deep-learning",
            "api",
            "interface"
        ]
    },
    "https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers": {
        "extra-tags": [],
        "date": "2013-01-14",
        "title": "Probabilistic-Programming-and-Bayesian-Methods-for-Hackers",
        "summary": "aka \"Bayesian Methods for Hackers\": An introduction to Bayesian methods + probabilistic programming with a computation/understanding-first, mathematics-second point of view. All in pure Python ;)  ",
        "tags": [
            "mathematical-analysis",
            "jupyter-notebook",
            "statistics",
            "jupyter notebook",
            "data-science",
            "pymc",
            "bayesian-methods"
        ]
    },
    "https://github.com/EthicalML/awesome-production-machine-learning": {
        "extra-tags": [],
        "date": "2018-08-15",
        "title": "awesome-production-machine-learning",
        "summary": "A curated list of awesome open source libraries to deploy, monitor, version and scale your machine learning",
        "tags": [
            "data-mining",
            "ml-ops",
            "responsible-ai",
            "machine-learning",
            "large-scale-machine-learning",
            "large-scale-ml",
            "interpretability",
            "explainability",
            "privacy-preserving-ml",
            "production-ml",
            "mlops",
            "deep-learning",
            "awesome",
            "awesome-list",
            "production-machine-learning",
            "machine-learning-operations",
            "ml-operations",
            "privacy-preserving-machine-learning",
            "privacy-preserving"
        ]
    },
    "https://github.com/Lyken17/pytorch-memonger": {
        "extra-tags": [],
        "date": "2019-07-19",
        "title": "pytorch-memonger",
        "summary": "Sublinear memory optimization for deep learning. https://arxiv.org/abs/1604.06174",
        "tags": [
            "python"
        ]
    },
    "https://github.com/coleifer/peewee": {
        "extra-tags": [],
        "date": "2010-10-11",
        "title": "peewee",
        "summary": "a small, expressive orm -- supports postgresql, mysql and sqlite",
        "tags": [
            "python",
            "gametight",
            "dank",
            "sqlite",
            "peewee"
        ]
    },
    "https://github.com/Lyken17/pytorch-OpCounter": {
        "extra-tags": [
            "flops",
            "pytorch"
        ],
        "date": "2018-01-26",
        "title": "pytorch-OpCounter",
        "summary": "Count the MACs / FLOPs of your PyTorch model.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/arnoweng/CheXNet": {
        "extra-tags": [],
        "date": "2017-12-26",
        "title": "CheXNet",
        "summary": "A pytorch reimplementation of CheXNet",
        "tags": [
            "thoracic-diseases",
            "python",
            "pneumonia",
            "x-ray",
            "classification",
            "pytorch",
            "deep-learning",
            "localization",
            "medical-images"
        ]
    },
    "https://github.com/joke2k/faker": {
        "extra-tags": [],
        "date": "2012-11-12",
        "title": "faker",
        "summary": "Faker is a Python package that generates fake data for you.",
        "tags": [
            "testing",
            "fake-data",
            "python",
            "test-data",
            "dataset",
            "fake",
            "test-data-generator"
        ]
    },
    "https://github.com/szagoruyko/binary-wide-resnet": {
        "extra-tags": [
            "resnet"
        ],
        "date": "2018-08-26",
        "title": "binary-wide-resnet",
        "summary": "PyTorch implementation of Wide Residual Networks with 1-bit weights by McDonnell (ICLR 2018)",
        "tags": [
            "wide-residual-networks",
            "python",
            "pytorch"
        ]
    },
    "https://github.com/DocF/Soft-NMS": {
        "extra-tags": [],
        "date": "2019-01-10",
        "title": "Soft-NMS",
        "summary": "Python and Pytorch two implements of Soft NMS algorithm ",
        "tags": [
            "python"
        ]
    },
    "https://github.com/facebookresearch/WSL-Images": {
        "extra-tags": [
            "learning",
            "images"
        ],
        "date": "2019-06-11",
        "title": "WSL-Images",
        "summary": "Weakly Supervised Learning On Images",
        "tags": [
            "python"
        ]
    },
    "https://github.com/BorealisAI/advertorch": {
        "extra-tags": [],
        "date": "2018-11-29",
        "title": "advertorch",
        "summary": "A Toolbox for Adversarial Robustness Research",
        "tags": [
            "toolbox",
            "adversarial-learning",
            "adversarial-perturbations",
            "adversarial-attacks",
            "adversarial-example",
            "adversarial-machine-learning",
            "machine-learning",
            "jupyter notebook",
            "robustness",
            "benchmarking",
            "adversarial-examples",
            "pytorch",
            "security"
        ]
    },
    "https://github.com/trekhleb/homemade-machine-learning": {
        "extra-tags": [],
        "date": "2018-11-01",
        "title": "homemade-machine-learning",
        "summary": "\ud83e\udd16 Python examples of popular machine learning algorithms with interactive Jupyter demos and math being explained",
        "tags": [
            "jupyter-notebook",
            "python",
            "algorithm",
            "machine-learning",
            "jupyter",
            "jupyter notebook",
            "machinelearning",
            "machine-learning-algorithms"
        ]
    },
    "https://github.com/CompVis/metric-learning-divide-and-conquer": {
        "extra-tags": [],
        "date": "2019-03-03",
        "title": "metric-learning-divide-and-conquer",
        "summary": "Source code for the paper \"Divide and Conquer the Embedding Space for Metric Learning\", CVPR 2019",
        "tags": [
            "few-shot-learning",
            "metric-learning",
            "python",
            "pytorch"
        ]
    },
    "https://github.com/posquit0/Awesome-CV": {
        "extra-tags": [],
        "date": "2015-01-18",
        "title": "Awesome-CV",
        "summary": ":page_facing_up: Awesome CV is LaTeX template for your outstanding job application",
        "tags": [
            "latex",
            "sharelatex",
            "pdf",
            "tex",
            "latex-template",
            "cv",
            "coverletter",
            "awesome",
            "overleaf",
            "resume"
        ]
    },
    "https://github.com/kuangliu/torchcv": {
        "extra-tags": [],
        "date": "2017-11-30",
        "title": "torchcv",
        "summary": "TorchCV: a PyTorch vision library mimics ChainerCV",
        "tags": [
            "python"
        ]
    },
    "https://github.com/pytorch/hub": {
        "extra-tags": [
            "pytorch"
        ],
        "date": "2019-04-22",
        "title": "hub",
        "summary": "Submission to https://pytorch.org/hub/",
        "tags": [
            "python"
        ]
    },
    "https://github.com/MorvanZhou/PyTorch-Tutorial": {
        "extra-tags": [],
        "date": "2017-05-05",
        "title": "PyTorch-Tutorial",
        "summary": "Build your neural network easy and fast, \u83ab\u70e6Python\u4e2d\u6587\u6559\u5b66",
        "tags": [
            "machine-learning",
            "dqn",
            "tutorial",
            "regression",
            "jupyter notebook",
            "rnn",
            "batch-normalization",
            "generative-adversarial-network",
            "dropout",
            "classification",
            "pytorch-tutorials",
            "batch",
            "neural-network",
            "python",
            "reinforcement-learning",
            "autoencoder",
            "pytorch-tutorial",
            "cnn",
            "pytorch",
            "gan"
        ]
    },
    "https://github.com/dirty-cat/dirty_cat": {
        "extra-tags": [],
        "date": "2018-03-12",
        "title": "dirty_cat",
        "summary": "Machine learning on dirty tabular data",
        "tags": [
            "python",
            "data-preprocessing",
            "data-cleaning",
            "machine-learning",
            "data-analysis",
            "data",
            "data-preparation",
            "data-science",
            "dirty-data"
        ]
    },
    "https://github.com/vinbhaskara/adams": {
        "extra-tags": [],
        "date": "2019-05-25",
        "title": "adams",
        "summary": "Exploiting Uncertainty of Loss Landscape for Stochastic Optimization",
        "tags": [
            "python"
        ]
    },
    "https://github.com/YU1ut/MixMatch-pytorch": {
        "extra-tags": [],
        "date": "2019-05-22",
        "title": "MixMatch-pytorch",
        "summary": "Code for \"MixMatch - A Holistic Approach to Semi-Supervised Learning\"",
        "tags": [
            "semi-supervised-learning",
            "python",
            "pytorch",
            "deep-learning"
        ]
    },
    "https://github.com/OValery16/Manga-colorization---cycle-gan": {
        "extra-tags": [
            "tutorial",
            "gan"
        ],
        "date": "2018-09-28",
        "title": "Manga-colorization---cycle-gan",
        "summary": "Tutorial about the use of cycle-gan to colorize a manga",
        "tags": [
            "python"
        ]
    },
    "https://github.com/implus/PytorchInsight": {
        "extra-tags": [],
        "date": "2019-05-17",
        "title": "PytorchInsight",
        "summary": "a pytorch lib with state-of-the-art architectures, pretrained models and real-time updated results",
        "tags": [
            "weight-decay",
            "detection",
            "sge",
            "attention-models",
            "classification",
            "sknet",
            "convolutional-networks",
            "tricks",
            "training-shufflenetv2",
            "python",
            "cnn-tricks",
            "pretrained-models",
            "state-of-the-art",
            "bam",
            "gcnet",
            "cbam",
            "cnn",
            "pytorch",
            "shufflenetv2",
            "senet",
            "weight-normalization-family"
        ]
    },
    "https://github.com/pytorch/vision": {
        "extra-tags": [],
        "date": "2016-11-09",
        "title": "vision",
        "summary": "Datasets, Transforms and Models specific to Computer Vision",
        "tags": [
            "computer-vision",
            "python",
            "machine-learning"
        ]
    },
    "https://github.com/pybind/pybind11": {
        "extra-tags": [],
        "date": "2015-07-05",
        "title": "pybind11",
        "summary": "Seamless operability between C++11 and Python",
        "tags": [
            "bindings",
            "c++",
            "python"
        ]
    },
    "https://github.com/google-research/mixmatch": {
        "extra-tags": [],
        "date": "2019-05-15",
        "title": "mixmatch",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/koshian2/OctConv-TFKeras": {
        "extra-tags": [],
        "date": "2019-04-18",
        "title": "OctConv-TFKeras",
        "summary": "Unofficial implementation of Octave Convolutions (OctConv) in TensorFlow / Keras.",
        "tags": [
            "octconv",
            "tensorflow-keras",
            "keras",
            "jupyter notebook",
            "tpu"
        ]
    },
    "https://github.com/Killkitten/Thinkerview-Recommandations-lecture": {
        "extra-tags": [],
        "date": "2019-05-10",
        "title": "Thinkerview-Recommandations-lecture",
        "summary": "Liste des recommandations lecture des invit\u00e9s",
        "tags": []
    },
    "https://github.com/bfortuner/ml-glossary": {
        "extra-tags": [],
        "date": "2017-04-20",
        "title": "ml-glossary",
        "summary": "Machine learning glossary",
        "tags": [
            "cheatsheets",
            "neural-network",
            "python",
            "machine-learning",
            "deep-learning-tutorial",
            "data-science",
            "deep-learning"
        ]
    },
    "https://github.com/TomAugspurger/effective-pandas": {
        "extra-tags": [],
        "date": "2016-05-15",
        "title": "effective-pandas",
        "summary": "Source code for my collection of articles on using pandas.",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/lckr/jupyterlab-variableInspector": {
        "extra-tags": [],
        "date": "2018-06-13",
        "title": "jupyterlab-variableInspector",
        "summary": "Variable Inspector extension for Jupyterlab",
        "tags": [
            "jupyterlab-variableinspector",
            "variable-inspector",
            "jupyterlab",
            "typescript",
            "jupyterlab-extension"
        ]
    },
    "https://github.com/fabiospampinato/cliflix": {
        "extra-tags": [],
        "date": "2017-09-03",
        "title": "cliflix",
        "summary": "Watch anything instantaneously, just write its name.",
        "tags": [
            "torrent",
            "cli",
            "watch",
            "typescript",
            "stream",
            "cliflix"
        ]
    },
    "https://github.com/rkern/line_profiler": {
        "extra-tags": [
            "profiling"
        ],
        "date": "2014-08-31",
        "title": "line_profiler",
        "summary": "(OLD REPO) Line-by-line profiling for Python - Current repo ->",
        "tags": [
            "python"
        ]
    },
    "https://github.com/facebookresearch/OctConv": {
        "extra-tags": [],
        "date": "2019-04-17",
        "title": "OctConv",
        "summary": "Code for paper",
        "tags": [
            "python"
        ]
    },
    "https://github.com/BloodAxe/pytorch-toolbelt": {
        "extra-tags": [],
        "date": "2019-03-15",
        "title": "pytorch-toolbelt",
        "summary": "PyTorch extensions for fast R&D prototyping and Kaggle farming",
        "tags": [
            "image-classification",
            "python",
            "test-time-augmentation",
            "image-segmentation",
            "focal-loss",
            "machine-learning",
            "kaggle",
            "image-processing",
            "augmentation",
            "segmentation",
            "tta",
            "pytorch",
            "deep-learning",
            "pipeline",
            "jaccard-loss",
            "object-detection"
        ]
    },
    "https://github.com/guipsamora/pandas_exercises": {
        "extra-tags": [],
        "date": "2016-07-12",
        "title": "pandas_exercises",
        "summary": "Practice your pandas skills!",
        "tags": [
            "exercise",
            "data-analysis",
            "tutorial",
            "jupyter notebook",
            "pandas",
            "practice"
        ]
    },
    "https://github.com/cool-RR/PySnooper": {
        "extra-tags": [
            "debugging"
        ],
        "date": "2019-04-18",
        "title": "PySnooper",
        "summary": "Never use print for debugging again",
        "tags": [
            "python",
            "logging",
            "debugger",
            "debug",
            "introspection"
        ]
    },
    "https://github.com/qubvel/segmentation_models.pytorch": {
        "extra-tags": [],
        "date": "2019-03-01",
        "title": "segmentation_models.pytorch",
        "summary": "Segmentation models with pretrained backbones. PyTorch.",
        "tags": [
            "segmentation-models",
            "image-segmentation",
            "unet",
            "unet-pytorch",
            "pretrained-weights",
            "pspnet",
            "python",
            "pretrained-backbones",
            "image-processing",
            "segmentation",
            "deeplabv3",
            "pretrained-models",
            "models",
            "semantic-segmentation",
            "imagenet",
            "deeplab-v3-plus",
            "fpn",
            "linknet",
            "hacktoberfest",
            "pytorch",
            "unetplusplus"
        ]
    },
    "https://github.com/iacolippo/octconv-pytorch": {
        "extra-tags": [],
        "date": "2019-04-16",
        "title": "octconv-pytorch",
        "summary": "Implementation of OctConv in Pytorch (https://arxiv.org/abs/1904.05049)",
        "tags": [
            "python"
        ]
    },
    "https://github.com/NVlabs/SPADE": {
        "extra-tags": [],
        "date": "2019-03-14",
        "title": "SPADE",
        "summary": "Semantic Image Synthesis with SPADE",
        "tags": [
            "python"
        ]
    },
    "https://github.com/gan3sh500/octaveconv-pytorch": {
        "extra-tags": [],
        "date": "2019-04-16",
        "title": "octaveconv-pytorch",
        "summary": "Implementation of Octave Convolution from Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution in Pytorch (https://arxiv.org/abs/1904.05049)",
        "tags": [
            "python",
            "pytorch",
            "deep-learning"
        ]
    },
    "https://github.com/pytorch/examples": {
        "extra-tags": [],
        "date": "2016-08-24",
        "title": "examples",
        "summary": "A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/google/jax": {
        "extra-tags": [],
        "date": "2018-10-25",
        "title": "jax",
        "summary": "Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more",
        "tags": [
            "jax",
            "python"
        ]
    },
    "https://github.com/santosjorge/cufflinks": {
        "extra-tags": [],
        "date": "2014-11-19",
        "title": "cufflinks",
        "summary": "Productivity Tools for Plotly + Pandas",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/aquadzn/PaintingsClassification": {
        "extra-tags": [
            "image",
            "fastai"
        ],
        "date": "2019-04-10",
        "title": "PaintingsClassification",
        "summary": "Image classifier with FastAI",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/python-pillow/Pillow": {
        "extra-tags": [],
        "date": "2012-07-24",
        "title": "Pillow",
        "summary": "Python Imaging Library (Fork)",
        "tags": [
            "image",
            "cross-platform",
            "python",
            "python-3",
            "image-processing",
            "pillow",
            "pil",
            "c"
        ]
    },
    "https://github.com/SeuTao/Humpback-Whale-Identification": {
        "extra-tags": [
            "kaggle",
            "identification"
        ],
        "date": "2019-03-12",
        "title": "Humpback-Whale-Identification",
        "summary": "Kaggle Humpback Whale Identification Challenge 2019",
        "tags": [
            "python"
        ]
    },
    "https://github.com/SeuTao/TGS-Salt-Identification": {
        "extra-tags": [
            "identification"
        ],
        "date": "2018-10-21",
        "title": "TGS-Salt-Identification",
        "summary": "Kaggle TGS Salt Identification Challenge",
        "tags": [
            "pytorch",
            "pseudo-labeling",
            "python",
            "kaggle"
        ]
    },
    "https://github.com/higgsfield/RL-Adventure": {
        "extra-tags": [],
        "date": "2018-03-24",
        "title": "RL-Adventure",
        "summary": "Pytorch Implementation of DQN / DDQN / Prioritized replay/ noisy networks/ distributional values/ Rainbow/ hierarchical RL",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/higgsfield/RL-Adventure-2": {
        "extra-tags": [],
        "date": "2018-05-26",
        "title": "RL-Adventure-2",
        "summary": "PyTorch0.4 implementation of: actor critic / proximal policy optimization / acer / ddpg / twin dueling ddpg / soft actor critic / generative adversarial imitation learning / hindsight experience replay",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/gbolmier/newspaper-crawler": {
        "extra-tags": [],
        "date": "2019-04-08",
        "title": "newspaper-crawler",
        "summary": ":spider: An autonomous French newspaper crawler based on Scrapy framework",
        "tags": [
            "python",
            "scrapy",
            "crawler"
        ]
    },
    "https://github.com/MG2033/A2C": {
        "extra-tags": [
            "tensorflow"
        ],
        "date": "2018-01-05",
        "title": "A2C",
        "summary": "A Clearer and Simpler Synchronous Advantage Actor Critic (A2C) Implementation in TensorFlow",
        "tags": [
            "gym",
            "actor-critic",
            "reinforcement-learning",
            "python",
            "a2c",
            "openai-gym-agents",
            "policy-gradient",
            "openai-gym-environments",
            "computer-vision"
        ]
    },
    "https://github.com/melling/MathAndScienceNotes": {
        "extra-tags": [],
        "date": "2016-03-11",
        "title": "MathAndScienceNotes",
        "summary": "Notes/links on math and science, including statistics, bayes, cmpsc, quant trading, machine learning, etc",
        "tags": []
    },
    "https://github.com/deepmind/trfl": {
        "extra-tags": [],
        "date": "2018-08-08",
        "title": "trfl",
        "summary": "TensorFlow Reinforcement Learning",
        "tags": [
            "python"
        ]
    },
    "https://github.com/hill-a/stable-baselines": {
        "extra-tags": [],
        "date": "2018-07-02",
        "title": "stable-baselines",
        "summary": "A fork of OpenAI Baselines, implementations of reinforcement learning algorithms",
        "tags": [
            "gym",
            "toolbox",
            "python",
            "baselines",
            "reinforcement-learning",
            "openai",
            "machine-learning",
            "data-science",
            "reinforcement-learning-algorithms"
        ]
    },
    "https://github.com/Maximellerbach/Image-Processing-using-AI": {
        "extra-tags": [],
        "date": "2019-03-13",
        "title": "Image-Processing-using-AI",
        "summary": "in this repo, I create models to process image (upscale, debluring...)",
        "tags": [
            "python",
            "fully-convolutional-networks",
            "upsampling-and-denoising",
            "machine-learning",
            "deblurring",
            "upscaling"
        ]
    },
    "https://github.com/dennybritz/deeplearning-papernotes": {
        "extra-tags": [],
        "date": "2015-12-19",
        "title": "deeplearning-papernotes",
        "summary": "Summaries and notes on Deep Learning research papers",
        "tags": []
    },
    "https://github.com/DanielTakeshi/Paper_Notes": {
        "extra-tags": [],
        "date": "2016-12-30",
        "title": "Paper_Notes",
        "summary": "This will contain my notes for research papers that I read.",
        "tags": []
    },
    "https://github.com/ShangtongZhang/DeepRL": {
        "extra-tags": [],
        "date": "2017-04-20",
        "title": "DeepRL",
        "summary": "Modularized Implementation of Deep RL Algorithms in PyTorch",
        "tags": [
            "prioritized-experience-replay",
            "dueling-network-architecture",
            "double-dqn",
            "option-critic-architecture",
            "quantile-regression",
            "option-critic",
            "python",
            "a2c",
            "ddpg",
            "deep-reinforcement-learning",
            "dqn",
            "td3",
            "rainbow",
            "deeprl",
            "pytorch",
            "ppo",
            "categorical-dqn"
        ]
    },
    "https://github.com/openai/baselines": {
        "extra-tags": [],
        "date": "2017-05-24",
        "title": "baselines",
        "summary": "OpenAI Baselines: high-quality implementations of reinforcement learning algorithms",
        "tags": [
            "python"
        ]
    },
    "https://github.com/fastai/fastai": {
        "extra-tags": [],
        "date": "2017-09-09",
        "title": "fastai",
        "summary": "The fastai deep learning library",
        "tags": [
            "fastai",
            "python",
            "gpu",
            "notebooks",
            "machine-learning",
            "jupyter notebook",
            "pytorch",
            "deep-learning",
            "colab"
        ]
    },
    "https://github.com/google/styleguide": {
        "extra-tags": [],
        "date": "2015-05-20",
        "title": "styleguide",
        "summary": "Style guides for Google-originated open-source projects",
        "tags": [
            "cpplint",
            "html",
            "styleguide",
            "style-guide"
        ]
    },
    "https://github.com/MaxHalford/prince": {
        "extra-tags": [],
        "date": "2016-10-22",
        "title": "prince",
        "summary": ":crown: Multivariate exploratory data analysis in Python: PCA, CA, MCA, MFA, FAMD, GPA",
        "tags": [
            "principal-component-analysis",
            "python",
            "mca",
            "ca",
            "mfa",
            "multiple-correspondence-analysis",
            "svd",
            "pca",
            "correspondence-analysis",
            "multiple-factor-analysis",
            "scikit-learn",
            "factor-analysis",
            "pandas",
            "famd"
        ]
    },
    "https://github.com/VividCortex/gohistogram": {
        "extra-tags": [
            "streaming",
            "histogram"
        ],
        "date": "2013-07-02",
        "title": "gohistogram",
        "summary": "Streaming approximate histograms in Go",
        "tags": [
            "go"
        ]
    },
    "https://github.com/Luolc/AdaBound": {
        "extra-tags": [],
        "date": "2019-02-15",
        "title": "AdaBound",
        "summary": "An optimizer that trains as fast as Adam and as good as SGD.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/HarisIqbal88/PlotNeuralNet": {
        "extra-tags": [],
        "date": "2018-07-24",
        "title": "PlotNeuralNet",
        "summary": "Latex code for making neural networks diagrams",
        "tags": [
            "latex",
            "deep-neural-networks",
            "tex"
        ]
    },
    "https://github.com/cxxr/LiveStats": {
        "extra-tags": [],
        "date": "2013-08-07",
        "title": "LiveStats",
        "summary": "Online Statistical Algorithms for Python",
        "tags": [
            "python"
        ]
    },
    "https://github.com/zaidalyafeai/Notebooks": {
        "extra-tags": [],
        "date": "2018-10-08",
        "title": "Notebooks",
        "summary": "Machine learning notebooks in different subjects optimized to run in google collaboratory",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/mtdvio/every-programmer-should-know": {
        "extra-tags": [
            "software"
        ],
        "date": "2017-08-24",
        "title": "every-programmer-should-know",
        "summary": "A collection of (mostly) technical things every software developer should know about",
        "tags": [
            "cc-by",
            "novice",
            "computer-science",
            "educational",
            "collection"
        ]
    },
    "https://github.com/tensorforce/tensorforce": {
        "extra-tags": [],
        "date": "2017-03-19",
        "title": "tensorforce",
        "summary": "Tensorforce: a TensorFlow library for applied reinforcement learning",
        "tags": [
            "python",
            "reinforcement-learning",
            "deep-reinforcement-learning",
            "control",
            "system-control",
            "tensorflow",
            "tensorforce",
            "tensorflow-library"
        ]
    },
    "https://github.com/hfawaz/dl-4-tsc": {
        "extra-tags": [],
        "date": "2018-09-10",
        "title": "dl-4-tsc",
        "summary": "Deep Learning for Time Series Classification",
        "tags": [
            "empirical-research",
            "python",
            "review",
            "research-paper",
            "convolutional-neural-networks",
            "time-series-classification",
            "deep-neural-networks",
            "deep-learning"
        ]
    },
    "https://github.com/ritchieng/the-incredible-pytorch": {
        "extra-tags": [],
        "date": "2017-02-11",
        "title": "the-incredible-pytorch",
        "summary": "The Incredible PyTorch: a curated list of tutorials, papers, projects, communities and more relating to PyTorch. ",
        "tags": [
            "python",
            "deep-learning-library",
            "deep-learning-tutorial",
            "deep-neural-networks",
            "deep-learning",
            "pytorch"
        ]
    },
    "https://github.com/facebook/prophet": {
        "extra-tags": [],
        "date": "2016-11-16",
        "title": "prophet",
        "summary": "Tool for producing high quality forecasts for time series data that has multiple seasonality with linear or non-linear growth.",
        "tags": [
            "r",
            "python",
            "forecasting"
        ]
    },
    "https://github.com/ecthros/uncaptcha2": {
        "extra-tags": [],
        "date": "2018-12-31",
        "title": "uncaptcha2",
        "summary": "defeating the latest version of ReCaptcha with 91% accuracy",
        "tags": [
            "python"
        ]
    },
    "https://github.com/jhwjhw0123/Imbalance-XGBoost": {
        "extra-tags": [],
        "date": "2018-09-27",
        "title": "Imbalance-XGBoost",
        "summary": "XGBoost for label-imbalanced data: XGBoost with weighted and focal loss functions",
        "tags": [
            "python"
        ]
    },
    "https://github.com/waleedka/hiddenlayer": {
        "extra-tags": [],
        "date": "2018-05-18",
        "title": "hiddenlayer",
        "summary": "Neural network graphs and training metrics for PyTorch, Tensorflow, and Keras.",
        "tags": [
            "python",
            "deeplearning",
            "keras",
            "tensorflow",
            "pytorch",
            "tensorboard",
            "visualization"
        ]
    },
    "https://github.com/shervinea/cheatsheet-translation": {
        "extra-tags": [],
        "date": "2018-09-13",
        "title": "cheatsheet-translation",
        "summary": "Translation of VIP cheatsheets for Machine Learning Deep Learning, and Artificial Intelligence",
        "tags": []
    },
    "https://github.com/dennybritz/reinforcement-learning": {
        "extra-tags": [],
        "date": "2016-08-24",
        "title": "reinforcement-learning",
        "summary": "Implementation of Reinforcement Learning Algorithms. Python, OpenAI Gym, Tensorflow. Exercises and Solutions to accompany Sutton's Book and David Silver's course.",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/DebPanigrahi/Machine-Learning": {
        "extra-tags": [],
        "date": "2017-10-21",
        "title": "Machine-Learning",
        "summary": "Machine  learning algorithms",
        "tags": [
            "python"
        ]
    },
    "https://github.com/lutzroeder/netron": {
        "extra-tags": [],
        "date": "2010-12-26",
        "title": "netron",
        "summary": "Visualizer for neural network, deep learning, and machine learning models",
        "tags": [
            "coreml",
            "caffe2",
            "keras",
            "machine-learning",
            "machinelearning",
            "mxnet",
            "caffe",
            "tensorflow-lite",
            "tensorflow",
            "torch",
            "visualizer",
            "neural-network",
            "javascript",
            "ai",
            "deep-learning",
            "deeplearning",
            "paddle",
            "pytorch",
            "ml",
            "onnx",
            "darknet"
        ]
    },
    "https://github.com/dask/dask": {
        "extra-tags": [],
        "date": "2015-01-04",
        "title": "dask",
        "summary": "Parallel computing with task scheduling",
        "tags": [
            "python",
            "numpy",
            "scikit-learn",
            "scipy",
            "pandas",
            "dask",
            "pydata"
        ]
    },
    "https://github.com/numba/numba": {
        "extra-tags": [],
        "date": "2012-03-08",
        "title": "numba",
        "summary": "NumPy aware dynamic Python compiler using LLVM",
        "tags": [
            "compiler",
            "python",
            "llvm",
            "parallel",
            "cuda",
            "numpy"
        ]
    },
    "https://github.com/vinta/awesome-python": {
        "extra-tags": [],
        "date": "2014-06-27",
        "title": "awesome-python",
        "summary": "A curated list of awesome Python frameworks, libraries, software and resources",
        "tags": [
            "python-framework",
            "python",
            "python-resources",
            "collections",
            "awesome",
            "python-library"
        ]
    },
    "https://github.com/Tencent/tencent-ml-images": {
        "extra-tags": [],
        "date": "2018-10-15",
        "title": "tencent-ml-images",
        "summary": "Largest multi-label image database; ResNet-101 model; 80.73% top-1 acc on ImageNet",
        "tags": [
            "computer-vision",
            "python",
            "database",
            "deep-learning"
        ]
    },
    "https://github.com/ray-project/ray": {
        "extra-tags": [],
        "date": "2016-10-25",
        "title": "ray",
        "summary": "Ray is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a toolkit of libraries (Ray AIR) for accelerating ML workloads.",
        "tags": [
            "java",
            "machine-learning",
            "optimization",
            "parallel",
            "tensorflow",
            "automl",
            "ray",
            "python",
            "reinforcement-learning",
            "deployment",
            "data-science",
            "deep-learning",
            "hyperparameter-optimization",
            "hyperparameter-search",
            "serving",
            "distributed",
            "pytorch",
            "model-selection",
            "rllib"
        ]
    },
    "https://github.com/modin-project/modin": {
        "extra-tags": [],
        "date": "2018-06-21",
        "title": "modin",
        "summary": "Modin: Scale your Pandas workflows by changing a single line of code",
        "tags": [
            "modin",
            "python",
            "analytics",
            "sql",
            "hacktoberfest",
            "distributed",
            "data-science",
            "pandas",
            "dataframe",
            "datascience"
        ]
    },
    "https://github.com/rapidsai/cudf": {
        "extra-tags": [],
        "date": "2017-05-07",
        "title": "cudf",
        "summary": "cuDF - GPU DataFrame Library ",
        "tags": [
            "arrow",
            "python",
            "cudf",
            "cuda",
            "gpu",
            "data-analysis",
            "rapids",
            "data-science",
            "pandas",
            "dask",
            "cpp",
            "c++",
            "dataframe",
            "pydata"
        ]
    },
    "https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap": {
        "extra-tags": [],
        "date": "2016-10-14",
        "title": "Deep-Learning-Papers-Reading-Roadmap",
        "summary": "Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech!",
        "tags": [
            "python",
            "deep-learning"
        ]
    },
    "https://github.com/PaddlePaddle/VisualDL": {
        "extra-tags": [],
        "date": "2017-12-20",
        "title": "VisualDL",
        "summary": "Deep Learning Visualization Toolkit\uff08\u300e\u98de\u6868\u300f\u6df1\u5ea6\u5b66\u4e60\u53ef\u89c6\u5316\u5de5\u5177 \uff09",
        "tags": [
            "caffe",
            "html",
            "deep-learning",
            "paddlepaddle",
            "onnx",
            "visualization"
        ]
    },
    "https://github.com/cemoody/lda2vec": {
        "extra-tags": [],
        "date": "2015-12-25",
        "title": "lda2vec",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/aleju/imgaug": {
        "extra-tags": [],
        "date": "2015-07-10",
        "title": "imgaug",
        "summary": "Image augmentation for machine learning experiments.",
        "tags": [
            "image-augmentation",
            "images",
            "python",
            "bounding-boxes",
            "keypoints",
            "polygon",
            "machine-learning",
            "segmentation-maps",
            "heatmap",
            "augmentation",
            "affine-transformation",
            "deep-learning",
            "contrast",
            "augment-images",
            "crop"
        ]
    },
    "https://github.com/google/dopamine": {
        "extra-tags": [],
        "date": "2018-07-26",
        "title": "dopamine",
        "summary": "Dopamine is a research framework for fast prototyping of reinforcement learning algorithms. ",
        "tags": [
            "google",
            "ai",
            "tensorflow",
            "jupyter notebook",
            "ml",
            "rl"
        ]
    },
    "https://github.com/scikit-learn-contrib/boruta_py": {
        "extra-tags": [],
        "date": "2016-01-30",
        "title": "boruta_py",
        "summary": "Python implementations of the Boruta all-relevant feature selection method.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/Developer-Y/cs-video-courses": {
        "extra-tags": [],
        "date": "2016-10-21",
        "title": "cs-video-courses",
        "summary": "List of Computer Science courses with video lectures.",
        "tags": [
            "embedded-systems",
            "machine-learning",
            "systems",
            "quantum-computing",
            "computational-biology",
            "computer-architecture",
            "bioinformatics",
            "programming-language",
            "algorithms",
            "reinforcement-learning",
            "database-systems",
            "web-development",
            "computer-science",
            "deep-learning",
            "databases",
            "computational-physics",
            "security",
            "robotics",
            "computer-vision"
        ]
    },
    "https://github.com/slundberg/shap": {
        "extra-tags": [],
        "date": "2016-11-22",
        "title": "shap",
        "summary": "A game theoretic approach to explain the output of any machine learning model.",
        "tags": [
            "gradient-boosting",
            "machine-learning",
            "jupyter notebook",
            "shap",
            "deep-learning",
            "shapley",
            "interpretability",
            "explainability"
        ]
    },
    "https://github.com/pytorch/pytorch": {
        "extra-tags": [],
        "date": "2016-08-13",
        "title": "pytorch",
        "summary": "Tensors and Dynamic neural networks in Python with strong GPU acceleration",
        "tags": [
            "neural-network",
            "tensor",
            "python",
            "gpu",
            "autograd",
            "machine-learning",
            "numpy",
            "deep-learning",
            "c++"
        ]
    },
    "https://github.com/openai/gym": {
        "extra-tags": [],
        "date": "2016-04-27",
        "title": "gym",
        "summary": "A toolkit for developing and comparing reinforcement learning algorithms.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/MehdiZouitine/game_of_tiles": {
        "extra-tags": [],
        "date": "2018-05-26",
        "title": "game_of_tiles",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/mxbi/mlcrate": {
        "extra-tags": [],
        "date": "2017-12-25",
        "title": "mlcrate",
        "summary": "A python module of handy tools and functions, mainly for ML and Kaggle",
        "tags": [
            "python"
        ]
    },
    "https://github.com/Kaggle/kaggle-api": {
        "extra-tags": [
            "kaggle",
            "api"
        ],
        "date": "2018-01-25",
        "title": "kaggle-api",
        "summary": "Official Kaggle API",
        "tags": [
            "python"
        ]
    },
    "https://github.com/mdbloice/Augmentor": {
        "extra-tags": [],
        "date": "2016-03-01",
        "title": "Augmentor",
        "summary": "Image augmentation library in Python for machine learning.",
        "tags": [
            "python",
            "neural-networks",
            "machine-learning",
            "augmentation",
            "deep-learning"
        ]
    },
    "https://github.com/dtuit/TwitterWebsiteSearch": {
        "extra-tags": [],
        "date": "2016-06-22",
        "title": "TwitterWebsiteSearch",
        "summary": "Extract Tweets from twitter search without using the official API",
        "tags": [
            "twitter",
            "scraper",
            "html",
            "tweets",
            "twitter-api",
            "api",
            "twitter-search"
        ]
    },
    "https://github.com/jerryjliu/llama_index": {
        "extra-tags": [],
        "date": "2022-11-02",
        "title": "llama_index",
        "summary": "LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM's with external data.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/community/community": {
        "extra-tags": [],
        "date": "2020-10-06",
        "title": "community",
        "summary": "Public feedback discussions for: GitHub Mobile, GitHub Discussions, GitHub Codespaces, GitHub Sponsors, GitHub Issues and more!",
        "tags": [
            "github-codespaces",
            "feedback",
            "github-packages",
            "github-sponsors",
            "github-mobile",
            "github-actions",
            "github",
            "github-discussions",
            "github-releases",
            "github-enterprise",
            "github-issues"
        ]
    },
    "https://github.com/pypackaging-native/pypackaging-native": {
        "extra-tags": [],
        "date": "2022-12-15",
        "title": "pypackaging-native",
        "summary": "A collection of content about key Python packaging topics and issues for projects using native code",
        "tags": []
    },
    "https://github.com/jlevy/the-art-of-command-line": {
        "extra-tags": [],
        "date": "2015-05-20",
        "title": "the-art-of-command-line",
        "summary": "Master the command line, in one page",
        "tags": [
            "documentation",
            "linux",
            "bash",
            "windows",
            "macos",
            "unix"
        ]
    },
    "https://github.com/kedro-org/kedro": {
        "extra-tags": [],
        "date": "2019-04-18",
        "title": "kedro",
        "summary": "A Python framework for creating reproducible, maintainable and modular data science code.",
        "tags": [
            "python",
            "pipeline",
            "hacktoberfest",
            "experiment-tracking",
            "mlops",
            "machine-learning",
            "kedro"
        ]
    },
    "https://github.com/fsspec/filesystem_spec": {
        "extra-tags": [
            "filesystem"
        ],
        "date": "2018-04-23",
        "title": "filesystem_spec",
        "summary": "A specification that python filesystems should adhere to.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/scikit-learn-contrib/MAPIE": {
        "extra-tags": [
            "scikit-learn",
            "prediction"
        ],
        "date": "2021-03-30",
        "title": "MAPIE",
        "summary": "A scikit-learn-compatible module for estimating prediction intervals.",
        "tags": [
            "python",
            "regression",
            "confidence-intervals",
            "sklearn",
            "data-science",
            "jupyter notebook",
            "classification"
        ]
    },
    "https://github.com/kubernetes-client/python": {
        "extra-tags": [],
        "date": "2016-10-31",
        "title": "python",
        "summary": "Official Python client library for kubernetes",
        "tags": [
            "python",
            "client-python",
            "k8s",
            "library",
            "kubernetes",
            "k8s-sig-api-machinery"
        ]
    },
    "https://github.com/GoogleContainerTools/distroless": {
        "extra-tags": [],
        "date": "2017-04-18",
        "title": "distroless",
        "summary": "\ud83e\udd51  Language focused docker images, minus the operating system.  ",
        "tags": [
            "bazel",
            "docker",
            "starlark"
        ]
    },
    "https://github.com/jablonskidev/how-to-make-a-docs-site": {
        "extra-tags": [
            "make"
        ],
        "date": "2022-05-15",
        "title": "how-to-make-a-docs-site",
        "summary": "How to Make a Docs Site: Shortcuts for Busy Devs",
        "tags": []
    },
    "https://github.com/facebook/docusaurus": {
        "extra-tags": [
            "source"
        ],
        "date": "2017-06-20",
        "title": "docusaurus",
        "summary": "Easy to maintain open source documentation websites.",
        "tags": [
            "hacktoberfest",
            "react",
            "documentation",
            "typescript",
            "javascript",
            "website",
            "open-source"
        ]
    },
    "https://github.com/direnv/direnv": {
        "extra-tags": [
            "profile"
        ],
        "date": "2011-01-04",
        "title": "direnv",
        "summary": "unclutter your .profile",
        "tags": [
            "fish",
            "zsh",
            "bash",
            "tcsh",
            "direnv",
            "go",
            "shell",
            "environment",
            "shell-extension"
        ]
    },
    "https://github.com/backstage/backstage": {
        "extra-tags": [
            "platform"
        ],
        "date": "2020-01-24",
        "title": "backstage",
        "summary": "Backstage is an open platform for building developer portals",
        "tags": [
            "service-catalog",
            "dx",
            "hacktoberfest",
            "typescript",
            "cncf",
            "microservices",
            "backstage",
            "infrastructure",
            "developer-experience",
            "developer-portal"
        ]
    },
    "https://github.com/intake/intake": {
        "extra-tags": [
            "package",
            "data"
        ],
        "date": "2017-08-14",
        "title": "intake",
        "summary": "Intake is a lightweight package for finding, investigating, loading and disseminating data.",
        "tags": [
            "data-access",
            "python",
            "data-catalog"
        ]
    },
    "https://github.com/logicai-io/recsys2019": {
        "extra-tags": [
            "recsys"
        ],
        "date": "2019-03-05",
        "title": "recsys2019",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/mamba-org/mamba": {
        "extra-tags": [],
        "date": "2019-03-05",
        "title": "mamba",
        "summary": "The Fast Cross-Platform Package Manager",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/conda/conda-pack": {
        "extra-tags": [
            "package"
        ],
        "date": "2017-10-17",
        "title": "conda-pack",
        "summary": "Package conda environments for redistribution",
        "tags": [
            "python"
        ]
    },
    "https://github.com/oras-project/oras": {
        "extra-tags": [],
        "date": "2018-12-24",
        "title": "oras",
        "summary": "OCI registry client - managing content like artifacts, images, packages",
        "tags": [
            "storage",
            "docker",
            "registry",
            "go",
            "oci"
        ]
    },
    "https://github.com/mlflow/mlflow": {
        "extra-tags": [],
        "date": "2018-06-05",
        "title": "mlflow",
        "summary": "Open source platform for the machine learning lifecycle",
        "tags": [
            "python",
            "mlflow",
            "apache-spark",
            "model-management",
            "ai",
            "ml",
            "machine-learning"
        ]
    },
    "https://github.com/chiphuyen/just-pandas-things": {
        "extra-tags": [
            "list"
        ],
        "date": "2020-06-29",
        "title": "just-pandas-things",
        "summary": "An ongoing list of pandas quirks",
        "tags": [
            "python",
            "pandas",
            "pandas-dataframe",
            "data-science",
            "jupyter notebook",
            "pandas-tutorial",
            "machine-learning"
        ]
    },
    "https://github.com/david-cortes/contextualbandits": {
        "extra-tags": [
            "algorithms"
        ],
        "date": "2018-03-27",
        "title": "contextualbandits",
        "summary": "Python implementations of contextual bandits algorithms",
        "tags": [
            "python",
            "exploration-exploitation",
            "multiarmed-bandits",
            "reinforcement-learning",
            "contextual-bandits"
        ]
    },
    "https://github.com/oras-project/oras-py": {
        "extra-tags": [],
        "date": "2021-04-28",
        "title": "oras-py",
        "summary": "ORAS Python SDK",
        "tags": [
            "python",
            "oci-registry-as-storage",
            "oras",
            "registry",
            "packages",
            "oci"
        ]
    },
    "https://github.com/Netflix/metaflow": {
        "extra-tags": [],
        "date": "2019-09-17",
        "title": "metaflow",
        "summary": ":rocket: Build and manage real-life data science projects with ease!",
        "tags": [
            "ml-platform",
            "r",
            "reproducible-research",
            "aws",
            "ml-infrastructure",
            "azure",
            "mlops",
            "gcp",
            "kubernetes",
            "python",
            "productivity",
            "model-management",
            "data-science",
            "rstats",
            "r-package",
            "ai",
            "high-performance-computing",
            "datascience",
            "ml",
            "machine-learning"
        ]
    },
    "https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement": {
        "extra-tags": [],
        "date": "2021-07-28",
        "title": "Image-Super-Resolution-via-Iterative-Refinement",
        "summary": "Unofficial implementation of Image Super-Resolution via Iterative Refinement by Pytorch",
        "tags": [
            "python",
            "image-generation",
            "diffusion-probabilistic",
            "super-resolution",
            "pytorch",
            "ddpm"
        ]
    },
    "https://github.com/cdfoundation/artwork": {
        "extra-tags": [],
        "date": "2019-02-14",
        "title": "artwork",
        "summary": "\ud83c\udfa8Continuous Delivery Foundation Artwork, Logos, and License Guidelines",
        "tags": [
            "continuous-delivery",
            "shell",
            "cdf"
        ]
    },
    "https://github.com/cncf/artwork": {
        "extra-tags": [],
        "date": "2015-12-07",
        "title": "artwork",
        "summary": "\ud83c\udfa8 CNCF-related logos and artwork",
        "tags": [
            "artwork",
            "logos",
            "cncf"
        ]
    },
    "https://github.com/rxhanson/Rectangle": {
        "extra-tags": [
            "windows",
            "macos"
        ],
        "date": "2019-06-21",
        "title": "Rectangle",
        "summary": "Move and resize windows on macOS with keyboard shortcuts and snap areas",
        "tags": [
            "swift"
        ]
    },
    "https://github.com/alacritty/alacritty": {
        "extra-tags": [],
        "date": "2016-02-18",
        "title": "alacritty",
        "summary": "A cross-platform, OpenGL terminal emulator.",
        "tags": [
            "bsd",
            "gpu",
            "terminal",
            "linux",
            "windows",
            "terminal-emulators",
            "vte",
            "opengl",
            "macos",
            "rust"
        ]
    },
    "https://github.com/koekeishiya/yabai": {
        "extra-tags": [],
        "date": "2019-05-04",
        "title": "yabai",
        "summary": "A tiling window manager for macOS based on binary space partitioning",
        "tags": [
            "c"
        ]
    },
    "https://github.com/idealo/image-super-resolution": {
        "extra-tags": [],
        "date": "2018-11-26",
        "title": "image-super-resolution",
        "summary": "\ud83d\udd0e Super-scale your images and run experiments with Residual Dense and Adversarial Networks.",
        "tags": [
            "deep-learning",
            "python",
            "tensorflow",
            "computer-vision",
            "e-commerce",
            "convolutional-neural-networks",
            "image-processing",
            "keras",
            "super-resolution",
            "neural-network",
            "aws",
            "idealo",
            "nvidia-docker",
            "image-super-resolution",
            "machine-learning",
            "docker"
        ]
    },
    "https://github.com/MehdiZouitine/NoPainNoGan": {
        "extra-tags": [],
        "date": "2020-07-07",
        "title": "NoPainNoGan",
        "summary": "From skinny-fat to God",
        "tags": []
    },
    "https://github.com/ikatyang/emoji-cheat-sheet": {
        "extra-tags": [],
        "date": "2017-03-17",
        "title": "emoji-cheat-sheet",
        "summary": "A markdown version emoji cheat sheet",
        "tags": [
            "cheat-sheet",
            "emoji",
            "github",
            "javascript",
            "markdown"
        ]
    },
    "https://github.com/joblib/threadpoolctl": {
        "extra-tags": [
            "threadpool"
        ],
        "date": "2019-03-26",
        "title": "threadpoolctl",
        "summary": "Python helpers to limit the number of threads used in native libraries that handle their own internal threadpool (BLAS and OpenMP implementations)",
        "tags": [
            "python"
        ]
    },
    "https://github.com/amueller/COMS4995-s20": {
        "extra-tags": [
            "machine learning"
        ],
        "date": "2020-01-19",
        "title": "COMS4995-s20",
        "summary": "COMS W4995 Applied Machine Learning - Spring 20",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/Wookai/paper-tips-and-tricks": {
        "extra-tags": [],
        "date": "2015-07-09",
        "title": "paper-tips-and-tricks",
        "summary": "Best practice and tips & tricks to write scientific papers in LaTeX, with figures generated in Python or Matlab.",
        "tags": [
            "python",
            "latex",
            "notation",
            "research-paper",
            "tips-and-tricks"
        ]
    },
    "https://github.com/mlpack/mlpack": {
        "extra-tags": [],
        "date": "2014-12-17",
        "title": "mlpack",
        "summary": "mlpack: a fast, header-only C++ machine learning library",
        "tags": [
            "deep-learning",
            "regression",
            "c++",
            "hacktoberfest",
            "nearest-neighbor-search",
            "scientific-computing",
            "machine-learning",
            "machine-learning-library",
            "c-plus-plus"
        ]
    },
    "https://github.com/dabl/dabl": {
        "extra-tags": [],
        "date": "2018-09-14",
        "title": "dabl",
        "summary": "Data Analysis Baseline Library",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/badges/shields": {
        "extra-tags": [],
        "date": "2013-01-30",
        "title": "shields",
        "summary": "Concise, consistent, and legible badges in SVG and raster format",
        "tags": [
            "badge-maker",
            "metadata",
            "github",
            "javascript",
            "badge",
            "status",
            "svg"
        ]
    },
    "https://github.com/openai/spinningup": {
        "extra-tags": [],
        "date": "2018-11-07",
        "title": "spinningup",
        "summary": "An educational resource to help anyone learn deep reinforcement learning.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/Cadene/pretrained-models.pytorch": {
        "extra-tags": [],
        "date": "2017-04-09",
        "title": "pretrained-models.pytorch",
        "summary": "Pretrained ConvNets for pytorch: NASNet, ResNeXt, ResNet, InceptionV4, InceptionResnetV2, Xception, DPN, etc.",
        "tags": [
            "python",
            "resnet",
            "imagenet",
            "inception",
            "pretrained",
            "pytorch",
            "resnext"
        ]
    },
    "https://github.com/alan-turing-institute/the-turing-way": {
        "extra-tags": [],
        "date": "2018-11-01",
        "title": "the-turing-way",
        "summary": "Host repository for The Turing Way: a how to guide for reproducible data science",
        "tags": [
            "hut23-396",
            "hut23-270",
            "tex",
            "hacktoberfest",
            "community",
            "data-science",
            "education",
            "hut23",
            "closember"
        ]
    },
    "https://github.com/rushter/MLAlgorithms": {
        "extra-tags": [],
        "date": "2016-10-05",
        "title": "MLAlgorithms",
        "summary": "Minimal and clean examples of machine learning algorithms implementations",
        "tags": [
            "deep-learning",
            "machine-learning-algorithms",
            "python",
            "neural-networks",
            "machine-learning"
        ]
    },
    "https://github.com/maciejkula/spotlight": {
        "extra-tags": [],
        "date": "2017-06-25",
        "title": "spotlight",
        "summary": "Deep recommender models using PyTorch.",
        "tags": [
            "deep-learning",
            "python",
            "recommender-system",
            "learning-to-rank",
            "pytorch",
            "machine-learning",
            "matrix-factorization"
        ]
    },
    "https://github.com/holtzy/data_to_viz": {
        "extra-tags": [
            "dataviz"
        ],
        "date": "2018-02-19",
        "title": "data_to_viz",
        "summary": "Leading to the dataviz you need",
        "tags": [
            "html"
        ]
    },
    "https://github.com/plotly/dash": {
        "extra-tags": [],
        "date": "2015-04-10",
        "title": "dash",
        "summary": "Data Apps & Dashboards for Python. No JavaScript Required.",
        "tags": [
            "modeling",
            "finance",
            "dash",
            "react",
            "r",
            "technical-computing",
            "julia",
            "bioinformatics",
            "flask",
            "gui-framework",
            "plotly",
            "charting",
            "python",
            "web-app",
            "jupyter",
            "productivity",
            "data-science",
            "rstats",
            "data-visualization",
            "plotly-dash"
        ]
    },
    "https://github.com/comake/handbook": {
        "extra-tags": [],
        "date": "2019-04-03",
        "title": "handbook",
        "summary": "An employee handbook built for inclusion",
        "tags": []
    },
    "https://github.com/clef/brunch-talks": {
        "extra-tags": [],
        "date": "2016-02-16",
        "title": "brunch-talks",
        "summary": "",
        "tags": []
    },
    "https://github.com/AdilZouitine/paper_notes": {
        "extra-tags": [
            "research",
            "papers"
        ],
        "date": "2019-04-04",
        "title": "paper_notes",
        "summary": " [WIP] Note on the research papers I read  :squirrel:",
        "tags": [
            "deep-learning",
            "deep-reinforcement-learning",
            "paper-notes"
        ]
    },
    "https://github.com/kmkolasinski/deep-learning-notes": {
        "extra-tags": [],
        "date": "2017-11-19",
        "title": "deep-learning-notes",
        "summary": "Experiments with Deep Learning",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/tensorflow/lucid": {
        "extra-tags": [],
        "date": "2018-01-25",
        "title": "lucid",
        "summary": "A collection of infrastructure and tools for research in neural network interpretability.",
        "tags": [
            "tensorflow",
            "interpretability",
            "visualization",
            "jupyter-notebook",
            "jupyter notebook",
            "machine-learning",
            "colab"
        ]
    },
    "https://github.com/alexbw/Netflix-Prize": {
        "extra-tags": [
            "code",
            "top"
        ],
        "date": "2012-08-24",
        "title": "Netflix-Prize",
        "summary": "The code I used to get in the top #150 in the Netflix Prize",
        "tags": [
            "c"
        ]
    },
    "https://github.com/reiinakano/scikit-plot": {
        "extra-tags": [],
        "date": "2017-02-04",
        "title": "scikit-plot",
        "summary": "An intuitive library to add plotting functionality to scikit-learn objects.",
        "tags": [
            "python",
            "plot",
            "visualization",
            "scikit-learn",
            "plotting",
            "data-science",
            "machine-learning"
        ]
    },
    "https://arxiv.org/abs/2202.06991": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "mostafa dehghani",
            "attention is all you need",
            "nlp google",
            "information retrieval"
        ],
        "title": "[2202.06991] Transformer Memory as a Differentiable Search Index",
        "summary": "In this paper, we demonstrate that information retrieval can be accomplished\nwith a single Transformer, in which all information about the corpus is encoded\nin the parameters of the model. To this end, we introduce the Differentiable\nSearch Index (DSI), a new paradigm that learns a text-to-text model that maps\nstring queries directly to relevant docids; in other words, a DSI model answers\nqueries directly using only its parameters, dramatically simplifying the whole\nretrieval process. We study variations in how documents and their identifiers\nare represented, variations in training procedures, and the interplay between\nmodels and corpus sizes. Experiments demonstrate that given appropriate design\nchoices, DSI significantly outperforms strong baselines such as dual encoder\nmodels. Moreover, DSI demonstrates strong generalization capabilities,\noutperforming a BM25 baseline in a zero-shot setup.",
        "date": "2022-02-14"
    },
    "https://arxiv.org/abs/physics/0004057": {
        "extra-tags": [],
        "tags": [
            "information bottleneck method",
            "naftali tishby",
            "arxiv doc"
        ],
        "title": "[physics/0004057] The information bottleneck method",
        "summary": "We define the relevant information in a signal $x\\in X$ as being the\ninformation that this signal provides about another signal $y\\in \\Y$. Examples\ninclude the information that face images provide about the names of the people\nportrayed, or the information that speech sounds provide about the words\nspoken. Understanding the signal $x$ requires more than just predicting $y$, it\nalso requires specifying which features of $\\X$ play a role in the prediction.\nWe formalize this problem as that of finding a short code for $\\X$ that\npreserves the maximum information about $\\Y$. That is, we squeeze the\ninformation that $\\X$ provides about $\\Y$ through a `bottleneck' formed by a\nlimited set of codewords $\\tX$. This constrained optimization problem can be\nseen as a generalization of rate distortion theory in which the distortion\nmeasure $d(x,\\x)$ emerges from the joint statistics of $\\X$ and $\\Y$. This\napproach yields an exact set of self consistent equations for the coding rules\n$X \\to \\tX$ and $\\tX \\to \\Y$. Solutions to these equations can be found by a\nconvergent re-estimation method that generalizes the Blahut-Arimoto algorithm.\nOur variational principle provides a surprisingly rich framework for discussing\na variety of problems in signal processing and learning, as will be described\nin detail elsewhere.",
        "date": "2000-04-24"
    },
    "https://arxiv.org/abs/1909.04120": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "memory in deep learning",
            "michael glass",
            "question answering",
            "not encoding knowledge in language model",
            "attention is all you need",
            "knowledge augmented language models",
            "nlp ibm",
            "language models knowledge"
        ],
        "title": "[1909.04120] Span Selection Pre-training for Question Answering",
        "summary": "BERT (Bidirectional Encoder Representations from Transformers) and related\npre-trained Transformers have provided large gains across many language\nunderstanding tasks, achieving a new state-of-the-art (SOTA). BERT is\npre-trained on two auxiliary tasks: Masked Language Model and Next Sentence\nPrediction. In this paper we introduce a new pre-training task inspired by\nreading comprehension and an effort to avoid encoding general knowledge in the\ntransformer network itself. We find significant and consistent improvements\nover both BERT-BASE and BERT-LARGE on multiple reading comprehension (MRC) and\nparaphrasing datasets. Specifically, our proposed model has strong empirical\nevidence as it obtains SOTA results on Natural Questions, a new benchmark MRC\ndataset, outperforming BERT-LARGE by 3 F1 points on short answer prediction. We\nalso establish a new SOTA in HotpotQA, improving answer prediction F1 by 4 F1\npoints and supporting fact prediction by 1 F1 point. Moreover, we show that our\npre-training approach is particularly effective when training data is limited,\nimproving the learning curve by a large amount.",
        "date": "2019-09-09"
    },
    "https://arxiv.org/abs/2106.10199": {
        "extra-tags": [],
        "tags": [
            "yoav goldberg",
            "bert fine tuning",
            "allen institute for ai a2i",
            "arxiv doc"
        ],
        "title": "[2106.10199] BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
        "summary": "We introduce BitFit, a sparse-finetuning method where only the bias-terms of\nthe model (or a subset of them) are being modified. We show that with\nsmall-to-medium training data, applying BitFit on pre-trained BERT models is\ncompetitive with (and sometimes better than) fine-tuning the entire model. For\nlarger data, the method is competitive with other sparse fine-tuning methods.\nBesides their practical utility, these findings are relevant for the question\nof understanding the commonly-used process of finetuning: they support the\nhypothesis that finetuning is mainly about exposing knowledge induced by\nlanguage-modeling training, rather than learning new task-specific linguistic\nknowledge.",
        "date": "2021-06-18"
    },
    "https://arxiv.org/abs/2004.04906": {
        "extra-tags": [],
        "tags": [
            "two tower algorithm",
            "arxiv doc",
            "emnlp 2020",
            "nlp facebook",
            "dense passage retrieval",
            "open domain question answering"
        ],
        "title": "[2004.04906] Dense Passage Retrieval for Open-Domain Question Answering",
        "summary": "Open-domain question answering relies on efficient passage retrieval to\nselect candidate contexts, where traditional sparse vector space models, such\nas TF-IDF or BM25, are the de facto method. In this work, we show that\nretrieval can be practically implemented using dense representations alone,\nwhere embeddings are learned from a small number of questions and passages by a\nsimple dual-encoder framework. When evaluated on a wide range of open-domain QA\ndatasets, our dense retriever outperforms a strong Lucene-BM25 system largely\nby 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our\nend-to-end QA system establish new state-of-the-art on multiple open-domain QA\nbenchmarks.",
        "date": "2020-04-10"
    },
    "https://arxiv.org/abs/1909.03193": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "pre trained language models",
            "knowledge graph completion",
            "discute avec raphael",
            "bert",
            "attention is all you need"
        ],
        "title": "[1909.03193] KG-BERT: BERT for Knowledge Graph Completion",
        "summary": "Knowledge graphs are important resources for many artificial intelligence\ntasks but often suffer from incompleteness. In this work, we propose to use\npre-trained language models for knowledge graph completion. We treat triples in\nknowledge graphs as textual sequences and propose a novel framework named\nKnowledge Graph Bidirectional Encoder Representations from Transformer\n(KG-BERT) to model these triples. Our method takes entity and relation\ndescriptions of a triple as input and computes scoring function of the triple\nwith the KG-BERT language model. Experimental results on multiple benchmark\nknowledge graphs show that our method can achieve state-of-the-art performance\nin triple classification, link prediction and relation prediction tasks.",
        "date": "2019-09-07"
    },
    "https://arxiv.org/abs/1706.00384": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "knowledge distillation",
            "mutual learning",
            "kd mkb biblio"
        ],
        "title": "[1706.00384] Deep Mutual Learning",
        "summary": "Model distillation is an effective and widely used technique to transfer\nknowledge from a teacher to a student network. The typical application is to\ntransfer from a powerful large network or ensemble to a small network, that is\nbetter suited to low-memory or fast execution requirements. In this paper, we\npresent a deep mutual learning (DML) strategy where, rather than one way\ntransfer between a static pre-defined teacher and a student, an ensemble of\nstudents learn collaboratively and teach each other throughout the training\nprocess. Our experiments show that a variety of network architectures benefit\nfrom mutual learning and achieve compelling results on CIFAR-100 recognition\nand Market-1501 person re-identification benchmarks. Surprisingly, it is\nrevealed that no prior powerful teacher network is necessary -- mutual learning\nof a collection of simple student networks works, and moreover outperforms\ndistillation from a more powerful yet static teacher.",
        "date": "2017-06-01"
    },
    "https://arxiv.org/abs/2110.08151": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "multilingual nlp",
            "entities and lm",
            "masked entity prediction task",
            "luke",
            "ikuya yamada"
        ],
        "title": "[2110.08151] mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models",
        "summary": "Recent studies have shown that multilingual pretrained language models can be\neffectively improved with cross-lingual alignment information from Wikipedia\nentities. However, existing methods only exploit entity information in\npretraining and do not explicitly use entities in downstream tasks. In this\nstudy, we explore the effectiveness of leveraging entity representations for\ndownstream cross-lingual tasks. We train a multilingual language model with 24\nlanguages with entity representations and show the model consistently\noutperforms word-based pretrained models in various cross-lingual transfer\ntasks. We also analyze the model and the key insight is that incorporating\nentity representations into the input allows us to extract more\nlanguage-agnostic features. We also evaluate the model with a multilingual\ncloze prompt task with the mLAMA dataset. We show that entity-based prompt\nelicits correct factual knowledge more likely than using only word\nrepresentations. Our source code and pretrained models are available at\nhttps://github.com/studio-ousia/luke.",
        "date": "2021-10-15"
    },
    "https://arxiv.org/abs/2201.04337": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "prompted models",
            "bert and sentence embeddings"
        ],
        "title": "[2201.04337] PromptBERT: Improving BERT Sentence Embeddings with Prompts",
        "summary": "The poor performance of the original BERT for sentence semantic similarity\nhas been widely discussed in previous works. We find that unsatisfactory\nperformance is mainly due to the static token embeddings biases and the\nineffective BERT layers, rather than the high cosine similarity of the sentence\nembeddings. To this end, we propose a prompt based sentence embeddings method\nwhich can reduce token embeddings biases and make the original BERT layers more\neffective. By reformulating the sentence embeddings task as the\nfillin-the-blanks problem, our method significantly improves the performance of\noriginal BERT. We discuss two prompt representing methods and three prompt\nsearching methods for prompt based sentence embeddings. Moreover, we propose a\nnovel unsupervised training objective by the technology of template denoising,\nwhich substantially shortens the performance gap between the supervised and\nunsupervised setting. For experiments, we evaluate our method on both non\nfine-tuned and fine-tuned settings. Even a non fine-tuned method can outperform\nthe fine-tuned methods like unsupervised ConSERT on STS tasks. Our fine-tuned\nmethod outperforms the state-of-the-art method SimCSE in both unsupervised and\nsupervised settings. Compared to SimCSE, we achieve 2.29 and 2.58 points\nimprovements on BERT and RoBERTa respectively under the unsupervised setting.",
        "date": "2022-01-12"
    },
    "https://arxiv.org/abs/2001.11631": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "short text clustering"
        ],
        "title": "[2001.11631] Enhancement of Short Text Clustering by Iterative Classification",
        "summary": "Short text clustering is a challenging task due to the lack of signal\ncontained in such short texts. In this work, we propose iterative\nclassification as a method to b o ost the clustering quality (e.g., accuracy)\nof short texts. Given a clustering of short texts obtained using an arbitrary\nclustering algorithm, iterative classification applies outlier removal to\nobtain outlier-free clusters. Then it trains a classification algorithm using\nthe non-outliers based on their cluster distributions. Using the trained\nclassification model, iterative classification reclassifies the outliers to\nobtain a new set of clusters. By repeating this several times, we obtain a much\nimproved clustering of texts. Our experimental results show that the proposed\nclustering enhancement method not only improves the clustering quality of\ndifferent clustering methods (e.g., k-means, k-means--, and hierarchical\nclustering) but also outperforms the state-of-the-art short text clustering\nmethods on several short text datasets by a statistically significant margin.",
        "date": "2020-01-31"
    },
    "https://arxiv.org/abs/1908.08983": {
        "extra-tags": [],
        "tags": [
            "named entity recognition",
            "labeled data",
            "cross lingual nlp",
            "arxiv doc"
        ],
        "title": "[1908.08983] A Little Annotation does a Lot of Good: A Study in Bootstrapping Low-resource Named Entity Recognizers",
        "summary": "Most state-of-the-art models for named entity recognition (NER) rely on the\navailability of large amounts of labeled data, making them challenging to\nextend to new, lower-resourced languages. However, there are now several\nproposed approaches involving either cross-lingual transfer learning, which\nlearns from other highly resourced languages, or active learning, which\nefficiently selects effective training data based on model predictions. This\npaper poses the question: given this recent progress, and limited human\nannotation, what is the most effective method for efficiently creating\nhigh-quality entity recognizers in under-resourced languages? Based on\nextensive experimentation using both simulated and real human annotation, we\nfind a dual-strategy approach best, starting with a cross-lingual transferred\nmodel, then performing targeted annotation of only uncertain entity spans in\nthe target language, minimizing annotator effort. Results demonstrate that\ncross-lingual transfer is a powerful tool when very little data can be\nannotated, but an entity-targeted annotation strategy can achieve competitive\naccuracy quickly, with just one-tenth of training data.",
        "date": "2019-08-23"
    },
    "https://arxiv.org/abs/1810.10531": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "knowledge",
            "deep learning"
        ],
        "title": "[1810.10531] A mathematical theory of semantic development in deep neural networks",
        "summary": "An extensive body of empirical research has revealed remarkable regularities\nin the acquisition, organization, deployment, and neural representation of\nhuman semantic knowledge, thereby raising a fundamental conceptual question:\nwhat are the theoretical principles governing the ability of neural networks to\nacquire, organize, and deploy abstract knowledge by integrating across many\nindividual experiences? We address this question by mathematically analyzing\nthe nonlinear dynamics of learning in deep linear networks. We find exact\nsolutions to this learning dynamics that yield a conceptual explanation for the\nprevalence of many disparate phenomena in semantic cognition, including the\nhierarchical differentiation of concepts through rapid developmental\ntransitions, the ubiquity of semantic illusions between such transitions, the\nemergence of item typicality and category coherence as factors controlling the\nspeed of semantic processing, changing patterns of inductive projection over\ndevelopment, and the conservation of semantic similarity in neural\nrepresentations across species. Thus, surprisingly, our simple neural model\nqualitatively recapitulates many diverse regularities underlying semantic\ndevelopment, while providing analytic insight into how the statistical\nstructure of an environment can interact with nonlinear deep learning dynamics\nto give rise to these regularities.",
        "date": "2018-10-23"
    },
    "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9363924": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "machine learning",
            "causal inference",
            "yoshua bengio"
        ],
        "title": "[2102.11107] Towards Causal Representation Learning",
        "summary": "The two fields of machine learning and graphical causality arose and\ndeveloped separately. However, there is now cross-pollination and increasing\ninterest in both fields to benefit from the advances of the other. In the\npresent paper, we review fundamental concepts of causal inference and relate\nthem to crucial open problems of machine learning, including transfer and\ngeneralization, thereby assaying how causality can contribute to modern machine\nlearning research. This also applies in the opposite direction: we note that\nmost work in causality starts from the premise that the causal variables are\ngiven. A central problem for AI and causality is, thus, causal representation\nlearning, the discovery of high-level causal variables from low-level\nobservations. Finally, we delineate some implications of causality for machine\nlearning and propose key research areas at the intersection of both\ncommunities.",
        "date": "2021-02-22"
    },
    "https://arxiv.org/abs/1910.02227": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "google deepmind",
            "artificial general intelligence"
        ],
        "title": "[1910.02227] Making sense of sensory input",
        "summary": "This paper attempts to answer a central question in unsupervised learning:\nwhat does it mean to \"make sense\" of a sensory sequence? In our formalization,\nmaking sense involves constructing a symbolic causal theory that both explains\nthe sensory sequence and also satisfies a set of unity conditions. The unity\nconditions insist that the constituents of the causal theory -- objects,\nproperties, and laws -- must be integrated into a coherent whole. On our\naccount, making sense of sensory input is a type of program synthesis, but it\nis unsupervised program synthesis.\nOur second contribution is a computer implementation, the Apperception\nEngine, that was designed to satisfy the above requirements. Our system is able\nto produce interpretable human-readable causal theories from very small amounts\nof data, because of the strong inductive bias provided by the unity conditions.\nA causal theory produced by our system is able to predict future sensor\nreadings, as well as retrodict earlier readings, and impute (fill in the blanks\nof) missing sensory readings, in any combination.\nWe tested the engine in a diverse variety of domains, including cellular\nautomata, rhythms and simple nursery tunes, multi-modal binding problems,\nocclusion tasks, and sequence induction intelligence tests. In each domain, we\ntest our engine's ability to predict future sensor values, retrodict earlier\nsensor values, and impute missing sensory data. The engine performs well in all\nthese domains, significantly out-performing neural net baselines. We note in\nparticular that in the sequence induction intelligence tests, our system\nachieved human-level performance. This is notable because our system is not a\nbespoke system designed specifically to solve intelligence tests, but a\ngeneral-purpose system that was designed to make sense of any sensory sequence.",
        "date": "2019-10-05"
    },
    "https://arxiv.org/abs/2010.06467": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "nlp long documents",
            "text ranking",
            "attention is all you need",
            "neural models for information retrieval"
        ],
        "title": "[2010.06467] Pretrained Transformers for Text Ranking: BERT and Beyond",
        "summary": "The goal of text ranking is to generate an ordered list of texts retrieved\nfrom a corpus in response to a query. Although the most common formulation of\ntext ranking is search, instances of the task can also be found in many natural\nlanguage processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has, without exaggeration, revolutionized the fields of natural\nlanguage processing (NLP), information retrieval (IR), and beyond. In this\nsurvey, we provide a synthesis of existing work as a single point of entry for\npractitioners who wish to gain a better understanding of how to apply\ntransformers to text ranking problems and researchers who wish to pursue work\nin this area. We cover a wide range of modern techniques, grouped into two\nhigh-level categories: transformer models that perform reranking in multi-stage\nranking architectures and learned dense representations that attempt to perform\nranking directly. There are two themes that pervade our survey: techniques for\nhandling long documents, beyond the typical sentence-by-sentence processing\napproaches used in NLP, and techniques for addressing the tradeoff between\neffectiveness (result quality) and efficiency (query latency). Although\ntransformer architectures and pretraining techniques are recent innovations,\nmany aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of\npretrained transformers for text ranking, this survey also attempts to\nprognosticate where the field is heading.",
        "date": "2020-10-13"
    },
    "https://arxiv.org/abs/2010.02353": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "african languages",
            "emnlp 2020",
            "low resource languages",
            "masakhane",
            "machine translation",
            "nlp 4 africa"
        ],
        "title": "[2010.02353] Participatory Research for Low-resourced Machine Translation: A Case Study in African Languages",
        "summary": "Research in NLP lacks geographic diversity, and the question of how NLP can\nbe scaled to low-resourced languages has not yet been adequately solved.\n\"Low-resourced\"-ness is a complex problem going beyond data availability and\nreflects systemic problems in society. In this paper, we focus on the task of\nMachine Translation (MT), that plays a crucial role for information\naccessibility and communication worldwide. Despite immense improvements in MT\nover the past decade, MT is centered around a few high-resourced languages. As\nMT researchers cannot solve the problem of low-resourcedness alone, we propose\nparticipatory research as a means to involve all necessary agents required in\nthe MT development process. We demonstrate the feasibility and scalability of\nparticipatory research with a case study on MT for African languages. Its\nimplementation leads to a collection of novel translation datasets, MT\nbenchmarks for over 30 languages, with human evaluations for a third of them,\nand enables participants without formal training to make a unique scientific\ncontribution. Benchmarks, models, data, code, and evaluation results are\nreleased under https://github.com/masakhane-io/masakhane-mt.",
        "date": "2020-10-05"
    },
    "https://arxiv.org/abs/1910.04126": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nearest neighbor search",
            "word mover s distance"
        ],
        "title": "[1910.04126] Scalable Nearest Neighbor Search for Optimal Transport",
        "summary": "The Optimal Transport (a.k.a. Wasserstein) distance is an increasingly\npopular similarity measure for rich data domains, such as images or text\ndocuments. This raises the necessity for fast nearest neighbor search with\nrespect to this distance, a problem that poses a substantial computational\nbottleneck for various tasks on massive datasets.\nIn this work, we study fast tree-based approximation algorithms for searching\nnearest neighbors w.r.t. the Wasserstein-1 distance. A standard tree-based\ntechnique, known as Quadtree, has been previously shown to obtain good results.\nWe introduce a variant of this algorithm, called Flowtree, and formally prove\nit achieves asymptotically better accuracy. Our extensive experiments, on\nreal-world text and image datasets, show that Flowtree improves over various\nbaselines and existing methods in either running time or accuracy. In\nparticular, its quality of approximation is in line with previous high-accuracy\nmethods, while its running time is much faster.",
        "date": "2019-10-09"
    },
    "https://arxiv.org/abs/2203.06169": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "zero shot",
            "unsupervised machine learning",
            "neural models for information retrieval"
        ],
        "title": "[2203.06169] LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval",
        "summary": "In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever\nthat does not require any supervised data for training. Specifically, we first\npresent Iterative Contrastive Learning (ICoL) that iteratively trains the query\nand document encoders with a cache mechanism. ICoL not only enlarges the number\nof negative instances but also keeps representations of cached examples in the\nsame hidden space. We then propose Lexicon-Enhanced Dense Retrieval (LEDR) as a\nsimple yet effective way to enhance dense retrieval with lexical matching. We\nevaluate LaPraDoR on the recently proposed BEIR benchmark, including 18\ndatasets of 9 zero-shot text retrieval tasks. Experimental results show that\nLaPraDoR achieves state-of-the-art performance compared with supervised dense\nretrieval models, and further analysis reveals the effectiveness of our\ntraining strategy and objectives. Compared to re-ranking, our lexicon-enhanced\napproach can be run in milliseconds (22.5x faster) while achieving superior\nperformance.",
        "date": "2022-03-11"
    },
    "https://arxiv.org/abs/1901.00596": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "graph neural networks",
            "survey"
        ],
        "title": "[1901.00596] A Comprehensive Survey on Graph Neural Networks",
        "summary": "Deep learning has revolutionized many machine learning tasks in recent years,\nranging from image classification and video processing to speech recognition\nand natural language understanding. The data in these tasks are typically\nrepresented in the Euclidean space. However, there is an increasing number of\napplications where data are generated from non-Euclidean domains and are\nrepresented as graphs with complex relationships and interdependency between\nobjects. The complexity of graph data has imposed significant challenges on\nexisting machine learning algorithms. Recently, many studies on extending deep\nlearning approaches for graph data have emerged. In this survey, we provide a\ncomprehensive overview of graph neural networks (GNNs) in data mining and\nmachine learning fields. We propose a new taxonomy to divide the\nstate-of-the-art graph neural networks into four categories, namely recurrent\ngraph neural networks, convolutional graph neural networks, graph autoencoders,\nand spatial-temporal graph neural networks. We further discuss the applications\nof graph neural networks across various domains and summarize the open source\ncodes, benchmark data sets, and model evaluation of graph neural networks.\nFinally, we propose potential research directions in this rapidly growing\nfield.",
        "date": "2019-01-03"
    },
    "https://arxiv.org/abs/2002.10640": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "virtual knowledge graph",
            "end to end learning",
            "neural memory",
            "differentiable reasoning over text",
            "multi hop reasonning",
            "ruslan salakhutdinov",
            "knowledge base"
        ],
        "title": "[2002.10640] Differentiable Reasoning over a Virtual Knowledge Base",
        "summary": "We consider the task of answering complex multi-hop questions using a corpus\nas a virtual knowledge base (KB). In particular, we describe a neural module,\nDrKIT, that traverses textual data like a KB, softly following paths of\nrelations between mentions of entities in the corpus. At each step the module\nuses a combination of sparse-matrix TFIDF indices and a maximum inner product\nsearch (MIPS) on a special index of contextual representations of the mentions.\nThis module is differentiable, so the full system can be trained end-to-end\nusing gradient based methods, starting from natural language inputs. We also\ndescribe a pretraining scheme for the contextual representation encoder by\ngenerating hard negative examples using existing knowledge bases. We show that\nDrKIT improves accuracy by 9 points on 3-hop questions in the MetaQA dataset,\ncutting the gap between text-based and KB-based state-of-the-art by 70%. On\nHotpotQA, DrKIT leads to a 10% improvement over a BERT-based re-ranking\napproach to retrieving the relevant passages required to answer a question.\nDrKIT is also very efficient, processing 10-100x more queries per second than\nexisting multi-hop systems.",
        "date": "2020-02-25"
    },
    "https://arxiv.org/abs/2105.00828": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "sebastian ruder",
            "pre trained language models"
        ],
        "title": "[2105.00828] Memorisation versus Generalisation in Pre-trained Language Models",
        "summary": "State-of-the-art pre-trained language models have been shown to memorise\nfacts and perform well with limited amounts of training data. To gain a better\nunderstanding of how these models learn, we study their generalisation and\nmemorisation capabilities in noisy and low-resource scenarios. We find that the\ntraining of these models is almost unaffected by label noise and that it is\npossible to reach near-optimal results even on extremely noisy datasets.\nHowever, our experiments also show that they mainly learn from high-frequency\npatterns and largely fail when tested on low-resource tasks such as few-shot\nlearning and rare entity recognition. To mitigate such limitations, we propose\nan extension based on prototypical networks that improves performance in\nlow-resource named entity recognition tasks.",
        "date": "2021-04-16"
    },
    "https://arxiv.org/abs/2004.05119": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "sbert fine tuning",
            "nlp amazon",
            "nlp microsoft",
            "acl 2020",
            "nlp low resource scenarios"
        ],
        "title": "[2004.05119] Beyond Fine-tuning: Few-Sample Sentence Embedding Transfer",
        "summary": "Fine-tuning (FT) pre-trained sentence embedding models on small datasets has\nbeen shown to have limitations. In this paper we show that concatenating the\nembeddings from the pre-trained model with those from a simple sentence\nembedding model trained only on the target data, can improve over the\nperformance of FT for few-sample tasks. To this end, a linear classifier is\ntrained on the combined embeddings, either by freezing the embedding model\nweights or training the classifier and embedding models end-to-end. We perform\nevaluation on seven small datasets from NLP tasks and show that our approach\nwith end-to-end training outperforms FT with negligible computational overhead.\nFurther, we also show that sophisticated combination techniques like CCA and\nKCCA do not work as well in practice as concatenation. We provide theoretical\nanalysis to explain this empirical observation.",
        "date": "2020-04-10"
    },
    "https://arxiv.org/abs/1906.08237": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "xlnet"
        ],
        "title": "[1906.08237] XLNet: Generalized Autoregressive Pretraining for Language Understanding",
        "summary": "With the capability of modeling bidirectional contexts, denoising\nautoencoding based pretraining like BERT achieves better performance than\npretraining approaches based on autoregressive language modeling. However,\nrelying on corrupting the input with masks, BERT neglects dependency between\nthe masked positions and suffers from a pretrain-finetune discrepancy. In light\nof these pros and cons, we propose XLNet, a generalized autoregressive\npretraining method that (1) enables learning bidirectional contexts by\nmaximizing the expected likelihood over all permutations of the factorization\norder and (2) overcomes the limitations of BERT thanks to its autoregressive\nformulation. Furthermore, XLNet integrates ideas from Transformer-XL, the\nstate-of-the-art autoregressive model, into pretraining. Empirically, under\ncomparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a\nlarge margin, including question answering, natural language inference,\nsentiment analysis, and document ranking.",
        "date": "2019-06-19"
    },
    "https://arxiv.org/abs/2003.11644": {
        "extra-tags": [],
        "tags": [
            "text multi label classification",
            "classification relations between classes",
            "graph attention networks",
            "arxiv doc"
        ],
        "title": "[2003.11644] MAGNET: Multi-Label Text Classification using Attention-based Graph Neural Network",
        "summary": "In Multi-Label Text Classification (MLTC), one sample can belong to more than\none class. It is observed that most MLTC tasks, there are dependencies or\ncorrelations among labels. Existing methods tend to ignore the relationship\namong labels. In this paper, a graph attention network-based model is proposed\nto capture the attentive dependency structure among the labels. The graph\nattention network uses a feature matrix and a correlation matrix to capture and\nexplore the crucial dependencies between the labels and generate classifiers\nfor the task. The generated classifiers are applied to sentence feature vectors\nobtained from the text feature extraction network (BiLSTM) to enable end-to-end\ntraining. Attention allows the system to assign different weights to neighbor\nnodes per label, thus allowing it to learn the dependencies among labels\nimplicitly. The results of the proposed model are validated on five real-world\nMLTC datasets. The proposed model achieves similar or better performance\ncompared to the previous state-of-the-art models.",
        "date": "2020-03-22"
    },
    "https://arxiv.org/abs/1503.02406": {
        "extra-tags": [],
        "tags": [
            "information bottleneck method",
            "information theory and deep learning",
            "naftali tishby",
            "arxiv doc"
        ],
        "title": "[1503.02406] Deep Learning and the Information Bottleneck Principle",
        "summary": "Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the\ninformation bottleneck (IB) principle. We first show that any DNN can be\nquantified by the mutual information between the layers and the input and\noutput variables. Using this representation we can calculate the optimal\ninformation theoretic limits of the DNN and obtain finite sample generalization\nbounds. The advantage of getting closer to the theoretical limit is\nquantifiable both by the generalization bound and by the network's simplicity.\nWe argue that both the optimal architecture, number of layers and\nfeatures/connections at each layer, are related to the bifurcation points of\nthe information bottleneck tradeoff, namely, relevant compression of the input\nlayer with respect to the output layer. The hierarchical representations at the\nlayered network naturally correspond to the structural phase transitions along\nthe information curve. We believe that this new insight can lead to new\noptimality bounds and deep learning algorithms.",
        "date": "2015-03-09"
    },
    "https://arxiv.org/abs/1910.06294": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "pre trained language models",
            "entity tagging",
            "moshe wasserblat",
            "nlp low resource scenarios"
        ],
        "title": "[1910.06294] Training Compact Models for Low Resource Entity Tagging using Pre-trained Language Models",
        "summary": "Training models on low-resource named entity recognition tasks has been shown\nto be a challenge, especially in industrial applications where deploying\nupdated models is a continuous effort and crucial for business operations. In\nsuch cases there is often an abundance of unlabeled data, while labeled data is\nscarce or unavailable. Pre-trained language models trained to extract\ncontextual features from text were shown to improve many natural language\nprocessing (NLP) tasks, including scarcely labeled tasks, by leveraging\ntransfer learning. However, such models impose a heavy memory and computational\nburden, making it a challenge to train and deploy such models for inference\nuse. In this work-in-progress we combined the effectiveness of transfer\nlearning provided by pre-trained masked language models with a semi-supervised\napproach to train a fast and compact model using labeled and unlabeled\nexamples. Preliminary evaluations show that the compact models can achieve\ncompetitive accuracy with 36x compression rate when compared with a\nstate-of-the-art pre-trained language model, and run significantly faster in\ninference, allowing deployment of such models in production environments or on\nedge devices.",
        "date": "2019-10-14"
    },
    "https://arxiv.org/abs/2104.14690": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "data augmentation",
            "nlp facebook",
            "few shot learning",
            "entailment"
        ],
        "title": "[2104.14690] Entailment as Few-Shot Learner",
        "summary": "Large pre-trained language models (LMs) have demonstrated remarkable ability\nas few-shot learners. However, their success hinges largely on scaling model\nparameters to a degree that makes it challenging to train and serve. In this\npaper, we propose a new approach, named as EFL, that can turn small LMs into\nbetter few-shot learners. The key idea of this approach is to reformulate\npotential NLP task into an entailment one, and then fine-tune the model with as\nlittle as 8 examples. We further demonstrate our proposed method can be: (i)\nnaturally combined with an unsupervised contrastive learning-based data\naugmentation method; (ii) easily extended to multilingual few-shot learning. A\nsystematic evaluation on 18 standard NLP tasks demonstrates that this approach\nimproves the various existing SOTA few-shot learning methods by 12\\%, and\nyields competitive few-shot performance with 500 times larger models, such as\nGPT-3.",
        "date": "2021-04-29"
    },
    "https://arxiv.org/abs/2301.07014": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "dataset distillation"
        ],
        "title": "[2301.07014] Dataset Distillation: A Comprehensive Review",
        "summary": "Recent success of deep learning can be largely attributed to the huge amount\nof data used for training deep neural networks. However, the sheer amount of\ndata significantly increase the burden on storage and transmission. It would\nalso consume considerable time and computational resources to train models on\nsuch large datasets. Moreover, directly publishing raw data inevitably raise\nconcerns on privacy and copyright. Focusing on these inconveniences, dataset\ndistillation (DD), also known as dataset condensation (DC), has become a\npopular research topic in recent years. Given an original large dataset, DD\naims at a much smaller dataset containing several synthetic samples, such that\nmodels trained on the synthetic dataset can have comparable performance with\nthose trained on the original real one. This paper presents a comprehensive\nreview and summary for recent advances in DD and its application. We first\nintroduce the task in formal and propose an overall algorithmic framework\nfollowed by all existing DD methods. Then, we provide a systematic taxonomy of\ncurrent methodologies in this area. Their theoretical relationship will also be\ndiscussed. We also point out current challenges in DD through extensive\nexperiments and envision possible directions for future works.",
        "date": "2023-01-17"
    },
    "https://arxiv.org/abs/2208.11857": {
        "extra-tags": [],
        "tags": [
            "confiance ai",
            "arxiv doc",
            "survey",
            "language model",
            "shortcut learning",
            "dataset bias",
            "nlu"
        ],
        "title": "[2208.11857] Shortcut Learning of Large Language Models in Natural Language Understanding: A Survey",
        "summary": "Large language models (LLMs) have achieved state-of-the-art performance on a\nseries of natural language understanding tasks. However, these LLMs might rely\non dataset bias and artifacts as shortcuts for prediction. This has\nsignificantly hurt their Out-of-Distribution (OOD) generalization and\nadversarial robustness. In this paper, we provide a review of recent\ndevelopments that address the robustness challenge of LLMs. We first introduce\nthe concepts and robustness challenge of LLMs. We then introduce methods to\nidentify shortcut learning behavior in LLMs, characterize the reasons for\nshortcut learning, as well as introduce mitigation solutions. Finally, we\nidentify key challenges and introduce the connections of this line of research\nto other directions.",
        "date": "2022-08-25"
    },
    "https://arxiv.org/abs/1912.13318": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "scanned documents",
            "layoutlm",
            "good"
        ],
        "title": "[1912.13318] LayoutLM: Pre-training of Text and Layout for Document Image Understanding",
        "summary": "Pre-training techniques have been verified successfully in a variety of NLP\ntasks in recent years. Despite the widespread use of pre-training models for\nNLP applications, they almost exclusively focus on text-level manipulation,\nwhile neglecting layout and style information that is vital for document image\nunderstanding. In this paper, we propose the \\textbf{LayoutLM} to jointly model\ninteractions between text and layout information across scanned document\nimages, which is beneficial for a great number of real-world document image\nunderstanding tasks such as information extraction from scanned documents.\nFurthermore, we also leverage image features to incorporate words' visual\ninformation into LayoutLM. To the best of our knowledge, this is the first time\nthat text and layout are jointly learned in a single framework for\ndocument-level pre-training. It achieves new state-of-the-art results in\nseveral downstream tasks, including form understanding (from 70.72 to 79.27),\nreceipt understanding (from 94.02 to 95.24) and document image classification\n(from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly\navailable at \\url{https://aka.ms/layoutlm}.",
        "date": "2019-12-31"
    },
    "https://arxiv.org/abs/1802.07044": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "minimum description length principle",
            "facebook fair",
            "occam s razor",
            "nlp ens",
            "overfitting",
            "information theory and deep learning",
            "dl why does it work"
        ],
        "title": "[1802.07044] The Description Length of Deep Learning Models",
        "summary": "Solomonoff's general theory of inference and the Minimum Description Length\nprinciple formalize Occam's razor, and hold that a good model of data is a\nmodel that is good at losslessly compressing the data, including the cost of\ndescribing the model itself. Deep neural networks might seem to go against this\nprinciple given the large number of parameters to be encoded.\nWe demonstrate experimentally the ability of deep neural networks to compress\nthe training data even when accounting for parameter encoding. The compression\nviewpoint originally motivated the use of variational methods in neural\nnetworks. Unexpectedly, we found that these variational methods provide\nsurprisingly poor compression bounds, despite being explicitly built to\nminimize such bounds. This might explain the relatively poor practical\nperformance of variational methods in deep learning. On the other hand, simple\nincremental encoding methods yield excellent compression values on deep\nnetworks, vindicating Solomonoff's approach.",
        "date": "2018-02-20"
    },
    "https://arxiv.org/abs/2003.08001": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "graph embeddings",
            "embedding evaluation",
            "critical evaluation",
            "knowledge graph completion",
            "link prediction"
        ],
        "title": "[2003.08001] Realistic Re-evaluation of Knowledge Graph Completion Methods: An Experimental Study",
        "summary": "In the active research area of employing embedding models for knowledge graph\ncompletion, particularly for the task of link prediction, most prior studies\nused two benchmark datasets FB15k and WN18 in evaluating such models. Most\ntriples in these and other datasets in such studies belong to reverse and\nduplicate relations which exhibit high data redundancy due to semantic\nduplication, correlation or data incompleteness. This is a case of excessive\ndata leakage---a model is trained using features that otherwise would not be\navailable when the model needs to be applied for real prediction. There are\nalso Cartesian product relations for which every triple formed by the Cartesian\nproduct of applicable subjects and objects is a true fact. Link prediction on\nthe aforementioned relations is easy and can be achieved with even better\naccuracy using straightforward rules instead of sophisticated embedding models.\nA more fundamental defect of these models is that the link prediction scenario,\ngiven such data, is non-existent in the real-world. This paper is the first\nsystematic study with the main objective of assessing the true effectiveness of\nembedding models when the unrealistic triples are removed. Our experiment\nresults show these models are much less accurate than what we used to perceive.\nTheir poor accuracy renders link prediction a task without truly effective\nautomated solution. Hence, we call for re-investigation of possible effective\napproaches.",
        "date": "2020-03-18"
    },
    "https://arxiv.org/abs/2004.11892": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "unsupervised qa",
            "discussed with ns",
            "nlp amazon",
            "acl 2020",
            "synthetic qa data",
            "factoid qa",
            "extractive question answering"
        ],
        "title": "[2004.11892] Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering",
        "summary": "Question Answering (QA) is in increasing demand as the amount of information\navailable online and the desire for quick access to this content grows. A\ncommon approach to QA has been to fine-tune a pretrained language model on a\ntask-specific labeled dataset. This paradigm, however, relies on scarce, and\ncostly to obtain, large-scale human-labeled data. We propose an unsupervised\napproach to training QA models with generated pseudo-training data. We show\nthat generating questions for QA training by applying a simple template on a\nrelated, retrieved sentence rather than the original context sentence improves\ndownstream QA performance by allowing the model to learn more complex\ncontext-question relationships. Training a QA model on this data gives a\nrelative improvement over a previous unsupervised model in F1 score on the\nSQuAD dataset by about 14%, and 20% when the answer is a named entity,\nachieving state-of-the-art performance on SQuAD for unsupervised QA.",
        "date": "2020-04-24"
    },
    "https://arxiv.org/abs/1906.05685": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp 4 africa",
            "neural machine translation"
        ],
        "title": "[1906.05685] A Focus on Neural Machine Translation for African Languages",
        "summary": "African languages are numerous, complex and low-resourced. The datasets\nrequired for machine translation are difficult to discover, and existing\nresearch is hard to reproduce. Minimal attention has been given to machine\ntranslation for African languages so there is scant research regarding the\nproblems that arise when using machine translation techniques. To begin\naddressing these problems, we trained models to translate English to five of\nthe official South African languages (Afrikaans, isiZulu, Northern Sotho,\nSetswana, Xitsonga), making use of modern neural machine translation\ntechniques. The results obtained show the promise of using neural machine\ntranslation techniques for African languages. By providing reproducible\npublicly-available data, code and results, this research aims to provide a\nstarting point for other researchers in African machine translation to compare\nto and build upon.",
        "date": "2019-06-11"
    },
    "https://arxiv.org/abs/2204.08173": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "entity type",
            "entity discovery and linking",
            "discute avec raphael",
            "open domain tasks",
            "nlp stanford",
            "entity linking"
        ],
        "title": "[2204.08173] TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval",
        "summary": "Entity retrieval--retrieving information about entity mentions in a query--is\na key step in open-domain tasks, such as question answering or fact checking.\nHowever, state-of-the-art entity retrievers struggle to retrieve rare entities\nfor ambiguous mentions due to biases towards popular entities. Incorporating\nknowledge graph types during training could help overcome popularity biases,\nbut there are several challenges: (1) existing type-based retrieval methods\nrequire mention boundaries as input, but open-domain tasks run on unstructured\ntext, (2) type-based methods should not compromise overall performance, and (3)\ntype-based methods should be robust to noisy and missing types. In this work,\nwe introduce TABi, a method to jointly train bi-encoders on knowledge graph\ntypes and unstructured text for entity retrieval for open-domain tasks. TABi\nleverages a type-enforced contrastive loss to encourage entities and queries of\nsimilar types to be close in the embedding space. TABi improves retrieval of\nrare entities on the Ambiguous Entity Retrieval (AmbER) sets, while maintaining\nstrong overall retrieval performance on open-domain tasks in the KILT benchmark\ncompared to state-of-the-art retrievers. TABi is also robust to incomplete type\nsystems, improving rare entity retrieval over baselines with only 5% type\ncoverage of the training dataset. We make our code publicly available at\nhttps://github.com/HazyResearch/tabi.",
        "date": "2022-04-18"
    },
    "https://arxiv.org/abs/2001.01447": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp microsoft",
            "entity linking",
            "entity type representation"
        ],
        "title": "[2001.01447] Improving Entity Linking by Modeling Latent Entity Type Information",
        "summary": "Existing state of the art neural entity linking models employ attention-based\nbag-of-words context model and pre-trained entity embeddings bootstrapped from\nword embeddings to assess topic level context compatibility. However, the\nlatent entity type information in the immediate context of the mention is\nneglected, which causes the models often link mentions to incorrect entities\nwith incorrect type. To tackle this problem, we propose to inject latent entity\ntype information into the entity embeddings based on pre-trained BERT. In\naddition, we integrate a BERT-based entity similarity score into the local\ncontext model of a state-of-the-art model to better capture latent entity type\ninformation. Our model significantly outperforms the state-of-the-art entity\nlinking models on standard benchmark (AIDA-CoNLL). Detailed experiment analysis\ndemonstrates that our model corrects most of the type errors produced by the\ndirect baseline.",
        "date": "2020-01-06"
    },
    "https://arxiv.org/abs/1904.02817": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "sequence labeling",
            "unsupervised domain adaptation nlp"
        ],
        "title": "[1904.02817] Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling",
        "summary": "Contextualized word embeddings such as ELMo and BERT provide a foundation for\nstrong performance across a wide range of natural language processing tasks by\npretraining on large corpora of unlabeled text. However, the applicability of\nthis approach is unknown when the target domain varies substantially from the\npretraining corpus. We are specifically interested in the scenario in which\nlabeled data is available in only a canonical source domain such as newstext,\nand the target domain is distinct from both the labeled and pretraining texts.\nTo address this scenario, we propose domain-adaptive fine-tuning, in which the\ncontextualized embeddings are adapted by masked language modeling on text from\nthe target domain. We test this approach on sequence labeling in two\nchallenging domains: Early Modern English and Twitter. Both domains differ\nsubstantially from existing pretraining corpora, and domain-adaptive\nfine-tuning yields substantial improvements over strong BERT baselines, with\nparticularly impressive results on out-of-vocabulary words. We conclude that\ndomain-adaptive fine-tuning offers a simple and effective approach for the\nunsupervised adaptation of sequence labeling to difficult new domains.",
        "date": "2019-04-04"
    },
    "https://arxiv.org/abs/2004.05150": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp long documents",
            "allen institute for ai a2i"
        ],
        "title": "[2004.05150] Longformer: The Long-Document Transformer",
        "summary": "Transformer-based models are unable to process long sequences due to their\nself-attention operation, which scales quadratically with the sequence length.\nTo address this limitation, we introduce the Longformer with an attention\nmechanism that scales linearly with sequence length, making it easy to process\ndocuments of thousands of tokens or longer. Longformer's attention mechanism is\na drop-in replacement for the standard self-attention and combines a local\nwindowed attention with a task motivated global attention. Following prior work\non long-sequence transformers, we evaluate Longformer on character-level\nlanguage modeling and achieve state-of-the-art results on text8 and enwik8. In\ncontrast to most prior work, we also pretrain Longformer and finetune it on a\nvariety of downstream tasks. Our pretrained Longformer consistently outperforms\nRoBERTa on long document tasks and sets new state-of-the-art results on WikiHop\nand TriviaQA.",
        "date": "2020-04-10"
    },
    "https://arxiv.org/abs/1306.6802": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "hierarchical classification evaluation",
            "ml evaluation",
            "hierarchical classification",
            "classification relations between classes"
        ],
        "title": "[1306.6802] Evaluation Measures for Hierarchical Classification: a unified view and novel approaches",
        "summary": "Hierarchical classification addresses the problem of classifying items into a\nhierarchy of classes. An important issue in hierarchical classification is the\nevaluation of different classification algorithms, which is complicated by the\nhierarchical relations among the classes. Several evaluation measures have been\nproposed for hierarchical classification using the hierarchy in different ways.\nThis paper studies the problem of evaluation in hierarchical classification by\nanalyzing and abstracting the key components of the existing performance\nmeasures. It also proposes two alternative generic views of hierarchical\nevaluation and introduces two corresponding novel measures. The proposed\nmeasures, along with the state-of-the art ones, are empirically tested on three\nlarge datasets from the domain of text classification. The empirical results\nillustrate the undesirable behavior of existing approaches and how the proposed\nmethods overcome most of these methods across a range of cases.",
        "date": "2013-06-28"
    },
    "https://arxiv.org/abs/1912.08904": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "chatbot",
            "question answering",
            "nlp tools",
            "nlp microsoft",
            "information retrieval"
        ],
        "title": "[1912.08904] Macaw: An Extensible Conversational Information Seeking Platform",
        "summary": "Conversational information seeking (CIS) has been recognized as a major\nemerging research area in information retrieval. Such research will require\ndata and tools, to allow the implementation and study of conversational\nsystems. This paper introduces Macaw, an open-source framework with a modular\narchitecture for CIS research. Macaw supports multi-turn, multi-modal, and\nmixed-initiative interactions, and enables research for tasks such as document\nretrieval, question answering, recommendation, and structured data exploration.\nIt has a modular design to encourage the study of new CIS algorithms, which can\nbe evaluated in batch mode. It can also integrate with a user interface, which\nallows user studies and data collection in an interactive mode, where the back\nend can be fully algorithmic or a wizard of oz setup. Macaw is distributed\nunder the MIT License.",
        "date": "2019-12-18"
    },
    "https://arxiv.org/abs/2012.15723": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "few shot learning",
            "pre trained language models"
        ],
        "title": "[2012.15723] Making Pre-trained Language Models Better Few-shot Learners",
        "summary": "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot\nperformance solely by leveraging a natural-language prompt and a few task\ndemonstrations as input context. Inspired by their findings, we study few-shot\nlearning in a more practical scenario, where we use smaller language models for\nwhich fine-tuning is computationally efficient. We present LM-BFF--better\nfew-shot fine-tuning of language models--a suite of simple and complementary\ntechniques for fine-tuning language models on a small number of annotated\nexamples. Our approach includes (1) prompt-based fine-tuning together with a\nnovel pipeline for automating prompt generation; and (2) a refined strategy for\ndynamically and selectively incorporating demonstrations into each context.\nFinally, we present a systematic evaluation for analyzing few-shot performance\non a range of NLP tasks, including classification and regression. Our\nexperiments demonstrate that our methods combine to dramatically outperform\nstandard fine-tuning procedures in this low resource setting, achieving up to\n30% absolute improvement, and 11% on average across all tasks. Our approach\nmakes minimal assumptions on task resources and domain expertise, and hence\nconstitutes a strong task-agnostic method for few-shot learning.",
        "date": "2020-12-31"
    },
    "https://arxiv.org/abs/1911.00172": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp facebook",
            "knn transformers",
            "knowledge augmented language models",
            "nlp stanford",
            "dan jurafsky",
            "generalization"
        ],
        "title": "[1911.00172] Generalization through Memorization: Nearest Neighbor Language Models",
        "summary": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM)\nby linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The\nnearest neighbors are computed according to distance in the pre-trained LM\nembedding space, and can be drawn from any text collection, including the\noriginal LM training data. Applying this augmentation to a strong Wikitext-103\nLM, with neighbors drawn from the original training set, our $k$NN-LM achieves\na new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no\nadditional training. We also show that this approach has implications for\nefficiently scaling up to larger training sets and allows for effective domain\nadaptation, by simply varying the nearest neighbor datastore, again without\nfurther training. Qualitatively, the model is particularly helpful in\npredicting rare patterns, such as factual knowledge. Together, these results\nstrongly suggest that learning similarity between sequences of text is easier\nthan predicting the next word, and that nearest neighbor search is an effective\napproach for language modeling in the long tail.",
        "date": "2019-11-01"
    },
    "https://arxiv.org/abs/2006.05987": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "bert fine tuning instabilities",
            "adam ml",
            "bert fine tuning",
            "nlp stanford"
        ],
        "title": "[2006.05987] Revisiting Few-sample BERT Fine-tuning",
        "summary": "This paper is a study of fine-tuning of BERT contextual representations, with\nfocus on commonly observed instabilities in few-sample scenarios. We identify\nseveral factors that cause this instability: the common use of a non-standard\noptimization method with biased gradient estimation; the limited applicability\nof significant parts of the BERT network for down-stream tasks; and the\nprevalent practice of using a pre-determined, and small number of training\niterations. We empirically test the impact of these factors, and identify\nalternative practices that resolve the commonly observed instability of the\nprocess. In light of these observations, we re-visit recently proposed methods\nto improve few-sample fine-tuning with BERT and re-evaluate their\neffectiveness. Generally, we observe the impact of these methods diminishes\nsignificantly with our modified process.",
        "date": "2020-06-10"
    },
    "https://arxiv.org/abs/2007.00077": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "facebook fair",
            "ai stanford",
            "active learning",
            "nearest neighbor search",
            "imbalanced data"
        ],
        "title": "[2007.00077] Similarity Search for Efficient Active Learning and Search of Rare Concepts",
        "summary": "Many active learning and search approaches are intractable for industrial\nsettings with billions of unlabeled examples. Existing approaches, such as\nuncertainty sampling or information density, search globally for the optimal\nexamples to label, scaling linearly or even quadratically with the unlabeled\ndata. However, in practice, data is often heavily skewed; only a small fraction\nof collected data will be relevant for a given learning task. For example, when\nidentifying rare classes, detecting malicious content, or debugging model\nperformance, the ratio of positive to negative examples can be 1 to 1,000 or\nmore. In this work, we exploit this skew in large training datasets to reduce\nthe number of unlabeled examples considered in each selection round by only\nlooking at the nearest neighbors to the labeled examples. Empirically, we\nobserve that learned representations effectively cluster unseen concepts,\nmaking active learning very effective and substantially reducing the number of\nviable unlabeled examples. We evaluate several active learning and search\ntechniques in this setting on three large-scale datasets: ImageNet, Goodreads\nspoiler detection, and OpenImages. For rare classes, active learning methods\nneed as little as 0.31% of the labeled data to match the average precision of\nfull supervision. By limiting active learning methods to only consider the\nimmediate neighbors of the labeled data as candidates for labeling, we need\nonly process as little as 1% of the unlabeled data while achieving similar\nreductions in labeling costs as the traditional global approach. This process\nof expanding the candidate pool with the nearest neighbors of the labeled set\ncan be done efficiently and reduces the computational complexity of selection\nby orders of magnitude.",
        "date": "2020-06-30"
    },
    "https://arxiv.org/abs/2301.04709": {
        "extra-tags": [],
        "tags": [
            "confiance ai",
            "arxiv doc",
            "explainable ai",
            "lime",
            "nlp stanford",
            "explainable nlp"
        ],
        "title": "[2301.04709] Causal Abstraction for Faithful Model Interpretation",
        "summary": "A faithful and interpretable explanation of an AI model's behavior and\ninternal structure is a high-level explanation that is human-intelligible but\nalso consistent with the known, but often opaque low-level causal details of\nthe model. We argue that the theory of causal abstraction provides the\nmathematical foundations for the desired kinds of model explanations. In causal\nabstraction analysis, we use interventions on model-internal states to\nrigorously assess whether an interpretable high-level causal model is a\nfaithful description of an AI model. Our contributions in this area are: (1) We\ngeneralize causal abstraction to cyclic causal structures and typed high-level\nvariables. (2) We show how multi-source interchange interventions can be used\nto conduct causal abstraction analyses. (3) We define a notion of approximate\ncausal abstraction that allows us to assess the degree to which a high-level\ncausal model is a causal abstraction of a lower-level one. (4) We prove\nconstructive causal abstraction can be decomposed into three operations we\nrefer to as marginalization, variable-merge, and value-merge. (5) We formalize\nthe XAI methods of LIME, causal effect estimation, causal mediation analysis,\niterated nullspace projection, and circuit-based explanations as special cases\nof causal abstraction analysis.",
        "date": "2023-01-11"
    },
    "https://arxiv.org/abs/2205.03983": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp low resource scenarios",
            "machine translation",
            "low resource languages"
        ],
        "title": "[2205.03983] Building Machine Translation Systems for the Next Thousand Languages",
        "summary": "In this paper we share findings from our effort to build practical machine\ntranslation (MT) systems capable of translating across over one thousand\nlanguages. We describe results in three research domains: (i) Building clean,\nweb-mined datasets for 1500+ languages by leveraging semi-supervised\npre-training for language identification and developing data-driven filtering\ntechniques; (ii) Developing practical MT models for under-served languages by\nleveraging massively multilingual models trained with supervised parallel data\nfor over 100 high-resource languages and monolingual datasets for an additional\n1000+ languages; and (iii) Studying the limitations of evaluation metrics for\nthese languages and conducting qualitative analysis of the outputs from our MT\nmodels, highlighting several frequent error modes of these types of models. We\nhope that our work provides useful insights to practitioners working towards\nbuilding MT systems for currently understudied languages, and highlights\nresearch directions that can complement the weaknesses of massively\nmultilingual models in data-sparse settings.",
        "date": "2022-05-09"
    },
    "https://arxiv.org/abs/2106.13474": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "pre trained language models",
            "lm adaptation to domain",
            "nlp microsoft",
            "domain adaptation new vocab",
            "domain adaptation nlp",
            "knowledge distillation",
            "good"
        ],
        "title": "[2106.13474] Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains",
        "summary": "Large pre-trained models have achieved great success in many natural language\nprocessing tasks. However, when they are applied in specific domains, these\nmodels suffer from domain shift and bring challenges in fine-tuning and online\nserving for latency and capacity constraints. In this paper, we present a\ngeneral approach to developing small, fast and effective pre-trained models for\nspecific domains. This is achieved by adapting the off-the-shelf general\npre-trained models and performing task-agnostic knowledge distillation in\ntarget domains. Specifically, we propose domain-specific vocabulary expansion\nin the adaptation stage and employ corpus level occurrence probability to\nchoose the size of incremental vocabulary automatically. Then we systematically\nexplore different strategies to compress the large pre-trained models for\nspecific domains. We conduct our experiments in the biomedical and computer\nscience domain. The experimental results demonstrate that our approach achieves\nbetter performance over the BERT BASE model in domain-specific tasks while 3.3x\nsmaller and 5.1x faster than BERT BASE. The code and pre-trained models are\navailable at https://aka.ms/adalm.",
        "date": "2021-06-25"
    },
    "https://arxiv.org/abs/2010.01057": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "emnlp 2020",
            "luke",
            "text aware kg embedding",
            "entity embeddings",
            "bert kb",
            "self attention",
            "ikuya yamada"
        ],
        "title": "[2010.01057] LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention",
        "summary": "Entity representations are useful in natural language tasks involving\nentities. In this paper, we propose new pretrained contextualized\nrepresentations of words and entities based on the bidirectional transformer.\nThe proposed model treats words and entities in a given text as independent\ntokens, and outputs contextualized representations of them. Our model is\ntrained using a new pretraining task based on the masked language model of\nBERT. The task involves predicting randomly masked words and entities in a\nlarge entity-annotated corpus retrieved from Wikipedia. We also propose an\nentity-aware self-attention mechanism that is an extension of the\nself-attention mechanism of the transformer, and considers the types of tokens\n(words or entities) when computing attention scores. The proposed model\nachieves impressive empirical performance on a wide range of entity-related\ntasks. In particular, it obtains state-of-the-art results on five well-known\ndatasets: Open Entity (entity typing), TACRED (relation classification),\nCoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering),\nand SQuAD 1.1 (extractive question answering). Our source code and pretrained\nrepresentations are available at https://github.com/studio-ousia/luke.",
        "date": "2020-10-02"
    },
    "https://arxiv.org/abs/1804.03235": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "critical evaluation",
            "geoffrey hinton",
            "knowledge distillation",
            "ml google"
        ],
        "title": "[1804.03235] Large scale distributed neural network training through online distillation",
        "summary": "Techniques such as ensembling and distillation promise model quality\nimprovements when paired with almost any base model. However, due to increased\ntest-time cost (for ensembles) and increased complexity of the training\npipeline (for distillation), these techniques are challenging to use in\nindustrial settings. In this paper we explore a variant of distillation which\nis relatively straightforward to use as it does not require a complicated\nmulti-stage setup or many new hyperparameters. Our first claim is that online\ndistillation enables us to use extra parallelism to fit very large datasets\nabout twice as fast. Crucially, we can still speed up training even after we\nhave already reached the point at which additional parallelism provides no\nbenefit for synchronous or asynchronous stochastic gradient descent. Two neural\nnetworks trained on disjoint subsets of the data can share knowledge by\nencouraging each model to agree with the predictions the other model would have\nmade. These predictions can come from a stale version of the other model so\nthey can be safely computed using weights that only rarely get transmitted. Our\nsecond claim is that online distillation is a cost-effective way to make the\nexact predictions of a model dramatically more reproducible. We support our\nclaims using experiments on the Criteo Display Ad Challenge dataset, ImageNet,\nand the largest to-date dataset used for neural language modeling, containing\n$6\\times 10^{11}$ tokens and based on the Common Crawl repository of web data.",
        "date": "2018-04-09"
    },
    "https://arxiv.org/abs/2203.13088": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "colbert"
        ],
        "title": "[2203.13088] Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized Late Interactions using Enhanced Reduction",
        "summary": "Recent progress in neural information retrieval has demonstrated large gains\nin effectiveness, while often sacrificing the efficiency and interpretability\nof the neural model compared to classical approaches. This paper proposes\nColBERTer, a neural retrieval model using contextualized late interaction\n(ColBERT) with enhanced reduction. Along the effectiveness Pareto frontier,\nColBERTer's reductions dramatically lower ColBERT's storage requirements while\nsimultaneously improving the interpretability of its token-matching scores. To\nthis end, ColBERTer fuses single-vector retrieval, multi-vector refinement, and\noptional lexical matching components into one model. For its multi-vector\ncomponent, ColBERTer reduces the number of stored vectors per document by\nlearning unique whole-word representations for the terms in each document and\nlearning to identify and remove word representations that are not essential to\neffective scoring. We employ an explicit multi-task, multi-stage training to\nfacilitate using very small vector dimensions. Results on the MS MARCO and\nTREC-DL collection show that ColBERTer can reduce the storage footprint by up\nto 2.5x, while maintaining effectiveness. With just one dimension per token in\nits smallest setting, ColBERTer achieves index storage parity with the\nplaintext size, with very strong effectiveness results. Finally, we demonstrate\nColBERTer's robustness on seven high-quality out-of-domain collections,\nyielding statistically significant gains over traditional retrieval baselines.",
        "date": "2022-03-24"
    },
    "https://arxiv.org/abs/2011.02260": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "graph neural networks",
            "survey",
            "recommender systems"
        ],
        "title": "[2011.02260] Graph Neural Networks in Recommender Systems: A Survey",
        "summary": "With the explosive growth of online information, recommender systems play a\nkey role to alleviate such information overload. Due to the important\napplication value of recommender system, there have always been emerging works\nin this field. In recent years, graph neural network (GNN) techniques have\ngained considerable interests which can naturally integrate node information\nand topological structure. Owing to the outperformance of GNN in learning on\ngraph data, GNN methods have been widely applied in many fields. In recommender\nsystems, the main challenge is to learn the efficient user/item embeddings from\ntheir interactions and side information if available. Since most of the\ninformation essentially has graph structure and GNNs have superiority in\nrepresentation learning, the field of utilizing graph neural network in\nrecommender systems is flourishing. This article aims to provide a\ncomprehensive review of recent research efforts on graph neural network based\nrecommender systems. Specifically, we provide a taxonomy of graph neural\nnetwork based recommendation models and state new perspectives pertaining to\nthe development of this field.",
        "date": "2020-11-04"
    },
    "https://arxiv.org/abs/2106.04647": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "language models size",
            "sebastian ruder",
            "nlp google",
            "language model fine tuning"
        ],
        "title": "[2106.04647] Compacter: Efficient Low-Rank Hypercomplex Adapter Layers",
        "summary": "Adapting large-scale pretrained language models to downstream tasks via\nfine-tuning is the standard method for achieving state-of-the-art performance\non NLP benchmarks. However, fine-tuning all weights of models with millions or\nbillions of parameters is sample-inefficient, unstable in low-resource\nsettings, and wasteful as it requires storing a separate copy of the model for\neach task. Recent work has developed parameter-efficient fine-tuning methods,\nbut these approaches either still require a relatively large number of\nparameters or underperform standard fine-tuning. In this work, we propose\nCompacter, a method for fine-tuning large-scale language models with a better\ntrade-off between task performance and the number of trainable parameters than\nprior work. Compacter accomplishes this by building on top of ideas from\nadapters, low-rank optimization, and parameterized hypercomplex multiplication\nlayers.\nSpecifically, Compacter inserts task-specific weight matrices into a\npretrained model's weights, which are computed efficiently as a sum of\nKronecker products between shared ``slow'' weights and ``fast'' rank-one\nmatrices defined per Compacter layer. By only training 0.047% of a pretrained\nmodel's parameters, Compacter performs on par with standard fine-tuning on GLUE\nand outperforms fine-tuning in low-resource settings. Our code is publicly\navailable in https://github.com/rabeehk/compacter/",
        "date": "2021-06-08"
    },
    "https://arxiv.org/abs/2011.05864": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "bert and sentence embeddings",
            "emnlp 2020",
            "anisotropy in lm space"
        ],
        "title": "[2011.05864] On the Sentence Embeddings from Pre-trained Language Models",
        "summary": "Pre-trained contextual representations like BERT have achieved great success\nin natural language processing. However, the sentence embeddings from the\npre-trained language models without fine-tuning have been found to poorly\ncapture semantic meaning of sentences. In this paper, we argue that the\nsemantic information in the BERT embeddings is not fully exploited. We first\nreveal the theoretical connection between the masked language model\npre-training objective and the semantic similarity task theoretically, and then\nanalyze the BERT sentence embeddings empirically. We find that BERT always\ninduces a non-smooth anisotropic semantic space of sentences, which harms its\nperformance of semantic similarity. To address this issue, we propose to\ntransform the anisotropic sentence embedding distribution to a smooth and\nisotropic Gaussian distribution through normalizing flows that are learned with\nan unsupervised objective. Experimental results show that our proposed\nBERT-flow method obtains significant performance gains over the\nstate-of-the-art sentence embeddings on a variety of semantic textual\nsimilarity tasks. The code is available at\nhttps://github.com/bohanli/BERT-flow.",
        "date": "2020-11-02"
    },
    "https://arxiv.org/abs/2009.02835": {
        "extra-tags": [],
        "tags": [
            "domain specific bert",
            "arxiv doc",
            "aspect detection",
            "bert kb",
            "e commerce data"
        ],
        "title": "[2009.02835] E-BERT: A Phrase and Product Knowledge Enhanced Language Model for E-commerce",
        "summary": "Pre-trained language models such as BERT have achieved great success in a\nbroad range of natural language processing tasks. However, BERT cannot well\nsupport E-commerce related tasks due to the lack of two levels of domain\nknowledge, i.e., phrase-level and product-level. On one hand, many E-commerce\ntasks require an accurate understanding of domain phrases, whereas such\nfine-grained phrase-level knowledge is not explicitly modeled by BERT's\ntraining objective. On the other hand, product-level knowledge like product\nassociations can enhance the language modeling of E-commerce, but they are not\nfactual knowledge thus using them indiscriminately may introduce noise. To\ntackle the problem, we propose a unified pre-training framework, namely,\nE-BERT. Specifically, to preserve phrase-level knowledge, we introduce Adaptive\nHybrid Masking, which allows the model to adaptively switch from learning\npreliminary word knowledge to learning complex phrases, based on the fitting\nprogress of two modes. To utilize product-level knowledge, we introduce\nNeighbor Product Reconstruction, which trains E-BERT to predict a product's\nassociated neighbors with a denoising cross attention layer. Our investigation\nreveals promising results in four downstream tasks, i.e., review-based question\nanswering, aspect extraction, aspect sentiment classification, and product\nclassification.",
        "date": "2020-09-07"
    },
    "https://arxiv.org/abs/1911.02685": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "transfer learning"
        ],
        "title": "[1911.02685] A Comprehensive Survey on Transfer Learning",
        "summary": "Transfer learning aims at improving the performance of target learners on\ntarget domains by transferring the knowledge contained in different but related\nsource domains. In this way, the dependence on a large number of target domain\ndata can be reduced for constructing target learners. Due to the wide\napplication prospects, transfer learning has become a popular and promising\narea in machine learning. Although there are already some valuable and\nimpressive surveys on transfer learning, these surveys introduce approaches in\na relatively isolated way and lack the recent advances in transfer learning.\nDue to the rapid expansion of the transfer learning area, it is both necessary\nand challenging to comprehensively review the relevant studies. This survey\nattempts to connect and systematize the existing transfer learning researches,\nas well as to summarize and interpret the mechanisms and the strategies of\ntransfer learning in a comprehensive way, which may help readers have a better\nunderstanding of the current research status and ideas. Unlike previous\nsurveys, this survey paper reviews more than forty representative transfer\nlearning approaches, especially homogeneous transfer learning approaches, from\nthe perspectives of data and model. The applications of transfer learning are\nalso briefly introduced. In order to show the performance of different transfer\nlearning models, over twenty representative transfer learning models are used\nfor experiments. The models are performed on three different datasets, i.e.,\nAmazon Reviews, Reuters-21578, and Office-31. And the experimental results\ndemonstrate the importance of selecting appropriate transfer learning models\nfor different applications in practice.",
        "date": "2019-11-07"
    },
    "https://arxiv.org/abs/1905.10070": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "good",
            "text multi label classification",
            "nlp 4 semanlink",
            "extreme multi label classification",
            "classification relations between classes"
        ],
        "title": "[1905.10070] Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification",
        "summary": "Extreme multi-label text classification (XMTC) aims at tagging a document\nwith most relevant labels from an extremely large-scale label set. It is a\nchallenging problem especially for the tail labels because there are only few\ntraining documents to build classifier. This paper is motivated to better\nexplore the semantic relationship between each document and extreme labels by\ntaking advantage of both document content and label correlation. Our objective\nis to establish an explicit label-aware representation for each document with a\nhybrid attention deep neural network model(LAHA). LAHA consists of three parts.\nThe first part adopts a multi-label self-attention mechanism to detect the\ncontribution of each word to labels. The second part exploits the label\nstructure and document content to determine the semantic connection between\nwords and labels in a same latent space. An adaptive fusion strategy is\ndesigned in the third part to obtain the final label-aware document\nrepresentation so that the essence of previous two parts can be sufficiently\nintegrated. Extensive experiments have been conducted on six benchmark datasets\nby comparing with the state-of-the-art methods. The results show the\nsuperiority of our proposed LAHA method, especially on the tail labels.",
        "date": "2019-05-24"
    },
    "https://arxiv.org/abs/1912.03927": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "lenka zdeborova",
            "active learning"
        ],
        "title": "[1912.03927] Large deviations for the perceptron model and consequences for active learning",
        "summary": "Active learning is a branch of machine learning that deals with problems\nwhere unlabeled data is abundant yet obtaining labels is expensive. The\nlearning algorithm has the possibility of querying a limited number of samples\nto obtain the corresponding labels, subsequently used for supervised learning.\nIn this work, we consider the task of choosing the subset of samples to be\nlabeled from a fixed finite pool of samples. We assume the pool of samples to\nbe a random matrix and the ground truth labels to be generated by a\nsingle-layer teacher random neural network. We employ replica methods to\nanalyze the large deviations for the accuracy achieved after supervised\nlearning on a subset of the original pool. These large deviations then provide\noptimal achievable performance boundaries for any active learning algorithm. We\nshow that the optimal learning performance can be efficiently approached by\nsimple message-passing active learning algorithms. We also provide a comparison\nwith the performance of some other popular active learning strategies.",
        "date": "2019-12-09"
    },
    "https://arxiv.org/abs/1902.10197": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "knowledge graph completion",
            "rotate",
            "knowledge graph embeddings",
            "kd mkb biblio"
        ],
        "title": "[1902.10197] RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space",
        "summary": "We study the problem of learning representations of entities and relations in\nknowledge graphs for predicting missing links. The success of such a task\nheavily relies on the ability of modeling and inferring the patterns of (or\nbetween) the relations. In this paper, we present a new approach for knowledge\ngraph embedding called RotatE, which is able to model and infer various\nrelation patterns including: symmetry/antisymmetry, inversion, and composition.\nSpecifically, the RotatE model defines each relation as a rotation from the\nsource entity to the target entity in the complex vector space. In addition, we\npropose a novel self-adversarial negative sampling technique for efficiently\nand effectively training the RotatE model. Experimental results on multiple\nbenchmark knowledge graphs show that the proposed RotatE model is not only\nscalable, but also able to infer and model various relation patterns and\nsignificantly outperform existing state-of-the-art models for link prediction.",
        "date": "2019-02-26"
    },
    "https://arxiv.org/abs/2011.06993": {
        "extra-tags": [],
        "tags": [
            "named entity recognition",
            "flair",
            "attention is all you need",
            "arxiv doc"
        ],
        "title": "[2011.06993] FLERT: Document-Level Features for Named Entity Recognition",
        "summary": "Current state-of-the-art approaches for named entity recognition (NER) using\nBERT-style transformers typically use one of two different approaches: (1) The\nfirst fine-tunes the transformer itself on the NER task and adds only a simple\nlinear layer for word-level predictions. (2) The second uses the transformer\nonly to provide features to a standard LSTM-CRF sequence labeling architecture\nand thus performs no fine-tuning. In this paper, we perform a comparative\nanalysis of both approaches in a variety of settings currently considered in\nthe literature. In particular, we evaluate how well they work when\ndocument-level features are leveraged. Our evaluation on the classic CoNLL\nbenchmark datasets for 4 languages shows that document-level features\nsignificantly improve NER quality and that fine-tuning generally outperforms\nthe feature-based approaches. We present recommendations for parameters as well\nas several new state-of-the-art numbers. Our approach is integrated into the\nFlair framework to facilitate reproduction of our experiments.",
        "date": "2020-11-13"
    },
    "https://arxiv.org/abs/1807.00082": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "neuroscience and ai",
            "ai stanford",
            "consciousness prior",
            "global workspace theory",
            "human ai collaboration",
            "computational neuroscience",
            "personal assistant",
            "ml google",
            "connectionist vs symbolic debate",
            "artificial human intelligence",
            "nn symbolic ai hybridation",
            "good"
        ],
        "title": "[1807.00082] Amanuensis: The Programmer's Apprentice",
        "summary": "This document provides an overview of the material covered in a course taught\nat Stanford in the spring quarter of 2018. The course draws upon insight from\ncognitive and systems neuroscience to implement hybrid connectionist and\nsymbolic reasoning systems that leverage and extend the state of the art in\nmachine learning by integrating human and machine intelligence. As a concrete\nexample we focus on digital assistants that learn from continuous dialog with\nan expert software engineer while providing initial value as powerful\nanalytical, computational and mathematical savants. Over time these savants\nlearn cognitive strategies (domain-relevant problem solving skills) and develop\nintuitions (heuristics and the experience necessary for applying them) by\nlearning from their expert associates. By doing so these savants elevate their\ninnate analytical skills allowing them to partner on an equal footing as\nversatile collaborators - effectively serving as cognitive extensions and\ndigital prostheses, thereby amplifying and emulating their human partner's\nconceptually-flexible thinking patterns and enabling improved access to and\ncontrol over powerful computing resources.",
        "date": "2018-06-29"
    },
    "https://arxiv.org/abs/2004.14843": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "knowledge graph embeddings",
            "explainable ai",
            "good"
        ],
        "title": "[2004.14843] Knowledge Graph Embeddings and Explainable AI",
        "summary": "Knowledge graph embeddings are now a widely adopted approach to knowledge\nrepresentation in which entities and relationships are embedded in vector\nspaces. In this chapter, we introduce the reader to the concept of knowledge\ngraph embeddings by explaining what they are, how they can be generated and how\nthey can be evaluated. We summarize the state-of-the-art in this field by\ndescribing the approaches that have been introduced to represent knowledge in\nthe vector space. In relation to knowledge representation, we consider the\nproblem of explainability, and discuss models and methods for explaining\npredictions obtained via knowledge graph embeddings.",
        "date": "2020-04-30"
    },
    "https://arxiv.org/abs/2210.07316": {
        "extra-tags": [],
        "tags": [
            "mteb",
            "text embeddings",
            "nils reimers",
            "arxiv doc"
        ],
        "title": "[2210.07316] MTEB: Massive Text Embedding Benchmark",
        "summary": "Text embeddings are commonly evaluated on a small set of datasets from a\nsingle task not covering their possible applications to other tasks. It is\nunclear whether state-of-the-art embeddings on semantic textual similarity\n(STS) can be equally well applied to other tasks like clustering or reranking.\nThis makes progress in the field difficult to track, as various models are\nconstantly being proposed without proper evaluation. To solve this problem, we\nintroduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding\ntasks covering a total of 56 datasets and 112 languages. Through the\nbenchmarking of 33 models on MTEB, we establish the most comprehensive\nbenchmark of text embeddings to date. We find that no particular text embedding\nmethod dominates across all tasks. This suggests that the field has yet to\nconverge on a universal text embedding method and scale it up sufficiently to\nprovide state-of-the-art results on all embedding tasks. MTEB comes with\nopen-source code and a public leaderboard at\nhttps://huggingface.co/spaces/mteb/leaderboard.",
        "date": "2022-10-13"
    },
    "https://arxiv.org/abs/2010.12309": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "low resource languages",
            "nlp low resource scenarios",
            "bosch"
        ],
        "title": "[2010.12309] A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios",
        "summary": "Deep neural networks and huge language models are becoming omnipresent in\nnatural language applications. As they are known for requiring large amounts of\ntraining data, there is a growing body of work to improve the performance in\nlow-resource settings. Motivated by the recent fundamental changes towards\nneural models and the popular pre-train and fine-tune paradigm, we survey\npromising approaches for low-resource natural language processing. After a\ndiscussion about the different dimensions of data availability, we give a\nstructured overview of methods that enable learning when training data is\nsparse. This includes mechanisms to create additional labeled data like data\naugmentation and distant supervision as well as transfer learning settings that\nreduce the need for target supervision. A goal of our survey is to explain how\nthese methods differ in their requirements as understanding them is essential\nfor choosing a technique suited for a specific low-resource setting. Further\nkey aspects of this work are to highlight open issues and to outline promising\ndirections for future research.",
        "date": "2020-10-23"
    },
    "https://arxiv.org/abs/1909.01259": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "wikipedia2vec",
            "nlp text classification",
            "ikuya yamada",
            "attention is all you need",
            "entities",
            "entity salience",
            "good"
        ],
        "title": "[1909.01259] Neural Attentive Bag-of-Entities Model for Text Classification",
        "summary": "This study proposes a Neural Attentive Bag-of-Entities model, which is a\nneural network model that performs text classification using entities in a\nknowledge base. Entities provide unambiguous and relevant semantic signals that\nare beneficial for capturing semantics in texts. We combine simple high-recall\nentity detection based on a dictionary, to detect entities in a document, with\na novel neural attention mechanism that enables the model to focus on a small\nnumber of unambiguous and relevant entities. We tested the effectiveness of our\nmodel using two standard text classification datasets (i.e., the 20 Newsgroups\nand R8 datasets) and a popular factoid question answering dataset based on a\ntrivia quiz game. As a result, our model achieved state-of-the-art results on\nall datasets. The source code of the proposed model is available online at\nhttps://github.com/wikipedia2vec/wikipedia2vec.",
        "date": "2019-09-03"
    },
    "https://arxiv.org/abs/2101.12294": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "pre trained language models",
            "kg and nlp",
            "nlp mit",
            "structured knowledge"
        ],
        "title": "[2101.12294] Combining pre-trained language models and structured knowledge",
        "summary": "In recent years, transformer-based language models have achieved state of the\nart performance in various NLP benchmarks. These models are able to extract\nmostly distributional information with some semantics from unstructured text,\nhowever it has proven challenging to integrate structured information, such as\nknowledge graphs into these models. We examine a variety of approaches to\nintegrate structured knowledge into current language models and determine\nchallenges, and possible opportunities to leverage both structured and\nunstructured information sources. From our survey, we find that there are still\nopportunities at exploiting adapter-based injections and that it may be\npossible to further combine various of the explored approaches into one system.",
        "date": "2021-01-28"
    },
    "https://arxiv.org/abs/1410.5859": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "first order logic",
            "knowledge graph embeddings",
            "guha",
            "nn symbolic ai hybridation"
        ],
        "title": "[1410.5859] Towards a Model Theory for Distributed Representations",
        "summary": "Distributed representations (such as those based on embeddings) and discrete\nrepresentations (such as those based on logic) have complementary strengths. We\nexplore one possible approach to combining these two kinds of representations.\nWe present a model theory/semantics for first order logic based on vectors of\nreals. We describe the model theory, discuss some interesting properties of\nsuch a system and present a simple approach to query answering.",
        "date": "2014-10-21"
    },
    "https://arxiv.org/abs/2004.07202": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "question answering",
            "knowledge graph augmented language models",
            "not encoding knowledge in language model",
            "attention is all you need",
            "memory networks",
            "nlp google"
        ],
        "title": "[2004.07202] Entities as Experts: Sparse Memory Access with Entity Supervision",
        "summary": "We focus on the problem of capturing declarative knowledge in the learned\nparameters of a language model. We introduce a new model, Entities as Experts\n(EaE), that can access distinct memories of the entities mentioned in a piece\nof text. Unlike previous efforts to integrate entity knowledge into sequence\nmodels, EaE's entity representations are learned directly from text. These\nrepresentations capture sufficient knowledge to answer TriviaQA questions such\nas \"Which Dr. Who villain has been played by Roger Delgado, Anthony Ainley,\nEric Roberts?\". EaE outperforms a Transformer model with $30\\times$ the\nparameters on this task. According to the Lama knowledge probes, EaE also\ncontains more factual knowledge than a similar sized Bert. We show that\nassociating parameters with specific entities means that EaE only needs to\naccess a fraction of its parameters at inference time, and we show that the\ncorrect identification, and representation, of entities is essential to EaE's\nperformance. We also argue that the discrete and independent entity\nrepresentations in EaE make it more modular and interpretable than the\nTransformer architecture on which it is based.",
        "date": "2020-04-15"
    },
    "https://arxiv.org/abs/1909.01380": {
        "extra-tags": [],
        "tags": [
            "arxiv doc"
        ],
        "title": "[1909.01380] The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives",
        "summary": "We seek to understand how the representations of individual tokens and the\nstructure of the learned feature space evolve between layers in deep neural\nnetworks under different learning objectives. We focus on the Transformers for\nour analysis as they have been shown effective on various tasks, including\nmachine translation (MT), standard left-to-right language models (LM) and\nmasked language modeling (MLM). Previous work used black-box probing tasks to\nshow that the representations learned by the Transformer differ significantly\ndepending on the objective. In this work, we use canonical correlation analysis\nand mutual information estimators to study how information flows across\nTransformer layers and how this process depends on the choice of learning\nobjective. For example, as you go from bottom to top layers, information about\nthe past in left-to-right language models gets vanished and predictions about\nthe future get formed. In contrast, for MLM, representations initially acquire\ninformation about the context around the token, partially forgetting the token\nidentity and producing a more generalized token representation. The token\nidentity then gets recreated at the top MLM layers.",
        "date": "2019-09-03"
    },
    "https://arxiv.org/abs/2002.12327": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "bertology",
            "bert"
        ],
        "title": "[2002.12327] A Primer in BERTology: What we know about how BERT works",
        "summary": "Transformer-based models are now widely used in NLP, but we still do not\nunderstand a lot about their inner workings. This paper describes what is known\nto date about the famous BERT model (Devlin et al. 2019), synthesizing over 40\nanalysis studies. We also provide an overview of the proposed modifications to\nthe model and its training regime. We then outline the directions for further\nresearch.",
        "date": "2020-02-27"
    },
    "https://arxiv.org/abs/2012.02558": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "automobile",
            "porsche",
            "language models as knowledge bases"
        ],
        "title": "[2012.02558] Pre-trained language models as knowledge bases for Automotive Complaint Analysis",
        "summary": "Recently it has been shown that large pre-trained language models like BERT\n(Devlin et al., 2018) are able to store commonsense factual knowledge captured\nin its pre-training corpus (Petroni et al., 2019). In our work we further\nevaluate this ability with respect to an application from industry creating a\nset of probes specifically designed to reveal technical quality issues captured\nas described incidents out of unstructured customer feedback in the automotive\nindustry. After probing the out-of-the-box versions of the pre-trained models\nwith fill-in-the-mask tasks we dynamically provide it with more knowledge via\ncontinual pre-training on the Office of Defects Investigation (ODI) Complaints\ndata set. In our experiments the models exhibit performance regarding queries\non domain-specific topics compared to when queried on factual knowledge itself,\nas Petroni et al. (2019) have done. For most of the evaluated architectures the\ncorrect token is predicted with a $Precision@1$ ($P@1$) of above 60\\%, while\nfor $P@5$ and $P@10$ even values of well above 80\\% and up to 90\\% respectively\nare reached. These results show the potential of using language models as a\nknowledge base for structured analysis of customer feedback.",
        "date": "2020-12-04"
    },
    "https://arxiv.org/abs/1711.00046": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "regex",
            "aho corasick algorithm",
            "string searching algorithm",
            "flashtext algorithm"
        ],
        "title": "[1711.00046] Replace or Retrieve Keywords In Documents at Scale",
        "summary": "In this paper we introduce, the FlashText algorithm for replacing keywords or\nfinding keywords in a given text. FlashText can search or replace keywords in\none pass over a document. The time complexity of this algorithm is not\ndependent on the number of terms being searched or replaced. For a document of\nsize N (characters) and a dictionary of M keywords, the time complexity will be\nO(N). This algorithm is much faster than Regex, because regex time complexity\nis O(MxN). It is also different from Aho Corasick Algorithm, as it doesn't\nmatch substrings. FlashText is designed to only match complete words (words\nwith boundary characters on both sides). For an input dictionary of {Apple},\nthis algorithm won't match it to 'I like Pineapple'. This algorithm is also\ndesigned to go for the longest match first. For an input dictionary {Machine,\nLearning, Machine learning} on a string 'I like Machine learning', it will only\nconsider the longest match, which is Machine Learning. We have made python\nimplementation of this algorithm available as open-source on GitHub, released\nunder the permissive MIT License.",
        "date": "2017-10-31"
    },
    "https://arxiv.org/abs/2003.03384": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "backpropagation vs biology",
            "automl",
            "evolutionary algorithm",
            "quoc le"
        ],
        "title": "[2003.03384] AutoML-Zero: Evolving Machine Learning Algorithms From Scratch",
        "summary": "Machine learning research has advanced in multiple aspects, including model\nstructures and learning methods. The effort to automate such research, known as\nAutoML, has also made significant progress. However, this progress has largely\nfocused on the architecture of neural networks, where it has relied on\nsophisticated expert-designed layers as building blocks---or similarly\nrestrictive search spaces. Our goal is to show that AutoML can go further: it\nis possible today to automatically discover complete machine learning\nalgorithms just using basic mathematical operations as building blocks. We\ndemonstrate this by introducing a novel framework that significantly reduces\nhuman bias through a generic search space. Despite the vastness of this space,\nevolutionary search can still discover two-layer neural networks trained by\nbackpropagation. These simple neural networks can then be surpassed by evolving\ndirectly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques\nemerge in the top algorithms, such as bilinear interactions, normalized\ngradients, and weight averaging. Moreover, evolution adapts algorithms to\ndifferent task types: e.g., dropout-like techniques appear when little data is\navailable. We believe these preliminary successes in discovering machine\nlearning algorithms from scratch indicate a promising new direction for the\nfield.",
        "date": "2020-03-06"
    },
    "https://arxiv.org/abs/2207.06300": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "question answering",
            "michael glass",
            "retriever reader",
            "retrieval augmented generation",
            "realm",
            "zero shot",
            "discute avec raphael",
            "knowledge distillation",
            "slot tagging"
        ],
        "title": "[2207.06300] Re2G: Retrieve, Rerank, Generate",
        "summary": "As demonstrated by GPT-3 and T5, transformers grow in capability as parameter\nspaces become larger and larger. However, for tasks that require a large amount\nof knowledge, non-parametric memory allows models to grow dramatically with a\nsub-linear increase in computational cost and GPU memory requirements. Recent\nmodels such as RAG and REALM have introduced retrieval into conditional\ngeneration. These models incorporate neural initial retrieval from a corpus of\npassages. We build on this line of research, proposing Re2G, which combines\nboth neural initial retrieval and reranking into a BART-based\nsequence-to-sequence generation. Our reranking approach also permits merging\nretrieval results from sources with incomparable scores, enabling an ensemble\nof BM25 and neural initial retrieval. To train our system end-to-end, we\nintroduce a novel variation of knowledge distillation to train the initial\nretrieval, reranker, and generation using only ground truth on the target\nsequence output. We find large gains in four diverse tasks: zero-shot slot\nfilling, question answering, fact-checking, and dialog, with relative gains of\n9% to 34% over the previous state-of-the-art on the KILT leaderboard. We make\nour code available as open source at\nhttps://github.com/IBM/kgi-slot-filling/tree/re2g.",
        "date": "2022-07-13"
    },
    "https://arxiv.org/abs/1906.03158": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp google",
            "relation learning"
        ],
        "title": "[1906.03158] Matching the Blanks: Distributional Similarity for Relation Learning",
        "summary": "General purpose relation extractors, which can model arbitrary relations, are\na core aspiration in information extraction. Efforts have been made to build\ngeneral purpose extractors that represent relations with their surface forms,\nor which jointly embed surface forms with relations from an existing knowledge\ngraph. However, both of these approaches are limited in their ability to\ngeneralize. In this paper, we build on extensions of Harris' distributional\nhypothesis to relations, as well as recent advances in learning text\nrepresentations (specifically, BERT), to build task agnostic relation\nrepresentations solely from entity-linked text. We show that these\nrepresentations significantly outperform previous work on exemplar based\nrelation extraction (FewRel) even without using any of that task's training\ndata. We also show that models initialized with our task agnostic\nrepresentations, and then tuned on supervised relation extraction datasets,\nsignificantly outperform the previous methods on SemEval 2010 Task 8, KBP37,\nand TACRED.",
        "date": "2019-06-07"
    },
    "https://arxiv.org/abs/2206.06520": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "not encoding knowledge in language model",
            "chris manning",
            "language model"
        ],
        "title": "[2206.06520] Memory-Based Model Editing at Scale",
        "summary": "Even the largest neural networks make errors, and once-correct predictions\ncan become invalid as the world changes. Model editors make local updates to\nthe behavior of base (pre-trained) models to inject updated knowledge or\ncorrect undesirable behaviors. Existing model editors have shown promise, but\nalso suffer from insufficient expressiveness: they struggle to accurately model\nan edit's intended scope (examples affected by the edit), leading to inaccurate\npredictions for test inputs loosely related to the edit, and they often fail\naltogether after many edits. As a higher-capacity alternative, we propose\nSemi-Parametric Editing with a Retrieval-Augmented Counterfactual Model\n(SERAC), which stores edits in an explicit memory and learns to reason over\nthem to modulate the base model's predictions as needed. To enable more\nrigorous evaluation of model editors, we introduce three challenging language\nmodel editing problems based on question answering, fact-checking, and dialogue\ngeneration. We find that only SERAC achieves high performance on all three\nproblems, consistently outperforming existing approaches to model editing by a\nsignificant margin. Code, data, and additional project information will be made\navailable at https://sites.google.com/view/serac-editing.",
        "date": "2022-06-13"
    },
    "https://arxiv.org/abs/1703.07464": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "triplet loss",
            "metric learning",
            "zero shot learning",
            "google research",
            "similarity learning"
        ],
        "title": "[1703.07464] No Fuss Distance Metric Learning using Proxies",
        "summary": "We address the problem of distance metric learning (DML), defined as learning\na distance consistent with a notion of semantic similarity. Traditionally, for\nthis problem supervision is expressed in the form of sets of points that follow\nan ordinal relationship -- an anchor point $x$ is similar to a set of positive\npoints $Y$, and dissimilar to a set of negative points $Z$, and a loss defined\nover these distances is minimized. While the specifics of the optimization\ndiffer, in this work we collectively call this type of supervision Triplets and\nall methods that follow this pattern Triplet-Based methods. These methods are\nchallenging to optimize. A main issue is the need for finding informative\ntriplets, which is usually achieved by a variety of tricks such as increasing\nthe batch size, hard or semi-hard triplet mining, etc. Even with these tricks,\nthe convergence rate of such methods is slow. In this paper we propose to\noptimize the triplet loss on a different space of triplets, consisting of an\nanchor data point and similar and dissimilar proxy points which are learned as\nwell. These proxies approximate the original data points, so that a triplet\nloss over the proxies is a tight upper bound of the original loss. This\nproxy-based loss is empirically better behaved. As a result, the proxy-loss\nimproves on state-of-art results for three standard zero-shot learning\ndatasets, by up to 15% points, while converging three times as fast as other\ntriplet-based losses.",
        "date": "2017-03-21"
    },
    "https://arxiv.org/abs/2003.02320": {
        "extra-tags": [],
        "tags": [
            "axel polleres",
            "knowledge graph",
            "arxiv doc",
            "survey",
            "aidan hogan"
        ],
        "title": "[2003.02320] Knowledge Graphs",
        "summary": "In this paper we provide a comprehensive introduction to knowledge graphs,\nwhich have recently garnered significant attention from both industry and\nacademia in scenarios that require exploiting diverse, dynamic, large-scale\ncollections of data. After a general introduction, we motivate and contrast\nvarious graph-based data models and query languages that are used for knowledge\ngraphs. We discuss the roles of schema, identity, and context in knowledge\ngraphs. We explain how knowledge can be represented and extracted using a\ncombination of deductive and inductive techniques. We summarise methods for the\ncreation, enrichment, quality assessment, refinement, and publication of\nknowledge graphs. We provide an overview of prominent open knowledge graphs and\nenterprise knowledge graphs, their applications, and how they use the\naforementioned techniques. We conclude with high-level future research\ndirections for knowledge graphs.",
        "date": "2020-03-04"
    },
    "https://arxiv.org/abs/2008.09093": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp long documents",
            "neural models for information retrieval"
        ],
        "title": "[2008.09093] PARADE: Passage Representation Aggregation for Document Reranking",
        "summary": "Pretrained transformer models, such as BERT and T5, have shown to be highly\neffective at ad-hoc passage and document ranking. Due to inherent sequence\nlength limits of these models, they need to be run over a document's passages,\nrather than processing the entire document sequence at once. Although several\napproaches for aggregating passage-level signals have been proposed, there has\nyet to be an extensive comparison of these techniques. In this work, we explore\nstrategies for aggregating relevance signals from a document's passages into a\nfinal ranking score. We find that passage representation aggregation techniques\ncan significantly improve over techniques proposed in prior work, such as\ntaking the maximum passage score. We call this new approach PARADE. In\nparticular, PARADE can significantly improve results on collections with broad\ninformation needs where relevance signals can be spread throughout the document\n(such as TREC Robust04 and GOV2). Meanwhile, less complex aggregation\ntechniques may work better on collections with an information need that can\noften be pinpointed to a single passage (such as TREC DL and TREC Genomics). We\nalso conduct efficiency analyses, and highlight several strategies for\nimproving transformer-based aggregation.",
        "date": "2020-08-20"
    },
    "https://arxiv.org/abs/2103.11811": {
        "extra-tags": [],
        "tags": [
            "named entity recognition",
            "arxiv doc",
            "nlp 4 africa",
            "masakhane"
        ],
        "title": "[2103.11811] MasakhaNER: Named Entity Recognition for African Languages",
        "summary": "We take a step towards addressing the under-representation of the African\ncontinent in NLP research by creating the first large publicly available\nhigh-quality dataset for named entity recognition (NER) in ten African\nlanguages, bringing together a variety of stakeholders. We detail\ncharacteristics of the languages to help researchers understand the challenges\nthat these languages pose for NER. We analyze our datasets and conduct an\nextensive empirical evaluation of state-of-the-art methods across both\nsupervised and transfer learning settings. We release the data, code, and\nmodels in order to inspire future research on African NLP.",
        "date": "2021-03-22"
    },
    "https://arxiv.org/abs/2212.02623": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp microsoft",
            "document processing"
        ],
        "title": "[2212.02623] Unifying Vision, Text, and Layout for Universal Document Processing",
        "summary": "We propose Universal Document Processing (UDOP), a foundation Document AI\nmodel which unifies text, image, and layout modalities together with varied\ntask formats, including document understanding and generation. UDOP leverages\nthe spatial correlation between textual content and document image to model\nimage, text, and layout modalities with one uniform representation. With a\nnovel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain\ndownstream tasks into a prompt-based sequence generation scheme. UDOP is\npretrained on both large-scale unlabeled document corpora using innovative\nself-supervised objectives and diverse labeled data. UDOP also learns to\ngenerate document images from text and layout modalities via masked image\nreconstruction. To the best of our knowledge, this is the first time in the\nfield of document AI that one model simultaneously achieves high-quality neural\ndocument editing and content customization. Our method sets the\nstate-of-the-art on 9 Document AI tasks, e.g., document understanding and QA,\nacross diverse data domains like finance reports, academic papers, and\nwebsites. UDOP ranks first on the leaderboard of the Document Understanding\nBenchmark (DUE).",
        "date": "2022-12-05"
    },
    "https://arxiv.org/abs/2006.01969": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "entity linker"
        ],
        "title": "[2006.01969] REL: An Entity Linker Standing on the Shoulders of Giants",
        "summary": "Entity linking is a standard component in modern retrieval system that is\noften performed by third-party toolkits. Despite the plethora of open source\noptions, it is difficult to find a single system that has a modular\narchitecture where certain components may be replaced, does not depend on\nexternal sources, can easily be updated to newer Wikipedia versions, and, most\nimportant of all, has state-of-the-art performance. The REL system presented in\nthis paper aims to fill that gap. Building on state-of-the-art neural\ncomponents from natural language processing research, it is provided as a\nPython package as well as a web API. We also report on an experimental\ncomparison against both well-established systems and the current\nstate-of-the-art on standard entity linking benchmarks.",
        "date": "2020-06-02"
    },
    "https://arxiv.org/abs/1904.01947": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "genetic algorithm",
            "pdf extract",
            "generative adversarial network"
        ],
        "title": "[1904.01947] Extracting Tables from Documents using Conditional Generative Adversarial Networks and Genetic Algorithms",
        "summary": "Extracting information from tables in documents presents a significant\nchallenge in many industries and in academic research. Existing methods which\ntake a bottom-up approach of integrating lines into cells and rows or columns\nneglect the available prior information relating to table structure. Our\nproposed method takes a top-down approach, first using a generative adversarial\nnetwork to map a table image into a standardised `skeleton' table form denoting\nthe approximate row and column borders without table content, then fitting\nrenderings of candidate latent table structures to the skeleton structure using\na distance measure optimised by a genetic algorithm.",
        "date": "2019-04-03"
    },
    "https://arxiv.org/abs/2203.14655": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "sbert",
            "sbert fine tuning",
            "few shot text classification",
            "siamese network"
        ],
        "title": "[2203.14655] Few-Shot Learning with Siamese Networks and Label Tuning",
        "summary": "We study the problem of building text classifiers with little or no training\ndata, commonly known as zero and few-shot text classification. In recent years,\nan approach based on neural textual entailment models has been found to give\nstrong results on a diverse range of tasks. In this work, we show that with\nproper pre-training, Siamese Networks that embed texts and labels offer a\ncompetitive alternative. These models allow for a large reduction in inference\ncost: constant in the number of labels rather than linear. Furthermore, we\nintroduce label tuning, a simple and computationally efficient approach that\nallows to adapt the models in a few-shot setup by only changing the label\nembeddings. While giving lower performance than model fine-tuning, this\napproach has the architectural advantage that a single encoder can be shared by\nmany different tasks.",
        "date": "2022-03-28"
    },
    "https://www.aclweb.org/anthology/D19-1276/": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "information bottleneck method",
            "emnlp 2019",
            "word embedding"
        ],
        "title": "[1910.00163] Specializing Word Embeddings (for Parsing) by Information Bottleneck",
        "summary": "Pre-trained word embeddings like ELMo and BERT contain rich syntactic and\nsemantic information, resulting in state-of-the-art performance on various\ntasks. We propose a very fast variational information bottleneck (VIB) method\nto nonlinearly compress these embeddings, keeping only the information that\nhelps a discriminative parser. We compress each word embedding to either a\ndiscrete tag or a continuous vector. In the discrete version, our automatically\ncompressed tags form an alternative tag set: we show experimentally that our\ntags capture most of the information in traditional POS tag annotations, but\nour tag sequences can be parsed more accurately at the same level of tag\ngranularity. In the continuous version, we show experimentally that moderately\ncompressing the word embeddings by our method yields a more accurate parser in\n8 of 9 languages, unlike simple dimensionality reduction.",
        "date": "2019-10-01"
    },
    "https://arxiv.org/abs/2012.04740": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "max halford",
            "raphaelsty"
        ],
        "title": "[2012.04740] River: machine learning for streaming data in Python",
        "summary": "River is a machine learning library for dynamic data streams and continual\nlearning. It provides multiple state-of-the-art learning methods, data\ngenerators/transformers, performance metrics and evaluators for different\nstream learning problems. It is the result from the merger of the two most\npopular packages for stream learning in Python: Creme and scikit-multiflow.\nRiver introduces a revamped architecture based on the lessons learnt from the\nseminal packages. River's ambition is to be the go-to library for doing machine\nlearning on streaming data. Additionally, this open source package brings under\nthe same umbrella a large community of practitioners and researchers. The\nsource code is available at https://github.com/online-ml/river.",
        "date": "2020-12-08"
    },
    "https://arxiv.org/abs/1904.13001": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "stacking ensemble learning",
            "reseaux bayesiens",
            "categorical variables"
        ],
        "title": "[1904.13001] Encoding Categorical Variables with Conjugate Bayesian Models for WeWork Lead Scoring Engine",
        "summary": "Applied Data Scientists throughout various industries are commonly faced with\nthe challenging task of encoding high-cardinality categorical features into\ndigestible inputs for machine learning algorithms. This paper describes a\nBayesian encoding technique developed for WeWork's lead scoring engine which\noutputs the probability of a person touring one of our office spaces based on\ninteraction, enrichment, and geospatial data. We present a paradigm for\nensemble modeling which mitigates the need to build complicated preprocessing\nand encoding schemes for categorical variables. In particular, domain-specific\nconjugate Bayesian models are employed as base learners for features in a\nstacked ensemble model. For each column of a categorical feature matrix we fit\na problem-specific prior distribution, for example, the Beta distribution for a\nbinary classification problem. In order to analytically derive the moments of\nthe posterior distribution, we update the prior with the conjugate likelihood\nof the corresponding target variable for each unique value of the given\ncategorical feature. This function of column and value encodes the categorical\nfeature matrix so that the final learner in the ensemble model ingests\nlow-dimensional numerical input. Experimental results on both curated and real\nworld datasets demonstrate impressive accuracy and computational efficiency on\na variety of problem archetypes. Particularly, for the lead scoring engine at\nWeWork -- where some categorical features have as many as 300,000 levels -- we\nhave seen an AUC improvement from 0.87 to 0.97 through implementing conjugate\nBayesian model encoding.",
        "date": "2019-04-30"
    },
    "https://arxiv.org/abs/1909.06356": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "emnlp 2019",
            "synthetic qa data",
            "question answering"
        ],
        "title": "[1909.06356] Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering",
        "summary": "Text-based Question Generation (QG) aims at generating natural and relevant\nquestions that can be answered by a given answer in some context. Existing QG\nmodels suffer from a \"semantic drift\" problem, i.e., the semantics of the\nmodel-generated question drifts away from the given context and answer. In this\npaper, we first propose two semantics-enhanced rewards obtained from downstream\nquestion paraphrasing and question answering tasks to regularize the QG model\nto generate semantically valid questions. Second, since the traditional\nevaluation metrics (e.g., BLEU) often fall short in evaluating the quality of\ngenerated questions, we propose a QA-based evaluation method which measures the\nQG model's ability to mimic human annotators in generating QA training data.\nExperiments show that our method achieves the new state-of-the-art performance\nw.r.t. traditional metrics, and also performs best on our QA-based evaluation\nmetrics. Further, we investigate how to use our QG model to augment QA datasets\nand enable semi-supervised QA. We propose two ways to generate synthetic QA\npairs: generate new questions from existing articles or collect QA pairs from\nnew articles. We also propose two empirically effective strategies, a data\nfilter and mixing mini-batch training, to properly use the QG-generated data\nfor QA. Experiments show that our method improves over both BiDAF and BERT QA\nbaselines, even without introducing new articles.",
        "date": "2019-09-13"
    },
    "https://arxiv.org/abs/2203.09435": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "low resource languages",
            "domain adaptation",
            "sebastian ruder",
            "dictionnaire"
        ],
        "title": "[2203.09435] Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation",
        "summary": "The performance of multilingual pretrained models is highly dependent on the\navailability of monolingual or parallel text present in a target language.\nThus, the majority of the world's languages cannot benefit from recent progress\nin NLP as they have no or limited textual data. To expand possibilities of\nusing NLP technology in these under-represented languages, we systematically\nstudy strategies that relax the reliance on conventional language resources\nthrough the use of bilingual lexicons, an alternative resource with much better\nlanguage coverage. We analyze different strategies to synthesize textual or\nlabeled data using lexicons, and how this data can be combined with monolingual\nor parallel text when available. For 19 under-represented languages across 3\ntasks, our methods lead to consistent improvements of up to 5 and 15 points\nwith and without extra monolingual text respectively. Overall, our study\nhighlights how NLP methods can be adapted to thousands more languages that are\nunder-served by current technology",
        "date": "2022-03-17"
    },
    "https://arxiv.org/abs/1805.09906": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "graphs nlp",
            "document embeddings"
        ],
        "title": "[1805.09906] Diffusion Maps for Textual Network Embedding",
        "summary": "Textual network embedding leverages rich text information associated with the\nnetwork to learn low-dimensional vectorial representations of vertices. Rather\nthan using typical natural language processing (NLP) approaches, recent\nresearch exploits the relationship of texts on the same edge to graphically\nembed text. However, these models neglect to measure the complete level of\nconnectivity between any two texts in the graph. We present diffusion maps for\ntextual network embedding (DMTE), integrating global structural information of\nthe graph to capture the semantic relatedness between texts, with a\ndiffusion-convolution operation applied on the text inputs. In addition, a new\nobjective function is designed to efficiently preserve the high-order proximity\nusing the graph diffusion. Experimental results show that the proposed approach\noutperforms state-of-the-art methods on the vertex-classification and\nlink-prediction tasks.",
        "date": "2018-05-24"
    },
    "https://arxiv.org/abs/2203.10581": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "lm adaptation to domain",
            "nlp text classification",
            "cold start problem",
            "nlp ibm",
            "nlp intermediate task training",
            "topical text classification"
        ],
        "title": "[2203.10581] Cluster & Tune: Boost Cold Start Performance in Text Classification",
        "summary": "In real-world scenarios, a text classification task often begins with a cold\nstart, when labeled data is scarce. In such cases, the common practice of\nfine-tuning pre-trained models, such as BERT, for a target classification task,\nis prone to produce poor performance. We suggest a method to boost the\nperformance of such models by adding an intermediate unsupervised\nclassification task, between the pre-training and fine-tuning phases. As such\nan intermediate task, we perform clustering and train the pre-trained model on\npredicting the cluster labels. We test this hypothesis on various data sets,\nand show that this additional classification phase can significantly improve\nperformance, mainly for topical classification tasks, when the number of\nlabeled instances available for fine-tuning is only a couple of dozen to a few\nhundred.",
        "date": "2022-03-20"
    },
    "https://arxiv.org/abs/2205.12410": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "adapter modules finetuning",
            "fine tuning"
        ],
        "title": "[2205.12410] AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning",
        "summary": "Standard fine-tuning of large pre-trained language models (PLMs) for\ndownstream tasks requires updating hundreds of millions to billions of\nparameters, and storing a large copy of the PLM weights for every task\nresulting in increased cost for storing, sharing and serving the models. To\naddress this, parameter-efficient fine-tuning (PEFT) techniques were introduced\nwhere small trainable components are injected in the PLM and updated during\nfine-tuning. We propose AdaMix as a general PEFT method that tunes a mixture of\nadaptation modules -- given the underlying PEFT method of choice -- introduced\nin each Transformer layer while keeping most of the PLM weights frozen. For\ninstance, AdaMix can leverage a mixture of adapters like Houlsby or a mixture\nof low rank decomposition matrices like LoRA to improve downstream task\nperformance over the corresponding PEFT methods for fully supervised and\nfew-shot NLU and NLG tasks. Further, we design AdaMix such that it matches the\nsame computational cost and the number of tunable parameters as the underlying\nPEFT method. By only tuning 0.1-0.2% of PLM parameters, we show that AdaMix\noutperforms SOTA parameter-efficient fine-tuning and full model fine-tuning for\nboth NLU and NLG tasks.",
        "date": "2022-05-24"
    },
    "https://arxiv.org/abs/1902.00751": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "continual learning",
            "transfer learning in nlp",
            "adapter modules finetuning",
            "bert",
            "nlp google"
        ],
        "title": "[1902.00751] Parameter-Efficient Transfer Learning for NLP",
        "summary": "Fine-tuning large pre-trained models is an effective transfer mechanism in\nNLP. However, in the presence of many downstream tasks, fine-tuning is\nparameter inefficient: an entire new model is required for every task. As an\nalternative, we propose transfer with adapter modules. Adapter modules yield a\ncompact and extensible model; they add only a few trainable parameters per\ntask, and new tasks can be added without revisiting previous ones. The\nparameters of the original network remain fixed, yielding a high degree of\nparameter sharing. To demonstrate adapter's effectiveness, we transfer the\nrecently proposed BERT Transformer model to 26 diverse text classification\ntasks, including the GLUE benchmark. Adapters attain near state-of-the-art\nperformance, whilst adding only a few parameters per task. On GLUE, we attain\nwithin 0.4% of the performance of full fine-tuning, adding only 3.6% parameters\nper task. By contrast, fine-tuning trains 100% of the parameters per task.",
        "date": "2019-02-02"
    },
    "https://arxiv.org/abs/2104.11882": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "incremental few shot text classification"
        ],
        "title": "[2104.11882] Incremental Few-shot Text Classification with Multi-round New Classes: Formulation, Dataset and System",
        "summary": "Text classification is usually studied by labeling natural language texts\nwith relevant categories from a predefined set. In the real world, new classes\nmight keep challenging the existing system with limited labeled data. The\nsystem should be intelligent enough to recognize upcoming new classes with a\nfew examples. In this work, we define a new task in the NLP domain, incremental\nfew-shot text classification, where the system incrementally handles multiple\nrounds of new classes. For each round, there is a batch of new classes with a\nfew labeled examples per class. Two major challenges exist in this new task:\n(i) For the learning process, the system should incrementally learn new classes\nround by round without re-training on the examples of preceding classes; (ii)\nFor the performance, the system should perform well on new classes without much\nloss on preceding classes. In addition to formulating the new task, we also\nrelease two benchmark datasets in the incremental few-shot setting: intent\nclassification and relation classification. Moreover, we propose two entailment\napproaches, ENTAILMENT and HYBRID, which show promise for solving this novel\nproblem.",
        "date": "2021-04-24"
    },
    "https://arxiv.org/abs/2002.05867": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "reasoning",
            "attention is all you need",
            "allen institute for ai a2i",
            "knowledge representation",
            "rules"
        ],
        "title": "[2002.05867] Transformers as Soft Reasoners over Language",
        "summary": "AI has long pursued the goal of having systems reason over *explicitly\nprovided* knowledge, but building suitable representations has proved\nchallenging. Here we explore whether transformers can similarly learn to reason\n(or emulate reasoning), but using rules expressed in language, thus bypassing a\nformal representation. We provide the first demonstration that this is\npossible, and characterize the extent of this capability. To do this, we use a\ncollection of synthetic datasets that test increasing levels of reasoning\ncomplexity (number of rules, presence of negation, and depth of chaining). We\nfind transformers appear to learn rule-based reasoning with high (99%) accuracy\non these datasets, and in a way that generalizes to test data requiring\nsubstantially deeper chaining than in the training data (95%+ scores). We also\ndemonstrate that the models transfer well to two hand-authored rulebases, and\nto rulebases paraphrased into more natural language. These findings are\nsignificant as it suggests a new role for transformers, namely as a limited\n\"soft theorem prover\" operating over explicit theories in language. This in\nturn suggests new possibilities for explainability, correctability, and\ncounterfactual reasoning in question-answering. All datasets and a live demo\nare available at http://rule-reasoning.apps.allenai.org/",
        "date": "2020-02-14"
    },
    "https://arxiv.org/abs/2008.09470": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "top2vec"
        ],
        "title": "[2008.09470] Top2Vec: Distributed Representations of Topics",
        "summary": "Topic modeling is used for discovering latent semantic structure, usually\nreferred to as topics, in a large collection of documents. The most widely used\nmethods are Latent Dirichlet Allocation and Probabilistic Latent Semantic\nAnalysis. Despite their popularity they have several weaknesses. In order to\nachieve optimal results they often require the number of topics to be known,\ncustom stop-word lists, stemming, and lemmatization. Additionally these methods\nrely on bag-of-words representation of documents which ignore the ordering and\nsemantics of words. Distributed representations of documents and words have\ngained popularity due to their ability to capture semantics of words and\ndocuments. We present $\\texttt{top2vec}$, which leverages joint document and\nword semantic embedding to find $\\textit{topic vectors}$. This model does not\nrequire stop-word lists, stemming or lemmatization, and it automatically finds\nthe number of topics. The resulting topic vectors are jointly embedded with the\ndocument and word vectors with distance between them representing semantic\nsimilarity. Our experiments demonstrate that $\\texttt{top2vec}$ finds topics\nwhich are significantly more informative and representative of the corpus\ntrained on than probabilistic generative models.",
        "date": "2020-08-19"
    },
    "https://arxiv.org/abs/1908.11860": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "language model fine tuning",
            "aspect target sentiment classification",
            "domain adaptation nlp"
        ],
        "title": "[1908.11860] Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification",
        "summary": "Aspect-Target Sentiment Classification (ATSC) is a subtask of Aspect-Based\nSentiment Analysis (ABSA), which has many applications e.g. in e-commerce,\nwhere data and insights from reviews can be leveraged to create value for\nbusinesses and customers. Recently, deep transfer-learning methods have been\napplied successfully to a myriad of Natural Language Processing (NLP) tasks,\nincluding ATSC. Building on top of the prominent BERT language model, we\napproach ATSC using a two-step procedure: self-supervised domain-specific BERT\nlanguage model finetuning, followed by supervised task-specific finetuning. Our\nfindings on how to best exploit domain-specific language model finetuning\nenable us to produce new state-of-the-art performance on the SemEval 2014 Task\n4 restaurants dataset. In addition, to explore the real-world robustness of our\nmodels, we perform cross-domain evaluation. We show that a cross-domain adapted\nBERT language model performs significantly better than strong baseline models\nlike vanilla BERT-base and XLNet-base. Finally, we conduct a case study to\ninterpret model prediction errors.",
        "date": "2019-08-30"
    },
    "https://arxiv.org/abs/2006.10713": {
        "extra-tags": [],
        "tags": [
            "common sense",
            "knowledge graph",
            "zero shot learning",
            "arxiv doc"
        ],
        "title": "[2006.10713] Zero-Shot Learning with Common Sense Knowledge Graphs",
        "summary": "Zero-shot learning relies on semantic class representations such as\nhand-engineered attributes or learned embeddings to predict classes without any\nlabeled examples. We propose to learn class representations by embedding nodes\nfrom common sense knowledge graphs in a vector space. Common sense knowledge\ngraphs are an untapped source of explicit high-level knowledge that requires\nlittle human effort to apply to a range of tasks. To capture the knowledge in\nthe graph, we introduce ZSL-KG, a general-purpose framework with a novel\ntransformer graph convolutional network (TrGCN) for generating class\nrepresentations. Our proposed TrGCN architecture computes non-linear\ncombinations of node neighbourhoods. Our results show that ZSL-KG improves over\nexisting WordNet-based methods on five out of six zero-shot benchmark datasets\nin language and vision.",
        "date": "2020-06-18"
    },
    "https://arxiv.org/abs/1805.04174": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "deep learning attention",
            "nlp text classification",
            "label embedding"
        ],
        "title": "[1805.04174] Joint Embedding of Words and Labels for Text Classification (ACL Anthology 2018)",
        "summary": "Word embeddings are effective intermediate representations for capturing\nsemantic regularities between words, when learning the representations of text\nsequences. We propose to view text classification as a label-word joint\nembedding problem: each label is embedded in the same space with the word\nvectors. We introduce an attention framework that measures the compatibility of\nembeddings between text sequences and labels. The attention is learned on a\ntraining set of labeled samples to ensure that, given a text sequence, the\nrelevant words are weighted higher than the irrelevant ones. Our method\nmaintains the interpretability of word embeddings, and enjoys a built-in\nability to leverage alternative sources of information, in addition to input\ntext sequences. Extensive results on the several large text datasets show that\nthe proposed framework outperforms the state-of-the-art methods by a large\nmargin, in terms of both accuracy and speed.",
        "date": "2018-05-10"
    },
    "https://arxiv.org/abs/2205.08012": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "knowledge graph",
            "link prediction",
            "allen institute for ai a2i"
        ],
        "title": "[2205.08012] CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction",
        "summary": "Knowledge graph (KG) link prediction is a fundamental task in artificial\nintelligence, with applications in natural language processing, information\nretrieval, and biomedicine. Recently, promising results have been achieved by\nleveraging cross-modal information in KGs, using ensembles that combine\nknowledge graph embeddings (KGEs) and contextual language models (LMs).\nHowever, existing ensembles are either (1) not consistently effective in terms\nof ranking accuracy gains or (2) impractically inefficient on larger datasets\ndue to the combinatorial explosion problem of pairwise ranking with deep\nlanguage models. In this paper, we propose a novel tiered ranking architecture\nCascadER to maintain the ranking accuracy of full ensembling while improving\nefficiency considerably. CascadER uses LMs to rerank the outputs of more\nefficient base KGEs, relying on an adaptive subset selection scheme aimed at\ninvoking the LMs minimally while maximizing accuracy gain over the KGE.\nExtensive experiments demonstrate that CascadER improves MRR by up to 9 points\nover KGE baselines, setting new state-of-the-art performance on four benchmarks\nwhile improving efficiency by one or more orders of magnitude over competitive\ncross-modal baselines. Our empirical analyses reveal that diversity of models\nacross modalities and preservation of individual models' confidence signals\nhelp explain the effectiveness of CascadER, and suggest promising directions\nfor cross-modal cascaded architectures. Code and pretrained models are\navailable at https://github.com/tsafavi/cascader.",
        "date": "2022-05-16"
    },
    "https://arxiv.org/abs/2007.04612": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "human in the loop",
            "ai stanford",
            "concept bottleneck models"
        ],
        "title": "[2007.04612] Concept Bottleneck Models",
        "summary": "We seek to learn models that we can interact with using high-level concepts:\nif the model did not think there was a bone spur in the x-ray, would it still\npredict severe arthritis? State-of-the-art models today do not typically\nsupport the manipulation of concepts like \"the existence of bone spurs\", as\nthey are trained end-to-end to go directly from raw input (e.g., pixels) to\noutput (e.g., arthritis severity). We revisit the classic idea of first\npredicting concepts that are provided at training time, and then using these\nconcepts to predict the label. By construction, we can intervene on these\n\\emph{concept bottleneck models} by editing their predicted concept values and\npropagating these changes to the final prediction. On x-ray grading and bird\nidentification, concept bottleneck models achieve competitive accuracy with\nstandard end-to-end models, while enabling interpretation in terms of\nhigh-level clinical concepts (\"bone spurs\") or bird attributes (\"wing color\").\nThese models also allow for richer human-model interaction: accuracy improves\nsignificantly if we can correct model mistakes on concepts at test time.",
        "date": "2020-07-09"
    },
    "https://arxiv.org/abs/2005.11401": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "knowledge intensive nlp tasks",
            "retrieval augmented generation",
            "nlp facebook",
            "discute avec raphael"
        ],
        "title": "[2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
        "summary": "Large pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on\ndownstream NLP tasks. However, their ability to access and precisely manipulate\nknowledge is still limited, and hence on knowledge-intensive tasks, their\nperformance lags behind task-specific architectures. Additionally, providing\nprovenance for their decisions and updating their world knowledge remain open\nresearch problems. Pre-trained models with a differentiable access mechanism to\nexplicit non-parametric memory can overcome this issue, but have so far been\nonly investigated for extractive downstream tasks. We explore a general-purpose\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\ncombine pre-trained parametric and non-parametric memory for language\ngeneration. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\nformulations, one which conditions on the same retrieved passages across the\nwhole generated sequence, the other can use different passages per token. We\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\nFor language generation tasks, we find that RAG models generate more specific,\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\nbaseline.",
        "date": "2020-05-22"
    },
    "https://arxiv.org/abs/1911.06136": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "entities and lm",
            "discute avec raphael",
            "good related work section",
            "text kg and embeddings",
            "good"
        ],
        "title": "[1911.06136] KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation",
        "summary": "Pre-trained language representation models (PLMs) cannot well capture factual\nknowledge from text. In contrast, knowledge embedding (KE) methods can\neffectively represent the relational facts in knowledge graphs (KGs) with\ninformative entity embeddings, but conventional KE models do not utilize the\nrich text data. In this paper, we propose a unified model for Knowledge\nEmbedding and Pre-trained LanguagE Representation (KEPLER), which can not only\nbetter integrate factual knowledge into PLMs but also effectively learn KE\nthrough the abundant information in text. In KEPLER, we encode textual\ndescriptions of entities with a PLM as their embeddings, and then jointly\noptimize the KE and language modeling objectives. Experimental results show\nthat KEPLER achieves state-of-the-art performance on various NLP tasks, and\nalso works remarkably well as an inductive KE model on the link prediction\ntask. Furthermore, for pre-training KEPLER and evaluating the KE performance,\nwe construct Wikidata5M, a large-scale KG dataset with aligned entity\ndescriptions, and benchmark state-of-the-art KE methods on it. It shall serve\nas a new KE benchmark and facilitate the research on large KG, inductive KE,\nand KG with text. The dataset can be obtained from\nhttps://deepgraphlearning.github.io/project/wikidata5m.",
        "date": "2019-11-13"
    },
    "https://arxiv.org/abs/2107.12708": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp reading comprehension",
            "survey",
            "question answering",
            "sebastian ruder",
            "allen institute for ai a2i",
            "nlp datasets"
        ],
        "title": "[2107.12708] QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension",
        "summary": "Alongside huge volumes of research on deep learning models in NLP in the\nrecent years, there has been also much work on benchmark datasets needed to\ntrack modeling progress. Question answering and reading comprehension have been\nparticularly prolific in this regard, with over 80 new datasets appearing in\nthe past two years. This study is the largest survey of the field to date. We\nprovide an overview of the various formats and domains of the current\nresources, highlighting the current lacunae for future work. We further discuss\nthe current classifications of ``reasoning types\" in question answering and\npropose a new taxonomy. We also discuss the implications of over-focusing on\nEnglish, and survey the current monolingual resources for other languages and\nmultilingual resources. The study is aimed at both practitioners looking for\npointers to the wealth of existing data, and at researchers working on new\nresources.",
        "date": "2021-07-27"
    },
    "https://arxiv.org/abs/2009.02252": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp facebook",
            "benchmark",
            "knowledge intensive nlp tasks"
        ],
        "title": "[2009.02252] KILT: a Benchmark for Knowledge Intensive Language Tasks",
        "summary": "Challenging problems such as open-domain question answering, fact checking,\nslot filling and entity linking require access to large, external knowledge\nsources. While some models do well on individual tasks, developing general\nmodels is difficult as each task might require computationally expensive\nindexing of custom knowledge sources, in addition to dedicated infrastructure.\nTo catalyze research on models that condition on specific information in large\ntextual resources, we present a benchmark for knowledge-intensive language\ntasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia,\nreducing engineering turnaround through the re-use of components, as well as\naccelerating research into task-agnostic memory architectures. We test both\ntask-specific and general baselines, evaluating downstream performance in\naddition to the ability of the models to provide provenance. We find that a\nshared dense vector index coupled with a seq2seq model is a strong baseline,\noutperforming more tailor-made approaches for fact checking, open-domain\nquestion answering and dialogue, and yielding competitive results on entity\nlinking and slot filling, by generating disambiguated text. KILT data and code\nare available at https://github.com/facebookresearch/KILT.",
        "date": "2020-09-04"
    },
    "https://arxiv.org/abs/2003.05473": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "good",
            "discute avec raphael",
            "bert",
            "bertology",
            "end to end entity linking"
        ],
        "title": "[2003.05473] Investigating Entity Knowledge in BERT with Simple Neural End-To-End Entity Linking (CoNNL 2019)",
        "summary": "A typical architecture for end-to-end entity linking systems consists of\nthree steps: mention detection, candidate generation and entity disambiguation.\nIn this study we investigate the following questions: (a) Can all those steps\nbe learned jointly with a model for contextualized text-representations, i.e.\nBERT (Devlin et al., 2019)? (b) How much entity knowledge is already contained\nin pretrained BERT? (c) Does additional entity knowledge improve BERT's\nperformance in downstream tasks? To this end, we propose an extreme\nsimplification of the entity linking setup that works surprisingly well: simply\ncast it as a per token classification over the entire entity vocabulary (over\n700K classes in our case). We show on an entity linking benchmark that (i) this\nmodel improves the entity representations over plain BERT, (ii) that it\noutperforms entity linking architectures that optimize the tasks separately and\n(iii) that it only comes second to the current state-of-the-art that does\nmention detection and entity disambiguation jointly. Additionally, we\ninvestigate the usefulness of entity-aware token-representations in the\ntext-understanding benchmark GLUE, as well as the question answering benchmarks\nSQUAD V2 and SWAG and also the EN-DE WMT14 machine translation benchmark. To\nour surprise, we find that most of those benchmarks do not benefit from\nadditional entity knowledge, except for a task with very small training data,\nthe RTE task in GLUE, which improves by 2%.",
        "date": "2020-03-11"
    },
    "https://arxiv.org/abs/1907.04829": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "chris manning",
            "quoc le",
            "knowledge distillation",
            "kd mkb biblio",
            "multitask learning in nlp"
        ],
        "title": "[1907.04829] BAM! Born-Again Multi-Task Networks for Natural Language Understanding",
        "summary": "It can be challenging to train multi-task neural networks that outperform or\neven match their single-task counterparts. To help address this, we propose\nusing knowledge distillation where single-task models teach a multi-task model.\nWe enhance this training with teacher annealing, a novel method that gradually\ntransitions the model from distillation to supervised learning, helping the\nmulti-task model surpass its single-task teachers. We evaluate our approach by\nmulti-task fine-tuning BERT on the GLUE benchmark. Our method consistently\nimproves over standard single-task and multi-task training.",
        "date": "2019-07-10"
    },
    "https://arxiv.org/abs/cmp-lg/9511007": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "taxonomies"
        ],
        "title": "[cmp-lg/9511007] Using Information Content to Evaluate Semantic Similarity in a Taxonomy (1995)",
        "summary": "This paper presents a new measure of semantic similarity in an IS-A taxonomy,\nbased on the notion of information content. Experimental evaluation suggests\nthat the measure performs encouragingly well (a correlation of r = 0.79 with a\nbenchmark set of human similarity judgments, with an upper bound of r = 0.90\nfor human subjects performing the same task), and significantly better than the\ntraditional edge counting approach (r = 0.66).",
        "date": "1995-11-29"
    },
    "https://arxiv.org/abs/1904.02342": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "natural language generation",
            "kg and nlp"
        ],
        "title": "[1904.02342] Text Generation from Knowledge Graphs with Graph Transformers",
        "summary": "Generating texts which express complex ideas spanning multiple sentences\nrequires a structured representation of their content (document plan), but\nthese representations are prohibitively expensive to manually produce. In this\nwork, we address the problem of generating coherent multi-sentence texts from\nthe output of an information extraction system, and in particular a knowledge\ngraph. Graphical knowledge representations are ubiquitous in computing, but\npose a significant challenge for text generation techniques due to their\nnon-hierarchical nature, collapsing of long-distance dependencies, and\nstructural variety. We introduce a novel graph transforming encoder which can\nleverage the relational structure of such knowledge graphs without imposing\nlinearization or hierarchical constraints. Incorporated into an encoder-decoder\nsetup, we provide an end-to-end trainable system for graph-to-text generation\nthat we apply to the domain of scientific text. Automatic and human evaluations\nshow that our technique produces more informative texts which exhibit better\ndocument structure than competitive encoder-decoder methods.",
        "date": "2019-04-04"
    },
    "https://arxiv.org/abs/2112.07708": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "open domain question answering",
            "unsupervised machine learning",
            "dense retriever"
        ],
        "title": "[2112.07708] Learning to Retrieve Passages without Supervision",
        "summary": "Dense retrievers for open-domain question answering (ODQA) have been shown to\nachieve impressive performance by training on large datasets of\nquestion-passage pairs. In this work we ask whether this dependence on labeled\ndata can be reduced via unsupervised pretraining that is geared towards ODQA.\nWe show this is in fact possible, via a novel pretraining scheme designed for\nretrieval. Our \"recurring span retrieval\" approach uses recurring spans across\npassages in a document to create pseudo examples for contrastive learning. Our\npretraining scheme directly controls for term overlap across pseudo queries and\nrelevant passages, thus allowing to model both lexical and semantic relations\nbetween them. The resulting model, named Spider, performs surprisingly well\nwithout any labeled training examples on a wide range of ODQA datasets.\nSpecifically, it significantly outperforms all other pretrained baselines in a\nzero-shot setting, and is competitive with BM25, a strong sparse baseline.\nMoreover, a hybrid retriever over Spider and BM25 improves over both, and is\noften competitive with DPR models, which are trained on tens of thousands of\nexamples. Last, notable gains are observed when using Spider as an\ninitialization for supervised training.",
        "date": "2021-12-14"
    },
    "https://arxiv.org/abs/2010.12566": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "dictionnaire",
            "nlp google",
            "multilingual language models"
        ],
        "title": "[2010.12566] DICT-MLM: Improved Multilingual Pre-Training using Bilingual Dictionaries",
        "summary": "Pre-trained multilingual language models such as mBERT have shown immense\ngains for several natural language processing (NLP) tasks, especially in the\nzero-shot cross-lingual setting. Most, if not all, of these pre-trained models\nrely on the masked-language modeling (MLM) objective as the key language\nlearning objective. The principle behind these approaches is that predicting\nthe masked words with the help of the surrounding text helps learn potent\ncontextualized representations. Despite the strong representation learning\ncapability enabled by MLM, we demonstrate an inherent limitation of MLM for\nmultilingual representation learning. In particular, by requiring the model to\npredict the language-specific token, the MLM objective disincentivizes learning\na language-agnostic representation -- which is a key goal of multilingual\npre-training. Therefore to encourage better cross-lingual representation\nlearning we propose the DICT-MLM method. DICT-MLM works by incentivizing the\nmodel to be able to predict not just the original masked word, but potentially\nany of its cross-lingual synonyms as well. Our empirical analysis on multiple\ndownstream tasks spanning 30+ languages, demonstrates the efficacy of the\nproposed approach and its ability to learn better multilingual representations.",
        "date": "2020-10-23"
    },
    "https://arxiv.org/abs/1812.02956": {
        "extra-tags": [],
        "tags": [
            "multi label classification",
            "arxiv doc"
        ],
        "title": "[1812.02956] LNEMLC: Label Network Embeddings for Multi-Label Classification",
        "summary": "Multi-label classification aims to classify instances with discrete\nnon-exclusive labels. Most approaches on multi-label classification focus on\neffective adaptation or transformation of existing binary and multi-class\nlearning approaches but fail in modelling the joint probability of labels or do\nnot preserve generalization abilities for unseen label combinations. To address\nthese issues we propose a new multi-label classification scheme, LNEMLC - Label\nNetwork Embedding for Multi-Label Classification, that embeds the label network\nand uses it to extend input space in learning and inference of any base\nmulti-label classifier. The approach allows capturing of labels' joint\nprobability at low computational complexity providing results comparable to the\nbest methods reported in the literature. We demonstrate how the method reveals\nstatistically significant improvements over the simple kNN baseline classifier.\nWe also provide hints for selecting the robust configuration that works\nsatisfactorily across data domains.",
        "date": "2018-12-07"
    },
    "https://arxiv.org/abs/1807.00745": {
        "extra-tags": [],
        "tags": [
            "named entity recognition",
            "nlp low resource scenarios",
            "automatically annotated data",
            "arxiv doc"
        ],
        "title": "[1807.00745] Training a Neural Network in a Low-Resource Setting on Automatically Annotated Noisy Data",
        "summary": "Manually labeled corpora are expensive to create and often not available for\nlow-resource languages or domains. Automatic labeling approaches are an\nalternative way to obtain labeled data in a quicker and cheaper way. However,\nthese labels often contain more errors which can deteriorate a classifier's\nperformance when trained on this data. We propose a noise layer that is added\nto a neural network architecture. This allows modeling the noise and train on a\ncombination of clean and noisy data. We show that in a low-resource NER task we\ncan improve performance by up to 35% by using additional, noisy data and\nhandling the noise.",
        "date": "2018-07-02"
    },
    "https://arxiv.org/abs/2108.13934": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "emnlp 2021",
            "michael glass",
            "retrieval based language models",
            "retrieval augmented generation",
            "zero shot",
            "discute avec raphael",
            "hard negative mining",
            "nlp ibm",
            "slot tagging",
            "knowledge extraction"
        ],
        "title": "[2108.13934] Robust Retrieval Augmented Generation for Zero-shot Slot Filling",
        "summary": "Automatically inducing high quality knowledge graphs from a given collection\nof documents still remains a challenging problem in AI. One way to make headway\nfor this problem is through advancements in a related task known as slot\nfilling. In this task, given an entity query in form of [Entity, Slot, ?], a\nsystem is asked to fill the slot by generating or extracting the missing value\nexploiting evidence extracted from relevant passage(s) in the given document\ncollection. The recent works in the field try to solve this task in an\nend-to-end fashion using retrieval-based language models. In this paper, we\npresent a novel approach to zero-shot slot filling that extends dense passage\nretrieval with hard negatives and robust training procedures for retrieval\naugmented generation models. Our model reports large improvements on both T-REx\nand zsRE slot filling datasets, improving both passage retrieval and slot value\ngeneration, and ranking at the top-1 position in the KILT leaderboard.\nMoreover, we demonstrate the robustness of our system showing its domain\nadaptation capability on a new variant of the TACRED dataset for slot filling,\nthrough a combination of zero/few-shot learning. We release the source code and\npre-trained models.",
        "date": "2021-08-31"
    },
    "https://arxiv.org/abs/1905.06088": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "neural symbolic computing"
        ],
        "title": "[1905.06088] Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning",
        "summary": "Current advances in Artificial Intelligence and machine learning in general,\nand deep learning in particular have reached unprecedented impact not only\nacross research communities, but also over popular media channels. However,\nconcerns about interpretability and accountability of AI have been raised by\ninfluential thinkers. In spite of the recent impact of AI, several works have\nidentified the need for principled knowledge representation and reasoning\nmechanisms integrated with deep learning-based systems to provide sound and\nexplainable models for such systems. Neural-symbolic computing aims at\nintegrating, as foreseen by Valiant, two most fundamental cognitive abilities:\nthe ability to learn from the environment, and the ability to reason from what\nhas been learned. Neural-symbolic computing has been an active topic of\nresearch for many years, reconciling the advantages of robust learning in\nneural networks and reasoning and interpretability of symbolic representation.\nIn this paper, we survey recent accomplishments of neural-symbolic computing as\na principled methodology for integrated machine learning and reasoning. We\nillustrate the effectiveness of the approach by outlining the main\ncharacteristics of the methodology: principled integration of neural learning\nwith symbolic knowledge representation and reasoning allowing for the\nconstruction of explainable AI systems. The insights provided by\nneural-symbolic computing shed new light on the increasingly prominent need for\ninterpretable and accountable AI systems.",
        "date": "2019-05-15"
    },
    "https://arxiv.org/abs/2204.11428": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "knowledge graph",
            "personal knowledge graph",
            "semanlink related"
        ],
        "title": "[2204.11428] Personal Research Knowledge Graphs",
        "summary": "Maintaining research-related information in an organized manner can be\nchallenging for a researcher. In this paper, we envision personal research\nknowledge graphs (PRKGs) as a means to represent structured information about\nthe research activities of a researcher. PRKGs can be used to power intelligent\npersonal assistants, and personalize various applications. We explore what\nentities and relations could be potentially included in a PRKG, how to extract\nthem from various sources, and how to share a PRKG within a research group.",
        "date": "2022-04-25"
    },
    "https://arxiv.org/abs/2202.14037": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "sanjeev arora",
            "contrastive learning"
        ],
        "title": "[2202.14037] Understanding Contrastive Learning Requires Incorporating Inductive Biases",
        "summary": "Contrastive learning is a popular form of self-supervised learning that\nencourages augmentations (views) of the same input to have more similar\nrepresentations compared to augmentations of different inputs. Recent attempts\nto theoretically explain the success of contrastive learning on downstream\nclassification tasks prove guarantees depending on properties of {\\em\naugmentations} and the value of {\\em contrastive loss} of representations. We\ndemonstrate that such analyses, that ignore {\\em inductive biases} of the\nfunction class and training algorithm, cannot adequately explain the success of\ncontrastive learning, even {\\em provably} leading to vacuous guarantees in some\nsettings. Extensive experiments on image and text domains highlight the\nubiquity of this problem -- different function classes and algorithms behave\nvery differently on downstream tasks, despite having the same augmentations and\ncontrastive losses. Theoretical analysis is presented for the class of linear\nrepresentations, where incorporating inductive biases of the function class\nallows contrastive learning to work with less stringent conditions compared to\nprior analyses.",
        "date": "2022-02-28"
    },
    "https://arxiv.org/abs/1909.01066": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "facebook fair",
            "nlp facebook",
            "knowledge graph deep learning",
            "language model",
            "language models as knowledge bases"
        ],
        "title": "[1909.01066] Language Models as Knowledge Bases?",
        "summary": "Recent progress in pretraining language models on large textual corpora led\nto a surge of improvements for downstream NLP tasks. Whilst learning linguistic\nknowledge, these models may also be storing relational knowledge present in the\ntraining data, and may be able to answer queries structured as\n\"fill-in-the-blank\" cloze statements. Language models have many advantages over\nstructured knowledge bases: they require no schema engineering, allow\npractitioners to query about an open class of relations, are easy to extend to\nmore data, and require no human supervision to train. We present an in-depth\nanalysis of the relational knowledge already present (without fine-tuning) in a\nwide range of state-of-the-art pretrained language models. We find that (i)\nwithout fine-tuning, BERT contains relational knowledge competitive with\ntraditional NLP methods that have some access to oracle knowledge, (ii) BERT\nalso does remarkably well on open-domain question answering against a\nsupervised baseline, and (iii) certain types of factual knowledge are learned\nmuch more readily than others by standard language model pretraining\napproaches. The surprisingly strong ability of these models to recall factual\nknowledge without any fine-tuning demonstrates their potential as unsupervised\nopen-domain QA systems. The code to reproduce our analysis is available at\nhttps://github.com/facebookresearch/LAMA.",
        "date": "2019-09-03"
    },
    "https://arxiv.org/abs/2203.08913": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "k nearest neighbors algorithm",
            "memory in deep learning",
            "language model",
            "nlp google"
        ],
        "title": "[2203.08913] Memorizing Transformers",
        "summary": "Language models typically need to be trained or finetuned in order to acquire\nnew knowledge, which involves updating their weights. We instead envision\nlanguage models that can simply read and memorize new data at inference time,\nthus acquiring new knowledge immediately. In this work, we extend language\nmodels with the ability to memorize the internal representations of past\ninputs. We demonstrate that an approximate kNN lookup into a non-differentiable\nmemory of recent (key, value) pairs improves language modeling across various\nbenchmarks and tasks, including generic webtext (C4), math papers (arXiv),\nbooks (PG-19), code (Github), as well as formal theorems (Isabelle). We show\nthat the performance steadily improves when we increase the size of memory up\nto 262K tokens. On benchmarks including code and mathematics, we find that the\nmodel is capable of making use of newly defined functions and theorems during\ntest time.",
        "date": "2022-03-16"
    },
    "https://arxiv.org/abs/2007.00849": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "knowledge graph deep learning",
            "not encoding knowledge in language model",
            "neural memory",
            "ai knowledge bases",
            "google research",
            "nn symbolic ai hybridation",
            "nlp google"
        ],
        "title": "[2007.00849] Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge",
        "summary": "Massive language models are the core of modern NLP modeling and have been\nshown to encode impressive amounts of commonsense and factual information.\nHowever, that knowledge exists only within the latent parameters of the model,\ninaccessible to inspection and interpretation, and even worse, factual\ninformation memorized from the training corpora is likely to become stale as\nthe world changes. Knowledge stored as parameters will also inevitably exhibit\nall of the biases inherent in the source materials. To address these problems,\nwe develop a neural language model that includes an explicit interface between\nsymbolically interpretable factual information and subsymbolic neural\nknowledge. We show that this model dramatically improves performance on two\nknowledge-intensive question-answering tasks. More interestingly, the model can\nbe updated without re-training by manipulating its symbolic representations. In\nparticular this model allows us to add new facts and overwrite existing ones in\nways that are not possible for earlier models.",
        "date": "2020-07-02"
    },
    "https://arxiv.org/abs/2104.06979": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nils reimers",
            "sentence embeddings",
            "emnlp 2021",
            "domain adaptation nlp",
            "tsdae",
            "unsupervised sentence embedding learning"
        ],
        "title": "[2104.06979] TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning",
        "summary": "Learning sentence embeddings often requires a large amount of labeled data.\nHowever, for most tasks and domains, labeled data is seldom available and\ncreating it is expensive. In this work, we present a new state-of-the-art\nunsupervised method based on pre-trained Transformers and Sequential Denoising\nAuto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points.\nIt can achieve up to 93.1% of the performance of in-domain supervised\napproaches. Further, we show that TSDAE is a strong domain adaptation and\npre-training method for sentence embeddings, significantly outperforming other\napproaches like Masked Language Model.\nA crucial shortcoming of previous studies is the narrow evaluation: Most work\nmainly evaluates on the single task of Semantic Textual Similarity (STS), which\ndoes not require any domain knowledge. It is unclear if these proposed methods\ngeneralize to other domains and tasks. We fill this gap and evaluate TSDAE and\nother recent approaches on four different datasets from heterogeneous domains.",
        "date": "2021-04-14"
    },
    "https://arxiv.org/abs/2106.04612": {
        "extra-tags": [],
        "tags": [
            "yoav goldberg",
            "arxiv doc",
            "cognitive search",
            "search",
            "allen institute for ai a2i",
            "neural models for information retrieval"
        ],
        "title": "[2106.04612] Neural Extractive Search",
        "summary": "Domain experts often need to extract structured information from large\ncorpora. We advocate for a search paradigm called ``extractive search'', in\nwhich a search query is enriched with capture-slots, to allow for such rapid\nextraction. Such an extractive search system can be built around syntactic\nstructures, resulting in high-precision, low-recall results. We show how the\nrecall can be improved using neural retrieval and alignment. The goals of this\npaper are to concisely introduce the extractive-search paradigm; and to\ndemonstrate a prototype neural retrieval system for extractive search and its\nbenefits and potential. Our prototype is available at\n\\url{https://spike.neural-sim.apps.allenai.org/} and a video demonstration is\navailable at \\url{https://vimeo.com/559586687}.",
        "date": "2021-06-08"
    },
    "https://arxiv.org/abs/2201.00042": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "catastrophic forgetting",
            "neuroscience and ai",
            "multi task learning"
        ],
        "title": "[2201.00042] Avoiding Catastrophe: Active Dendrites Enable Multi-Task Learning in Dynamic Environments",
        "summary": "A key challenge for AI is to build embodied systems that operate in\ndynamically changing environments. Such systems must adapt to changing task\ncontexts and learn continuously. Although standard deep learning systems\nachieve state of the art results on static benchmarks, they often struggle in\ndynamic scenarios. In these settings, error signals from multiple contexts can\ninterfere with one another, ultimately leading to a phenomenon known as\ncatastrophic forgetting. In this article we investigate biologically inspired\narchitectures as solutions to these problems. Specifically, we show that the\nbiophysical properties of dendrites and local inhibitory systems enable\nnetworks to dynamically restrict and route information in a context-specific\nmanner. Our key contributions are as follows. First, we propose a novel\nartificial neural network architecture that incorporates active dendrites and\nsparse representations into the standard deep learning framework. Next, we\nstudy the performance of this architecture on two separate benchmarks requiring\ntask-based adaptation: Meta-World, a multi-task reinforcement learning\nenvironment where a robotic agent must learn to solve a variety of manipulation\ntasks simultaneously; and a continual learning benchmark in which the model's\nprediction task changes throughout training. Analysis on both benchmarks\ndemonstrates the emergence of overlapping but distinct and sparse subnetworks,\nallowing the system to fluidly learn multiple tasks with minimal forgetting.\nOur neural implementation marks the first time a single architecture has\nachieved competitive results on both multi-task and continual learning\nsettings. Our research sheds light on how biological properties of neurons can\ninform deep learning systems to address dynamic scenarios that are typically\nimpossible for traditional ANNs to solve.",
        "date": "2021-12-31"
    },
    "https://arxiv.org/abs/1906.04980": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "unsupervised qa",
            "nlp facebook",
            "acl 2019",
            "synthetic qa data",
            "ludovic denoyer"
        ],
        "title": "[1906.04980] Unsupervised Question Answering by Cloze Translation",
        "summary": "Obtaining training data for Question Answering (QA) is time-consuming and\nresource-intensive, and existing QA datasets are only available for limited\ndomains and languages. In this work, we explore to what extent high quality\ntraining data is actually required for Extractive QA, and investigate the\npossibility of unsupervised Extractive QA. We approach this problem by first\nlearning to generate context, question and answer triples in an unsupervised\nmanner, which we then use to synthesize Extractive QA training data\nautomatically. To generate such triples, we first sample random context\nparagraphs from a large corpus of documents and then random noun phrases or\nnamed entity mentions from these paragraphs as answers. Next we convert answers\nin context to \"fill-in-the-blank\" cloze questions and finally translate them\ninto natural questions. We propose and compare various unsupervised ways to\nperform cloze-to-natural question translation, including training an\nunsupervised NMT model using non-aligned corpora of natural questions and cloze\nquestions as well as a rule-based approach. We find that modern QA models can\nlearn to answer human questions surprisingly well using only synthetic training\ndata. We demonstrate that, without using the SQuAD training data at all, our\napproach achieves 56.4 F1 on SQuAD v1 (64.5 F1 when the answer is a Named\nentity mention), outperforming early supervised models.",
        "date": "2019-06-12"
    },
    "https://arxiv.org/abs/2112.09118": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "gautier izacard",
            "unsupervised domain adaptation nlp",
            "nlp facebook",
            "nlp ens",
            "dense passage retrieval",
            "contrastive learning"
        ],
        "title": "[2112.09118] Towards Unsupervised Dense Information Retrieval with Contrastive Learning",
        "summary": "Information retrieval is an important component in natural language\nprocessing, for knowledge intensive tasks such as question answering and fact\nchecking. Recently, information retrieval has seen the emergence of dense\nretrievers, based on neural networks, as an alternative to classical sparse\nmethods based on term-frequency. These models have obtained state-of-the-art\nresults on datasets and tasks where large training sets are available. However,\nthey do not transfer well to new domains or applications with no training data,\nand are often outperformed by term-frequency methods such as BM25 which are not\nsupervised. Thus, a natural question is whether it is possible to train dense\nretrievers without supervision. In this work, we explore the limits of\ncontrastive learning as a way to train unsupervised dense retrievers, and show\nthat it leads to strong retrieval performance. More precisely, we show on the\nBEIR benchmark that our model outperforms BM25 on 11 out of 15 datasets.\nFurthermore, when a few thousands examples are available, we show that\nfine-tuning our model on these leads to strong improvements compared to BM25.\nFinally, when used as pre-training before fine-tuning on the MS-MARCO dataset,\nour technique obtains state-of-the-art results on the BEIR benchmark.",
        "date": "2021-12-16"
    },
    "https://arxiv.org/abs/1906.00300": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "orqa",
            "end to end learning",
            "semi supervised qa",
            "open domain question answering",
            "nlp google"
        ],
        "title": "[1906.00300] Latent Retrieval for Weakly Supervised Open Domain Question Answering",
        "summary": "Recent work on open domain question answering (QA) assumes strong supervision\nof the supporting evidence and/or assumes a blackbox information retrieval (IR)\nsystem to retrieve evidence candidates. We argue that both are suboptimal,\nsince gold evidence is not always available, and QA is fundamentally different\nfrom IR. We show for the first time that it is possible to jointly learn the\nretriever and reader from question-answer string pairs and without any IR\nsystem. In this setting, evidence retrieval from all of Wikipedia is treated as\na latent variable. Since this is impractical to learn from scratch, we\npre-train the retriever with an Inverse Cloze Task. We evaluate on open\nversions of five QA datasets. On datasets where the questioner already knows\nthe answer, a traditional IR system such as BM25 is sufficient. On datasets\nwhere a user is genuinely seeking an answer, we show that learned retrieval is\ncrucial, outperforming BM25 by up to 19 points in exact match.",
        "date": "2019-06-01"
    },
    "https://arxiv.org/abs/2007.12603": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "bert",
            "semantic search",
            "okapi bm25",
            "embeddings in ir"
        ],
        "title": "[2007.12603] IR-BERT: Leveraging BERT for Semantic Search in Background Linking for News Articles",
        "summary": "This work describes our two approaches for the background linking task of\nTREC 2020 News Track. The main objective of this task is to recommend a list of\nrelevant articles that the reader should refer to in order to understand the\ncontext and gain background information of the query article. Our first\napproach focuses on building an effective search query by combining weighted\nkeywords extracted from the query document and uses BM25 for retrieval. The\nsecond approach leverages the capability of SBERT (Nils Reimers et al.) to\nlearn contextual representations of the query in order to perform semantic\nsearch over the corpus. We empirically show that employing a language model\nbenefits our approach in understanding the context as well as the background of\nthe query article. The proposed approaches are evaluated on the TREC 2018\nWashington Post dataset and our best model outperforms the TREC median as well\nas the highest scoring model of 2018 in terms of the nDCG@5 metric. We further\npropose a diversity measure to evaluate the effectiveness of the various\napproaches in retrieving a diverse set of documents. This would potentially\nmotivate researchers to work on introducing diversity in their recommended\nlist. We have open sourced our implementation on Github and plan to submit our\nruns for the background linking task in TREC 2020.",
        "date": "2020-07-24"
    },
    "https://arxiv.org/abs/2004.10964": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "lm adaptation to domain",
            "domain adaptation nlp",
            "allennlp",
            "nlp pretraining",
            "language model fine tuning"
        ],
        "title": "[2004.10964] Don't Stop Pretraining: Adapt Language Models to Domains and Tasks",
        "summary": "Language models pretrained on text from a wide variety of sources form the\nfoundation of today's NLP. In light of the success of these broad-coverage\nmodels, we investigate whether it is still helpful to tailor a pretrained model\nto the domain of a target task. We present a study across four domains\n(biomedical and computer science publications, news, and reviews) and eight\nclassification tasks, showing that a second phase of pretraining in-domain\n(domain-adaptive pretraining) leads to performance gains, under both high- and\nlow-resource settings. Moreover, adapting to the task's unlabeled data\n(task-adaptive pretraining) improves performance even after domain-adaptive\npretraining. Finally, we show that adapting to a task corpus augmented using\nsimple data selection strategies is an effective alternative, especially when\nresources for domain-adaptive pretraining might be unavailable. Overall, we\nconsistently find that multi-phase adaptive pretraining offers large gains in\ntask performance.",
        "date": "2020-04-23"
    },
    "https://arxiv.org/abs/2010.11882": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "convolutional neural network",
            "deep learning"
        ],
        "title": "[2010.11882] Learning Invariances in Neural Networks",
        "summary": "Invariances to translations have imbued convolutional neural networks with\npowerful generalization properties. However, we often do not know a priori what\ninvariances are present in the data, or to what extent a model should be\ninvariant to a given symmetry group. We show how to \\emph{learn} invariances\nand equivariances by parameterizing a distribution over augmentations and\noptimizing the training loss simultaneously with respect to the network\nparameters and augmentation parameters. With this simple procedure we can\nrecover the correct set and extent of invariances on image classification,\nregression, segmentation, and molecular property prediction from a large space\nof augmentations, on training data alone.",
        "date": "2020-10-22"
    },
    "https://arxiv.org/abs/1909.00426": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "ikuya yamada",
            "target entity disambiguation",
            "bert"
        ],
        "title": "[1909.00426] Global Entity Disambiguation with BERT",
        "summary": "We propose a global entity disambiguation (ED) model based on BERT. To\ncapture global contextual information for ED, our model treats not only words\nbut also entities as input tokens, and solves the task by sequentially\nresolving mentions to their referent entities and using resolved entities as\ninputs at each step. We train the model using a large entity-annotated corpus\nobtained from Wikipedia. We achieve new state-of-the-art results on five\nstandard ED datasets: AIDA-CoNLL, MSNBC, AQUAINT, ACE2004, and WNED-WIKI. The\nsource code and model checkpoint are available at\nhttps://github.com/studio-ousia/luke.",
        "date": "2019-09-01"
    },
    "https://arxiv.org/abs/2003.08505": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "ai facebook",
            "metric learning",
            "ml evaluation"
        ],
        "title": "[2003.08505] A Metric Learning Reality Check",
        "summary": "Deep metric learning papers from the past four years have consistently\nclaimed great advances in accuracy, often more than doubling the performance of\ndecade-old methods. In this paper, we take a closer look at the field to see if\nthis is actually true. We find flaws in the experimental setup of these papers,\nand propose a new way to evaluate metric learning algorithms. Finally, we\npresent experimental results that show that the improvements over time have\nbeen marginal at best.",
        "date": "2020-03-18"
    },
    "https://arxiv.org/abs/2009.12030": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "emnlp 2020",
            "discute avec raphael",
            "knowledge graph embeddings",
            "entity type representation"
        ],
        "title": "[2009.12030] AutoETER: Automated Entity Type Representation for Knowledge Graph Embedding",
        "summary": "Recent advances in Knowledge Graph Embedding (KGE) allow for representing\nentities and relations in continuous vector spaces. Some traditional KGE models\nleveraging additional type information can improve the representation of\nentities which however totally rely on the explicit types or neglect the\ndiverse type representations specific to various relations. Besides, none of\nthe existing methods is capable of inferring all the relation patterns of\nsymmetry, inversion and composition as well as the complex properties of 1-N,\nN-1 and N-N relations, simultaneously. To explore the type information for any\nKG, we develop a novel KGE framework with Automated Entity TypE Representation\n(AutoETER), which learns the latent type embedding of each entity by regarding\neach relation as a translation operation between the types of two entities with\na relation-aware projection mechanism. Particularly, our designed automated\ntype representation learning mechanism is a pluggable module which can be\neasily incorporated with any KGE model. Besides, our approach could model and\ninfer all the relation patterns and complex relations. Experiments on four\ndatasets demonstrate the superior performance of our model compared to\nstate-of-the-art baselines on link prediction tasks, and the visualization of\ntype clustering provides clearly the explanation of type embeddings and\nverifies the effectiveness of our model.",
        "date": "2020-09-25"
    },
    "https://arxiv.org/abs/1911.03681": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "entity embeddings",
            "discute avec raphael",
            "bert",
            "bert kb",
            "text kg and embeddings"
        ],
        "title": "[1911.03681] E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT",
        "summary": "We present a novel way of injecting factual knowledge about entities into the\npretrained BERT model (Devlin et al., 2019): We align Wikipedia2Vec entity\nvectors (Yamada et al., 2016) with BERT's native wordpiece vector space and use\nthe aligned entity vectors as if they were wordpiece vectors. The resulting\nentity-enhanced version of BERT (called E-BERT) is similar in spirit to ERNIE\n(Zhang et al., 2019) and KnowBert (Peters et al., 2019), but it requires no\nexpensive further pretraining of the BERT encoder. We evaluate E-BERT on\nunsupervised question answering (QA), supervised relation classification (RC)\nand entity linking (EL). On all three tasks, E-BERT outperforms BERT and other\nbaselines. We also show quantitatively that the original BERT model is overly\nreliant on the surface form of entity names (e.g., guessing that someone with\nan Italian-sounding name speaks Italian), and that E-BERT mitigates this\nproblem.",
        "date": "2019-11-09"
    },
    "https://arxiv.org/abs/2010.07245": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "self training",
            "text classification using label names only",
            "zero shot text classifier"
        ],
        "title": "[2010.07245] Text Classification Using Label Names Only: A Language Model Self-Training Approach",
        "summary": "Current text classification methods typically require a good number of\nhuman-labeled documents as training data, which can be costly and difficult to\nobtain in real applications. Humans can perform classification without seeing\nany labeled examples but only based on a small set of words describing the\ncategories to be classified. In this paper, we explore the potential of only\nusing the label name of each class to train classification models on unlabeled\ndata, without using any labeled documents. We use pre-trained neural language\nmodels both as general linguistic knowledge sources for category understanding\nand as representation learning models for document classification. Our method\n(1) associates semantically related words with the label names, (2) finds\ncategory-indicative words and trains the model to predict their implied\ncategories, and (3) generalizes the model via self-training. We show that our\nmodel achieves around 90% accuracy on four benchmark datasets including topic\nand sentiment classification without using any labeled documents but learning\nfrom unlabeled data supervised by at most 3 words (1 in most cases) per class\nas the label name.",
        "date": "2020-10-14"
    },
    "https://arxiv.org/abs/2010.12321": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp french"
        ],
        "title": "[2010.12321] BARThez: a Skilled Pretrained French Sequence-to-Sequence Model",
        "summary": "Inductive transfer learning has taken the entire NLP field by storm, with\nmodels such as BERT and BART setting new state of the art on countless NLU\ntasks. However, most of the available models and research have been conducted\nfor English. In this work, we introduce BARThez, the first large-scale\npretrained seq2seq model for French. Being based on BART, BARThez is\nparticularly well-suited for generative tasks. We evaluate BARThez on five\ndiscriminative tasks from the FLUE benchmark and two generative tasks from a\nnovel summarization dataset, OrangeSum, that we created for this research. We\nshow BARThez to be very competitive with state-of-the-art BERT-based French\nlanguage models such as CamemBERT and FlauBERT. We also continue the\npretraining of a multilingual BART on BARThez' corpus, and show our resulting\nmodel, mBARThez, to significantly boost BARThez' generative performance. Code,\ndata and models are publicly available.",
        "date": "2020-10-23"
    },
    "https://arxiv.org/abs/1911.02168": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "attention knowledge graphs",
            "knowledge graph embeddings",
            "baidu",
            "link prediction"
        ],
        "title": "[1911.02168] CoKE: Contextualized Knowledge Graph Embedding",
        "summary": "Knowledge graph embedding, which projects symbolic entities and relations\ninto continuous vector spaces, is gaining increasing attention. Previous\nmethods allow a single static embedding for each entity or relation, ignoring\ntheir intrinsic contextual nature, i.e., entities and relations may appear in\ndifferent graph contexts, and accordingly, exhibit different properties. This\nwork presents Contextualized Knowledge Graph Embedding (CoKE), a novel paradigm\nthat takes into account such contextual nature, and learns dynamic, flexible,\nand fully contextualized entity and relation embeddings. Two types of graph\ncontexts are studied: edges and paths, both formulated as sequences of entities\nand relations. CoKE takes a sequence as input and uses a Transformer encoder to\nobtain contextualized representations. These representations are hence\nnaturally adaptive to the input, capturing contextual meanings of entities and\nrelations therein. Evaluation on a wide variety of public benchmarks verifies\nthe superiority of CoKE in link prediction and path query answering. It\nperforms consistently better than, or at least equally well as current\nstate-of-the-art in almost every case, in particular offering an absolute\nimprovement of 21.0% in H@10 on path query answering. Our code is available at\n\\url{https://github.com/PaddlePaddle/Research/tree/master/KG/CoKE}.",
        "date": "2019-11-06"
    },
    "https://arxiv.org/abs/2104.08663": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nils reimers",
            "beir",
            "zero shot",
            "benchmark",
            "okapi bm25",
            "nlp datasets",
            "information retrieval",
            "neural models for information retrieval"
        ],
        "title": "[2104.08663] BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models",
        "summary": "Neural IR models have often been studied in homogeneous and narrow settings,\nwhich has considerably limited insights into their generalization capabilities.\nTo address this, and to allow researchers to more broadly establish the\neffectiveness of their models, we introduce BEIR (Benchmarking IR), a\nheterogeneous benchmark for information retrieval. We leverage a careful\nselection of 17 datasets for evaluation spanning diverse retrieval tasks\nincluding open-domain datasets as well as narrow expert domains. We study the\neffectiveness of nine state-of-the-art retrieval models in a zero-shot\nevaluation setup on BEIR, finding that performing well consistently across all\ndatasets is challenging. Our results show BM25 is a robust baseline and\nReranking-based models overall achieve the best zero-shot performances,\nhowever, at high computational costs. In contrast, Dense-retrieval models are\ncomputationally more efficient but often underperform other approaches,\nhighlighting the considerable room for improvement in their generalization\ncapabilities. In this work, we extensively analyze different retrieval models\nand provide several suggestions that we believe may be useful for future work.\nBEIR datasets and code are available at https://github.com/UKPLab/beir.",
        "date": "2021-04-17"
    },
    "https://arxiv.org/abs/2004.03705": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "nlp text classification"
        ],
        "title": "[2004.03705] Deep Learning Based Text Classification: A Comprehensive Review",
        "summary": "Deep learning based models have surpassed classical machine learning based\napproaches in various text classification tasks, including sentiment analysis,\nnews categorization, question answering, and natural language inference. In\nthis work, we provide a detailed review of more than 150 deep learning based\nmodels for text classification developed in recent years, and discuss their\ntechnical contributions, similarities, and strengths. We also provide a summary\nof more than 40 popular datasets widely used for text classification. Finally,\nwe provide a quantitative analysis of the performance of different deep\nlearning models on popular benchmarks, and discuss future research directions.",
        "date": "2020-04-06"
    },
    "https://arxiv.org/abs/2001.07685": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "semi supervised learning",
            "google research"
        ],
        "title": "[2001.07685] FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
        "summary": "Semi-supervised learning (SSL) provides an effective means of leveraging\nunlabeled data to improve a model's performance. In this paper, we demonstrate\nthe power of a simple combination of two common SSL methods: consistency\nregularization and pseudo-labeling. Our algorithm, FixMatch, first generates\npseudo-labels using the model's predictions on weakly-augmented unlabeled\nimages. For a given image, the pseudo-label is only retained if the model\nproduces a high-confidence prediction. The model is then trained to predict the\npseudo-label when fed a strongly-augmented version of the same image. Despite\nits simplicity, we show that FixMatch achieves state-of-the-art performance\nacross a variety of standard semi-supervised learning benchmarks, including\n94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just\n4 labels per class. Since FixMatch bears many similarities to existing SSL\nmethods that achieve worse performance, we carry out an extensive ablation\nstudy to tease apart the experimental factors that are most important to\nFixMatch's success. We make our code available at\nhttps://github.com/google-research/fixmatch.",
        "date": "2020-01-21"
    },
    "https://arxiv.org/abs/1810.02840": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "snorkel",
            "weak supervision"
        ],
        "title": "[1810.02840] Training Complex Models with Multi-Task Weak Supervision",
        "summary": "As machine learning models continue to increase in complexity, collecting\nlarge hand-labeled training sets has become one of the biggest roadblocks in\npractice. Instead, weaker forms of supervision that provide noisier but cheaper\nlabels are often used. However, these weak supervision sources have diverse and\nunknown accuracies, may output correlated labels, and may label different tasks\nor apply at different levels of granularity. We propose a framework for\nintegrating and modeling such weak supervision sources by viewing them as\nlabeling different related sub-tasks of a problem, which we refer to as the\nmulti-task weak supervision setting. We show that by solving a matrix\ncompletion-style problem, we can recover the accuracies of these multi-task\nsources given their dependency structure, but without any labeled data, leading\nto higher-quality supervision for training an end model. Theoretically, we show\nthat the generalization error of models trained with this approach improves\nwith the number of unlabeled data points, and characterize the scaling with\nrespect to the task and dependency structures. On three fine-grained\nclassification problems, we show that our approach leads to average gains of\n20.2 points in accuracy over a traditional supervised approach, 6.8 points over\na majority vote baseline, and 4.1 points over a previously proposed weak\nsupervision method that models tasks separately.",
        "date": "2018-10-05"
    },
    "https://arxiv.org/abs/2208.01815": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "spellchecker",
            "personal assistant",
            "writing assistant"
        ],
        "title": "[2208.01815] Effidit: Your AI Writing Assistant",
        "summary": "In this technical report, we introduce Effidit (Efficient and Intelligent\nEditing), a digital writing assistant that facilitates users to write\nhigher-quality text more efficiently by using artificial intelligence (AI)\ntechnologies. Previous writing assistants typically provide the function of\nerror checking (to detect and correct spelling and grammatical errors) and\nlimited text-rewriting functionality. With the emergence of large-scale neural\nlanguage models, some systems support automatically completing a sentence or a\nparagraph. In Effidit, we significantly expand the capacities of a writing\nassistant by providing functions in five categories: text completion, error\nchecking, text polishing, keywords to sentences (K2S), and cloud input methods\n(cloud IME). In the text completion category, Effidit supports generation-based\nsentence completion, retrieval-based sentence completion, and phrase\ncompletion. In contrast, many other writing assistants so far only provide one\nor two of the three functions. For text polishing, we have three functions:\n(context-aware) phrase polishing, sentence paraphrasing, and sentence\nexpansion, whereas many other writing assistants often support one or two\nfunctions in this category. The main contents of this report include major\nmodules of Effidit, methods for implementing these modules, and evaluation\nresults of some key methods.",
        "date": "2022-08-03"
    },
    "https://arxiv.org/abs/2002.08909": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "retriever reader",
            "realm",
            "not encoding knowledge in language model",
            "retrieval augmented lm",
            "knowledge augmented language models",
            "nlp google",
            "neural models for information retrieval"
        ],
        "title": "[2002.08909] REALM: Retrieval-Augmented Language Model Pre-Training",
        "summary": "Language model pre-training has been shown to capture a surprising amount of\nworld knowledge, crucial for NLP tasks such as question answering. However,\nthis knowledge is stored implicitly in the parameters of a neural network,\nrequiring ever-larger networks to cover more facts.\nTo capture knowledge in a more modular and interpretable way, we augment\nlanguage model pre-training with a latent knowledge retriever, which allows the\nmodel to retrieve and attend over documents from a large corpus such as\nWikipedia, used during pre-training, fine-tuning and inference. For the first\ntime, we show how to pre-train such a knowledge retriever in an unsupervised\nmanner, using masked language modeling as the learning signal and\nbackpropagating through a retrieval step that considers millions of documents.\nWe demonstrate the effectiveness of Retrieval-Augmented Language Model\npre-training (REALM) by fine-tuning on the challenging task of Open-domain\nQuestion Answering (Open-QA). We compare against state-of-the-art models for\nboth explicit and implicit knowledge storage on three popular Open-QA\nbenchmarks, and find that we outperform all previous methods by a significant\nmargin (4-16% absolute accuracy), while also providing qualitative benefits\nsuch as interpretability and modularity.",
        "date": "2020-02-10"
    },
    "https://arxiv.org/abs/2004.10151": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "meaning in nlp",
            "survey",
            "grounded language learning",
            "yoshua bengio"
        ],
        "title": "[2004.10151] Experience Grounds Language",
        "summary": "Successful linguistic communication relies on a shared experience of the\nworld, and it is this shared experience that makes utterances meaningful.\nDespite the incredible effectiveness of language processing models trained on\ntext alone, today's best systems still make mistakes that arise from a failure\nto relate language to the physical world it describes and to the social\ninteractions it facilitates.\nNatural Language Processing is a diverse field, and progress throughout its\ndevelopment has come from new representational theories, modeling techniques,\ndata collection paradigms, and tasks. We posit that the present success of\nrepresentation learning approaches trained on large text corpora can be deeply\nenriched from the parallel tradition of research on the contextual and social\nnature of language.\nIn this article, we consider work on the contextual foundations of language:\ngrounding, embodiment, and social interaction. We describe a brief history and\npossible progression of how contextual information can factor into our\nrepresentations, with an eye towards how this integration can move the field\nforward and where it is currently being pioneered. We believe this framing will\nserve as a roadmap for truly contextual language understanding.",
        "date": "2020-04-21"
    },
    "https://arxiv.org/abs/1908.01580": {
        "extra-tags": [],
        "tags": [
            "information bottleneck method",
            "arxiv doc",
            "backpropagation vs biology",
            "neuroscience and ai",
            "backpropagation",
            "information theory and deep learning"
        ],
        "title": "[1908.01580] The HSIC Bottleneck: Deep Learning without Back-Propagation",
        "summary": "We introduce the HSIC (Hilbert-Schmidt independence criterion) bottleneck for\ntraining deep neural networks. The HSIC bottleneck is an alternative to the\nconventional cross-entropy loss and backpropagation that has a number of\ndistinct advantages. It mitigates exploding and vanishing gradients, resulting\nin the ability to learn very deep networks without skip connections. There is\nno requirement for symmetric feedback or update locking. We find that the HSIC\nbottleneck provides performance on MNIST/FashionMNIST/CIFAR10 classification\ncomparable to backpropagation with a cross-entropy target, even when the system\nis not encouraged to make the output resemble the classification labels.\nAppending a single layer trained with SGD (without backpropagation) to reformat\nthe information further improves performance.",
        "date": "2019-08-05"
    },
    "https://arxiv.org/abs/1812.00417": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "snorkel",
            "weak supervision",
            "nlp using knowledge",
            "knowledge resources"
        ],
        "title": "[1812.00417] Snorkel DryBell: A Case Study in Deploying Weak Supervision at Industrial Scale",
        "summary": "Labeling training data is one of the most costly bottlenecks in developing\nmachine learning-based applications. We present a first-of-its-kind study\nshowing how existing knowledge resources from across an organization can be\nused as weak supervision in order to bring development time and cost down by an\norder of magnitude, and introduce Snorkel DryBell, a new weak supervision\nmanagement system for this setting. Snorkel DryBell builds on the Snorkel\nframework, extending it in three critical aspects: flexible, template-based\ningestion of diverse organizational knowledge, cross-feature production\nserving, and scalable, sampling-free execution. On three classification tasks\nat Google, we find that Snorkel DryBell creates classifiers of comparable\nquality to ones trained with tens of thousands of hand-labeled examples,\nconverts non-servable organizational resources to servable models for an\naverage 52% performance improvement, and executes over millions of data points\nin tens of minutes.",
        "date": "2018-12-02"
    },
    "https://arxiv.org/abs/1503.02531": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "knowledge distillation",
            "geoffrey hinton"
        ],
        "title": "[1503.02531] Distilling the Knowledge in a Neural Network",
        "summary": "A very simple way to improve the performance of almost any machine learning\nalgorithm is to train many different models on the same data and then to\naverage their predictions. Unfortunately, making predictions using a whole\nensemble of models is cumbersome and may be too computationally expensive to\nallow deployment to a large number of users, especially if the individual\nmodels are large neural nets. Caruana and his collaborators have shown that it\nis possible to compress the knowledge in an ensemble into a single model which\nis much easier to deploy and we develop this approach further using a different\ncompression technique. We achieve some surprising results on MNIST and we show\nthat we can significantly improve the acoustic model of a heavily used\ncommercial system by distilling the knowledge in an ensemble of models into a\nsingle model. We also introduce a new type of ensemble composed of one or more\nfull models and many specialist models which learn to distinguish fine-grained\nclasses that the full models confuse. Unlike a mixture of experts, these\nspecialist models can be trained rapidly and in parallel.",
        "date": "2015-03-09"
    },
    "https://arxiv.org/abs/2207.05221": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "catherine olsson",
            "language model",
            "christopher olah",
            "explainable nlp"
        ],
        "title": "[2207.05221] Language Models (Mostly) Know What They Know",
        "summary": "We study whether language models can evaluate the validity of their own\nclaims and predict which questions they will be able to answer correctly. We\nfirst show that larger models are well-calibrated on diverse multiple choice\nand true/false questions when they are provided in the right format. Thus we\ncan approach self-evaluation on open-ended sampling tasks by asking models to\nfirst propose answers, and then to evaluate the probability \"P(True)\" that\ntheir answers are correct. We find encouraging performance, calibration, and\nscaling for P(True) on a diverse array of tasks. Performance at self-evaluation\nfurther improves when we allow models to consider many of their own samples\nbefore predicting the validity of one specific possibility. Next, we\ninvestigate whether models can be trained to predict \"P(IK)\", the probability\nthat \"I know\" the answer to a question, without reference to any particular\nproposed answer. Models perform well at predicting P(IK) and partially\ngeneralize across tasks, though they struggle with calibration of P(IK) on new\ntasks. The predicted P(IK) probabilities also increase appropriately in the\npresence of relevant source materials in the context, and in the presence of\nhints towards the solution of mathematical word problems. We hope these\nobservations lay the groundwork for training more honest models, and for\ninvestigating how honesty generalizes to cases where models are trained on\nobjectives other than the imitation of human writing.",
        "date": "2022-07-11"
    },
    "https://arxiv.org/abs/1706.03610": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "question answering",
            "domain adaptation nlp",
            "biomedical nlp",
            "nlp low resource scenarios"
        ],
        "title": "[1706.03610] Neural Domain Adaptation for Biomedical Question Answering",
        "summary": "Factoid question answering (QA) has recently benefited from the development\nof deep learning (DL) systems. Neural network models outperform traditional\napproaches in domains where large datasets exist, such as SQuAD (ca. 100,000\nquestions) for Wikipedia articles. However, these systems have not yet been\napplied to QA in more specific domains, such as biomedicine, because datasets\nare generally too small to train a DL system from scratch. For example, the\nBioASQ dataset for biomedical QA comprises less then 900 factoid (single\nanswer) and list (multiple answers) QA instances. In this work, we adapt a\nneural QA system trained on a large open-domain dataset (SQuAD, source) to a\nbiomedical dataset (BioASQ, target) by employing various transfer learning\ntechniques. Our network architecture is based on a state-of-the-art QA system,\nextended with biomedical word embeddings and a novel mechanism to answer list\nquestions. In contrast to existing biomedical QA systems, our system does not\nrely on domain-specific ontologies, parsers or entity taggers, which are\nexpensive to create. Despite this fact, our systems achieve state-of-the-art\nresults on factoid questions and competitive results on list questions.",
        "date": "2017-06-12"
    },
    "https://arxiv.org/abs/2005.03675": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "graphs machine learning"
        ],
        "title": "[2005.03675] Machine Learning on Graphs: A Model and Comprehensive Taxonomy",
        "summary": "There has been a surge of recent interest in learning representations for\ngraph-structured data. Graph representation learning methods have generally\nfallen into three main categories, based on the availability of labeled data.\nThe first, network embedding (such as shallow graph embedding or graph\nauto-encoders), focuses on learning unsupervised representations of relational\nstructure. The second, graph regularized neural networks, leverages graphs to\naugment neural network losses with a regularization objective for\nsemi-supervised learning. The third, graph neural networks, aims to learn\ndifferentiable functions over discrete topologies with arbitrary structure.\nHowever, despite the popularity of these areas there has been surprisingly\nlittle work on unifying the three paradigms. Here, we aim to bridge the gap\nbetween graph neural networks, network embedding and graph regularization\nmodels. We propose a comprehensive taxonomy of representation learning methods\nfor graph-structured data, aiming to unify several disparate bodies of work.\nSpecifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which\ngeneralizes popular algorithms for semi-supervised learning on graphs (e.g.\nGraphSage, Graph Convolutional Networks, Graph Attention Networks), and\nunsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc)\ninto a single consistent approach. To illustrate the generality of this\napproach, we fit over thirty existing methods into this framework. We believe\nthat this unifying view both provides a solid foundation for understanding the\nintuition behind these methods, and enables future research in the area.",
        "date": "2020-05-07"
    },
    "https://arxiv.org/abs/2001.08053": {
        "extra-tags": [],
        "tags": [
            "named entity recognition",
            "patrick gallinari",
            "ner unseen mentions",
            "arxiv doc"
        ],
        "title": "[2001.08053] Contextualized Embeddings in Named-Entity Recognition: An Empirical Study on Generalization",
        "summary": "Contextualized embeddings use unsupervised language model pretraining to\ncompute word representations depending on their context. This is intuitively\nuseful for generalization, especially in Named-Entity Recognition where it is\ncrucial to detect mentions never seen during training. However, standard\nEnglish benchmarks overestimate the importance of lexical over contextual\nfeatures because of an unrealistic lexical overlap between train and test\nmentions. In this paper, we perform an empirical analysis of the generalization\ncapabilities of state-of-the-art contextualized embeddings by separating\nmentions by novelty and with out-of-domain evaluation. We show that they are\nparticularly beneficial for unseen mentions detection, especially\nout-of-domain. For models trained on CoNLL03, language model contextualization\nleads to a +1.2% maximal relative micro-F1 score increase in-domain against\n+13% out-of-domain on the WNUT dataset",
        "date": "2020-01-22"
    },
    "https://arxiv.org/abs/2112.07577": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nils reimers",
            "unsupervised domain adaptation nlp",
            "synthetic qa data",
            "dense passage retrieval",
            "gpl generative pseudo labeling"
        ],
        "title": "[2112.07577] GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval",
        "summary": "Dense retrieval approaches can overcome the lexical gap and lead to\nsignificantly improved search results. However, they require large amounts of\ntraining data which is not available for most domains. As shown in previous\nwork (Thakur et al., 2021b), the performance of dense retrievers severely\ndegrades under a domain shift. This limits the usage of dense retrieval\napproaches to only a few domains with large training datasets.\nIn this paper, we propose the novel unsupervised domain adaptation method\nGenerative Pseudo Labeling (GPL), which combines a query generator with pseudo\nlabeling from a cross-encoder. On six representative domain-specialized\ndatasets, we find the proposed GPL can outperform an out-of-the-box\nstate-of-the-art dense retrieval approach by up to 8.9 points nDCG@10. GPL\nrequires less (unlabeled) data from the target domain and is more robust in its\ntraining than previous methods.\nWe further investigate the role of six recent pre-training methods in the\nscenario of domain adaptation for retrieval tasks, where only three could yield\nimproved results. The best approach, TSDAE (Wang et al., 2021) can be combined\nwith GPL, yielding another average improvement of 1.0 points nDCG@10 across the\nsix tasks.",
        "date": "2021-12-14"
    },
    "https://arxiv.org/abs/2010.02666": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "knowledge distillation",
            "neural ranking models"
        ],
        "title": "[2010.02666] Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation",
        "summary": "Retrieval and ranking models are the backbone of many applications such as\nweb search, open domain QA, or text-based recommender systems. The latency of\nneural ranking models at query time is largely dependent on the architecture\nand deliberate choices by their designers to trade-off effectiveness for higher\nefficiency. This focus on low query latency of a rising number of efficient\nranking architectures make them feasible for production deployment. In machine\nlearning an increasingly common approach to close the effectiveness gap of more\nefficient models is to apply knowledge distillation from a large teacher model\nto a smaller student model. We find that different ranking architectures tend\nto produce output scores in different magnitudes. Based on this finding, we\npropose a cross-architecture training procedure with a margin focused loss\n(Margin-MSE), that adapts knowledge distillation to the varying score output\ndistributions of different BERT and non-BERT passage ranking architectures. We\napply the teachable information as additional fine-grained labels to existing\ntraining triples of the MSMARCO-Passage collection. We evaluate our procedure\nof distilling knowledge from state-of-the-art concatenated BERT models to four\ndifferent efficient architectures (TK, ColBERT, PreTT, and a BERT CLS dot\nproduct model). We show that across our evaluated architectures our Margin-MSE\nknowledge distillation significantly improves re-ranking effectiveness without\ncompromising their efficiency. Additionally, we show our general distillation\nmethod to improve nearest neighbor based index retrieval with the BERT dot\nproduct model, offering competitive results with specialized and much more\ncostly training methods. To benefit the community, we publish the teacher-score\ntraining files in a ready-to-use package.",
        "date": "2020-10-06"
    },
    "https://arxiv.org/abs/1511.03643": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "machines teaching machines",
            "ai facebook",
            "knowledge distillation"
        ],
        "title": "[1511.03643] Unifying distillation and privileged information",
        "summary": "Distillation (Hinton et al., 2015) and privileged information (Vapnik &\nIzmailov, 2015) are two techniques that enable machines to learn from other\nmachines. This paper unifies these two techniques into generalized\ndistillation, a framework to learn from multiple machines and data\nrepresentations. We provide theoretical and causal insight about the inner\nworkings of generalized distillation, extend it to unsupervised, semisupervised\nand multitask learning scenarios, and illustrate its efficacy on a variety of\nnumerical simulations on both synthetic and real-world data.",
        "date": "2015-11-11"
    },
    "https://arxiv.org/abs/1812.06280": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "ikuya yamada",
            "wikipedia2vec"
        ],
        "title": "[1812.06280] Wikipedia2Vec: An Efficient Toolkit for Learning and Visualizing the Embeddings of Words and Entities from Wikipedia",
        "summary": "The embeddings of entities in a large knowledge base (e.g., Wikipedia) are\nhighly beneficial for solving various natural language tasks that involve real\nworld knowledge. In this paper, we present Wikipedia2Vec, a Python-based\nopen-source tool for learning the embeddings of words and entities from\nWikipedia. The proposed tool enables users to learn the embeddings efficiently\nby issuing a single command with a Wikipedia dump file as an argument. We also\nintroduce a web-based demonstration of our tool that allows users to visualize\nand explore the learned embeddings. In our experiments, our tool achieved a\nstate-of-the-art result on the KORE entity relatedness dataset, and competitive\nresults on various standard benchmark datasets. Furthermore, our tool has been\nused as a key component in various recent studies. We publicize the source\ncode, demonstration, and the pretrained embeddings for 12 languages at\nhttps://wikipedia2vec.github.io/.",
        "date": "2018-12-15"
    },
    "https://arxiv.org/abs/1812.05944": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "tutorial",
            "similarity learning"
        ],
        "title": "[1812.05944] A Tutorial on Distance Metric Learning: Mathematical Foundations, Algorithms and Experiments",
        "summary": "Distance metric learning is a branch of machine learning that aims to learn\ndistances from the data. Distance metric learning can be useful to improve\nsimilarity learning algorithms, and also has applications in dimensionality\nreduction. This paper describes the distance metric learning problem and\nanalyzes its main mathematical foundations. In addition, it also discusses some\nof the most popular distance metric learning techniques used in classification,\nshowing their goals and the required information to understand and use them.\nFurthermore, some experiments to evaluate the performance of the different\nalgorithms are also provided. Finally, this paper discusses several\npossibilities of future work in this topic.",
        "date": "2018-12-14"
    },
    "https://arxiv.org/abs/2208.03299": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "gautier izacard",
            "nlp facebook",
            "few shot learning",
            "nlp ens",
            "retrieval augmented lm"
        ],
        "title": "[2208.03299] Few-shot Learning with Retrieval Augmented Language Model",
        "summary": "Large language models have shown impressive few-shot results on a wide range\nof tasks. However, when knowledge is key for such results, as is the case for\ntasks such as question answering and fact checking, massive parameter counts to\nstore knowledge seem to be needed. Retrieval augmented models are known to\nexcel at knowledge intensive tasks without the need for as many parameters, but\nit is unclear whether they work in few-shot settings. In this work we present\nAtlas, a carefully designed and pre-trained retrieval augmented language model\nable to learn knowledge intensive tasks with very few training examples. We\nperform evaluations on a wide range of tasks, including MMLU, KILT and\nNaturalQuestions, and study the impact of the content of the document index,\nshowing that it can easily be updated. Notably, Atlas reaches over 42\\%\naccuracy on Natural Questions using only 64 examples, outperforming a 540B\nparameters model by 3% despite having 50x fewer parameters.",
        "date": "2022-08-05"
    },
    "https://arxiv.org/abs/2008.11228": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "sbert fine tuning",
            "domain adaptation nlp",
            "sentence bert and domain adaptation",
            "universal sentence encoder"
        ],
        "title": "[2008.11228] A simple method for domain adaptation of sentence embeddings",
        "summary": "Pre-trained sentence embeddings have been shown to be very useful for a\nvariety of NLP tasks. Due to the fact that training such embeddings requires a\nlarge amount of data, they are commonly trained on a variety of text data. An\nadaptation to specific domains could improve results in many cases, but such a\nfinetuning is usually problem-dependent and poses the risk of over-adapting to\nthe data used for adaptation. In this paper, we present a simple universal\nmethod for finetuning Google's Universal Sentence Encoder (USE) using a Siamese\narchitecture. We demonstrate how to use this approach for a variety of data\nsets and present results on different data sets representing similar problems.\nThe approach is also compared to traditional finetuning on these data sets. As\na further advantage, the approach can be used for combining data sets with\ndifferent annotations. We also present an embedding finetuned on all data sets\nin parallel.",
        "date": "2020-08-25"
    },
    "https://arxiv.org/abs/1909.07606": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "knowledge graph deep learning",
            "language model",
            "bert",
            "nlp using knowledge graphs"
        ],
        "title": "[1909.07606] K-BERT: Enabling Language Representation with Knowledge Graph",
        "summary": "Pre-trained language representation models, such as BERT, capture a general\nlanguage representation from large-scale corpora, but lack domain-specific\nknowledge. When reading a domain text, experts make inferences with relevant\nknowledge. For machines to achieve this capability, we propose a\nknowledge-enabled language representation model (K-BERT) with knowledge graphs\n(KGs), in which triples are injected into the sentences as domain knowledge.\nHowever, too much knowledge incorporation may divert the sentence from its\ncorrect meaning, which is called knowledge noise (KN) issue. To overcome KN,\nK-BERT introduces soft-position and visible matrix to limit the impact of\nknowledge. K-BERT can easily inject domain knowledge into the models by\nequipped with a KG without pre-training by-self because it is capable of\nloading model parameters from the pre-trained BERT. Our investigation reveals\npromising results in twelve NLP tasks. Especially in domain-specific tasks\n(including finance, law, and medicine), K-BERT significantly outperforms BERT,\nwhich demonstrates that K-BERT is an excellent choice for solving the\nknowledge-driven problems that require experts.",
        "date": "2019-09-17"
    },
    "https://arxiv.org/abs/1909.02164": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "table based fact verification"
        ],
        "title": "[1909.02164] TabFact: A Large-scale Dataset for Table-based Fact Verification",
        "summary": "The problem of verifying whether a textual hypothesis holds based on the\ngiven evidence, also known as fact verification, plays an important role in the\nstudy of natural language understanding and semantic representation. However,\nexisting studies are mainly restricted to dealing with unstructured evidence\n(e.g., natural language sentences and documents, news, etc), while verification\nunder structured evidence, such as tables, graphs, and databases, remains\nunder-explored. This paper specifically aims to study the fact verification\ngiven semi-structured data as evidence. To this end, we construct a large-scale\ndataset called TabFact with 16k Wikipedia tables as the evidence for 118k\nhuman-annotated natural language statements, which are labeled as either\nENTAILED or REFUTED. TabFact is challenging since it involves both soft\nlinguistic reasoning and hard symbolic reasoning. To address these reasoning\nchallenges, we design two different models: Table-BERT and Latent Program\nAlgorithm (LPA). Table-BERT leverages the state-of-the-art pre-trained language\nmodel to encode the linearized tables and statements into continuous vectors\nfor verification. LPA parses statements into programs and executes them against\nthe tables to obtain the returned binary value for verification. Both methods\nachieve similar accuracy but still lag far behind human performance. We also\nperform a comprehensive analysis to demonstrate great future opportunities. The\ndata and code of the dataset are provided in\n\\url{https://github.com/wenhuchen/Table-Fact-Checking}.",
        "date": "2019-09-05"
    },
    "https://arxiv.org/abs/2102.07043": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "virtual knowledge graph",
            "ruslan salakhutdinov",
            "open domain question answering",
            "nlp google"
        ],
        "title": "[2102.07043] Reasoning Over Virtual Knowledge Bases With Open Predicate Relations",
        "summary": "We present the Open Predicate Query Language (OPQL); a method for\nconstructing a virtual KB (VKB) trained entirely from text. Large Knowledge\nBases (KBs) are indispensable for a wide-range of industry applications such as\nquestion answering and recommendation. Typically, KBs encode world knowledge in\na structured, readily accessible form derived from laborious human annotation\nefforts. Unfortunately, while they are extremely high precision, KBs are\ninevitably highly incomplete and automated methods for enriching them are far\ntoo inaccurate. Instead, OPQL constructs a VKB by encoding and indexing a set\nof relation mentions in a way that naturally enables reasoning and can be\ntrained without any structured supervision. We demonstrate that OPQL\noutperforms prior VKB methods on two different KB reasoning tasks and,\nadditionally, can be used as an external memory integrated into a language\nmodel (OPQL-LM) leading to improvements on two open-domain question answering\ntasks.",
        "date": "2021-02-14"
    },
    "https://arxiv.org/abs/2109.06270": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "task augmentation",
            "few shot learning",
            "quoc le",
            "self training"
        ],
        "title": "[2109.06270] STraTA: Self-Training with Task Augmentation for Better Few-shot Learning",
        "summary": "Despite their recent successes in tackling many NLP tasks, large-scale\npre-trained language models do not perform as well in few-shot settings where\nonly a handful of training examples are available. To address this shortcoming,\nwe propose STraTA, which stands for Self-Training with Task Augmentation, an\napproach that builds on two key ideas for effective leverage of unlabeled data.\nFirst, STraTA uses task augmentation, a novel technique that synthesizes a\nlarge amount of data for auxiliary-task fine-tuning from target-task unlabeled\ntexts. Second, STraTA performs self-training by further fine-tuning the strong\nbase model created by task augmentation on a broad distribution of\npseudo-labeled data. Our experiments demonstrate that STraTA can substantially\nimprove sample efficiency across 12 few-shot benchmarks. Remarkably, on the\nSST-2 sentiment dataset, STraTA, with only 8 training examples per class,\nachieves comparable results to standard fine-tuning with 67K training examples.\nOur analyses reveal that task augmentation and self-training are both\ncomplementary and independently effective.",
        "date": "2021-09-13"
    },
    "https://arxiv.org/abs/2209.01975": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "few shot learning",
            "deep active learning",
            "annotations ml",
            "prompted models",
            "allen institute for ai a2i"
        ],
        "title": "[2209.01975] Selective Annotation Makes Language Models Better Few-Shot Learners",
        "summary": "Many recent approaches to natural language tasks are built on the remarkable\nabilities of large language models. Large language models can perform\nin-context learning, where they learn a new task from a few task\ndemonstrations, without any parameter updates. This work examines the\nimplications of in-context learning for the creation of datasets for new\nnatural language tasks. Departing from recent in-context learning methods, we\nformulate an annotation-efficient, two-step framework: selective annotation\nthat chooses a pool of examples to annotate from unlabeled data in advance,\nfollowed by prompt retrieval that retrieves task examples from the annotated\npool at test time. Based on this framework, we propose an unsupervised,\ngraph-based selective annotation method, voke-k, to select diverse,\nrepresentative examples to annotate. Extensive experiments on 10 datasets\n(covering classification, commonsense reasoning, dialogue, and text/code\ngeneration) demonstrate that our selective annotation method improves the task\nperformance by a large margin. On average, vote-k achieves a 12.9%/11.4%\nrelative gain under an annotation budget of 18/100, as compared to randomly\nselecting examples to annotate. Compared to state-of-the-art supervised\nfinetuning approaches, it yields similar performance with 10-100x less\nannotation cost across 10 tasks. We further analyze the effectiveness of our\nframework in various scenarios: language models with varying sizes, alternative\nselective annotation methods, and cases where there is a test data domain\nshift. We hope that our studies will serve as a basis for data annotations as\nlarge language models are increasingly applied to new tasks. Our code is\navailable at https://github.com/HKUNLP/icl-selective-annotation.",
        "date": "2022-09-05"
    },
    "https://arxiv.org/abs/2107.00676": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "low resource languages",
            "multilingual language models",
            "bosch"
        ],
        "title": "[2107.00676] A Primer on Pretrained Multilingual Language Models",
        "summary": "Multilingual Language Models (MLLMs) such as mBERT, XLM, XLM-R, \\textit{etc.}\nhave emerged as a viable option for bringing the power of pretraining to a\nlarge number of languages. Given their success in zero shot transfer learning,\nthere has emerged a large body of work in (i) building bigger MLLMs covering a\nlarge number of languages (ii) creating exhaustive benchmarks covering a wider\nvariety of tasks and languages for evaluating MLLMs (iii) analysing the\nperformance of MLLMs on monolingual, zero shot crosslingual and bilingual tasks\n(iv) understanding the universal language patterns (if any) learnt by MLLMs and\n(v) augmenting the (often) limited capacity of MLLMs to improve their\nperformance on seen or even unseen languages. In this survey, we review the\nexisting literature covering the above broad areas of research pertaining to\nMLLMs. Based on our survey, we recommend some promising directions of future\nresearch.",
        "date": "2021-07-01"
    },
    "https://arxiv.org/abs/2205.00820": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "entities and lm",
            "kg aware low resource learning",
            "entity discovery and linking",
            "discute avec raphael",
            "bert kb",
            "sigir 2022"
        ],
        "title": "[2205.00820] Entity-aware Transformers for Entity Search",
        "summary": "Pre-trained language models such as BERT have been a key ingredient to\nachieve state-of-the-art results on a variety of tasks in natural language\nprocessing and, more recently, also in information retrieval.Recent research\neven claims that BERT is able to capture factual knowledge about entity\nrelations and properties, the information that is commonly obtained from\nknowledge graphs. This paper investigates the following question: Do BERT-based\nentity retrieval models benefit from additional entity information stored in\nknowledge graphs? To address this research question, we map entity embeddings\ninto the same input space as a pre-trained BERT model and inject these entity\nembeddings into the BERT model. This entity-enriched language model is then\nemployed on the entity retrieval task. We show that the entity-enriched BERT\nmodel improves effectiveness on entity-oriented queries over a regular BERT\nmodel, establishing a new state-of-the-art result for the entity retrieval\ntask, with substantial improvements for complex natural language queries and\nqueries requesting a list of entities with a certain property. Additionally, we\nshow that the entity information provided by our entity-enriched model\nparticularly helps queries related to less popular entities. Last, we observe\nempirically that the entity-enriched BERT models enable fine-tuning on limited\ntraining data, which otherwise would not be feasible due to the known\ninstabilities of BERT in few-sample fine-tuning, thereby contributing to\ndata-efficient training of BERT for entity search.",
        "date": "2022-05-02"
    },
    "https://arxiv.org/abs/1802.01528": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "jeremy howard",
            "matrix calculus"
        ],
        "title": "[1802.01528] The Matrix Calculus You Need For Deep Learning",
        "summary": "This paper is an attempt to explain all the matrix calculus you need in order\nto understand the training of deep neural networks. We assume no math knowledge\nbeyond what you learned in calculus 1, and provide links to help you refresh\nthe necessary math where needed. Note that you do not need to understand this\nmaterial before you start learning to train and use deep learning in practice;\nrather, this material is for those who are already familiar with the basics of\nneural networks, and wish to deepen their understanding of the underlying math.\nDon't worry if you get stuck at some point along the way---just go back and\nreread the previous section, and try writing down and working through some\nexamples. And if you're still stuck, we're happy to answer your questions in\nthe Theory category at forums.fast.ai. Note: There is a reference section at\nthe end of the paper summarizing all the key matrix calculus rules and\nterminology discussed here. See related articles at http://explained.ai",
        "date": "2018-02-05"
    },
    "https://arxiv.org/abs/2010.11967": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "knowledge graph",
            "pre trained language models",
            "language models as knowledge bases"
        ],
        "title": "[2010.11967] Language Models are Open Knowledge Graphs",
        "summary": "This paper shows how to construct knowledge graphs (KGs) from pre-trained\nlanguage models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs\n(e.g, Wikidata, NELL) are built in either a supervised or semi-supervised\nmanner, requiring humans to create knowledge. Recent deep language models\nautomatically acquire knowledge from large-scale corpora via pre-training. The\nstored knowledge has enabled the language models to improve downstream NLP\ntasks, e.g., answering questions, and writing code and articles. In this paper,\nwe propose an unsupervised method to cast the knowledge contained within\nlanguage models into KGs. We show that KGs are constructed with a single\nforward pass of the pre-trained language models (without fine-tuning) over the\ncorpora. We demonstrate the quality of the constructed KGs by comparing to two\nKGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual\nknowledge that is new in the existing KGs. Our code and KGs will be made\npublicly available.",
        "date": "2020-10-22"
    },
    "https://arxiv.org/abs/2011.06225": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "uncertainty quantification",
            "uncertainty in deep learning",
            "deep learning"
        ],
        "title": "[2011.06225] A Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and Challenges",
        "summary": "Uncertainty quantification (UQ) plays a pivotal role in reduction of\nuncertainties during both optimization and decision making processes. It can be\napplied to solve a variety of real-world applications in science and\nengineering. Bayesian approximation and ensemble learning techniques are two\nmost widely-used UQ methods in the literature. In this regard, researchers have\nproposed different UQ methods and examined their performance in a variety of\napplications such as computer vision (e.g., self-driving cars and object\ndetection), image processing (e.g., image restoration), medical image analysis\n(e.g., medical image classification and segmentation), natural language\nprocessing (e.g., text classification, social media texts and recidivism\nrisk-scoring), bioinformatics, etc. This study reviews recent advances in UQ\nmethods used in deep learning. Moreover, we also investigate the application of\nthese methods in reinforcement learning (RL). Then, we outline a few important\napplications of UQ methods. Finally, we briefly highlight the fundamental\nresearch challenges faced by UQ methods and discuss the future research\ndirections in this field.",
        "date": "2020-11-12"
    },
    "https://arxiv.org/abs/1906.02715": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "these irit renault biblio",
            "bert",
            "bertology",
            "nlp google",
            "tree embeddings",
            "geometry of language embeddings"
        ],
        "title": "[1906.02715] Visualizing and Measuring the Geometry of BERT",
        "summary": "Transformer architectures show significant promise for natural language\nprocessing. Given that a single pretrained model can be fine-tuned to perform\nwell on many different tasks, these networks appear to extract generally useful\nlinguistic features. A natural question is how such networks represent this\ninformation internally. This paper describes qualitative and quantitative\ninvestigations of one particularly effective model, BERT. At a high level,\nlinguistic features seem to be represented in separate semantic and syntactic\nsubspaces. We find evidence of a fine-grained geometric representation of word\nsenses. We also present empirical descriptions of syntactic representations in\nboth attention matrices and individual word embeddings, as well as a\nmathematical argument to explain the geometry of these representations.",
        "date": "2019-06-06"
    },
    "https://arxiv.org/abs/1905.12149": {
        "extra-tags": [],
        "tags": [
            "constraint satisfaction problem",
            "nn symbolic ai hybridation",
            "arxiv doc"
        ],
        "title": "[1905.12149] SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver",
        "summary": "Integrating logical reasoning within deep learning architectures has been a\nmajor goal of modern AI systems. In this paper, we propose a new direction\ntoward this goal by introducing a differentiable (smoothed) maximum\nsatisfiability (MAXSAT) solver that can be integrated into the loop of larger\ndeep learning systems. Our (approximate) solver is based upon a fast coordinate\ndescent approach to solving the semidefinite program (SDP) associated with the\nMAXSAT problem. We show how to analytically differentiate through the solution\nto this SDP and efficiently solve the associated backward pass. We demonstrate\nthat by integrating this solver into end-to-end learning systems, we can learn\nthe logical structure of challenging problems in a minimally supervised\nfashion. In particular, we show that we can learn the parity function using\nsingle-bit supervision (a traditionally hard task for deep networks) and learn\nhow to play 9x9 Sudoku solely from examples. We also solve a \"visual Sudok\"\nproblem that maps images of Sudoku puzzles to their associated logical\nsolutions by combining our MAXSAT solver with a traditional convolutional\narchitecture. Our approach thus shows promise in integrating logical structures\nwithin deep learning.",
        "date": "2019-05-29"
    },
    "https://arxiv.org/abs/1808.02590": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "ml google",
            "survey",
            "graph embeddings"
        ],
        "title": "[1808.02590] A Tutorial on Network Embeddings",
        "summary": "Network embedding methods aim at learning low-dimensional latent\nrepresentation of nodes in a network. These representations can be used as\nfeatures for a wide range of tasks on graphs such as classification,\nclustering, link prediction, and visualization. In this survey, we give an\noverview of network embeddings by summarizing and categorizing recent\nadvancements in this research field. We first discuss the desirable properties\nof network embeddings and briefly introduce the history of network embedding\nalgorithms. Then, we discuss network embedding methods under different\nscenarios, such as supervised versus unsupervised learning, learning embeddings\nfor homogeneous networks versus for heterogeneous networks, etc. We further\ndemonstrate the applications of network embeddings, and conclude the survey\nwith future work in this area.",
        "date": "2018-08-08"
    },
    "https://arxiv.org/abs/1907.03950": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nn symbolic ai hybridation",
            "consciousness prior",
            "chris manning"
        ],
        "title": "[1907.03950] Learning by Abstraction: The Neural State Machine",
        "summary": "We introduce the Neural State Machine, seeking to bridge the gap between the\nneural and symbolic views of AI and integrate their complementary strengths for\nthe task of visual reasoning. Given an image, we first predict a probabilistic\ngraph that represents its underlying semantics and serves as a structured world\nmodel. Then, we perform sequential reasoning over the graph, iteratively\ntraversing its nodes to answer a given question or draw a new inference. In\ncontrast to most neural architectures that are designed to closely interact\nwith the raw sensory data, our model operates instead in an abstract latent\nspace, by transforming both the visual and linguistic modalities into semantic\nconcept-based representations, thereby achieving enhanced transparency and\nmodularity. We evaluate our model on VQA-CP and GQA, two recent VQA datasets\nthat involve compositionality, multi-step inference and diverse reasoning\nskills, achieving state-of-the-art results in both cases. We provide further\nexperiments that illustrate the model's strong generalization capacity across\nmultiple dimensions, including novel compositions of concepts, changes in the\nanswer distribution, and unseen linguistic structures, demonstrating the\nqualities and efficacy of our approach.",
        "date": "2019-07-09"
    },
    "https://arxiv.org/abs/1909.10506": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "two tower algorithm",
            "entity discovery and linking",
            "unsupervised negative mining",
            "hard negative mining",
            "nlp google"
        ],
        "title": "[1909.10506] Learning Dense Representations for Entity Retrieval",
        "summary": "We show that it is feasible to perform entity linking by training a dual\nencoder (two-tower) model that encodes mentions and entities in the same dense\nvector space, where candidate entities are retrieved by approximate nearest\nneighbor search. Unlike prior work, this setup does not rely on an alias table\nfollowed by a re-ranker, and is thus the first fully learned entity retrieval\nmodel. We show that our dual encoder, trained using only anchor-text links in\nWikipedia, outperforms discrete alias table and BM25 baselines, and is\ncompetitive with the best comparable results on the standard TACKBP-2010\ndataset. In addition, it can retrieve candidates extremely fast, and\ngeneralizes well to a new dataset derived from Wikinews. On the modeling side,\nwe demonstrate the dramatic value of an unsupervised negative mining algorithm\nfor this task.",
        "date": "2019-09-23"
    },
    "https://arxiv.org/abs/2210.13952": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp ibm",
            "text to kg",
            "knowledge extraction"
        ],
        "title": "[2210.13952] KnowGL: Knowledge Generation and Linking from Text",
        "summary": "We propose KnowGL, a tool that allows converting text into structured\nrelational data represented as a set of ABox assertions compliant with the TBox\nof a given Knowledge Graph (KG), such as Wikidata. We address this problem as a\nsequence generation task by leveraging pre-trained sequence-to-sequence\nlanguage models, e.g. BART. Given a sentence, we fine-tune such models to\ndetect pairs of entity mentions and jointly generate a set of facts consisting\nof the full set of semantic annotations for a KG, such as entity labels, entity\ntypes, and their relationships. To showcase the capabilities of our tool, we\nbuild a web application consisting of a set of UI widgets that help users to\nnavigate through the semantic data extracted from a given input text. We make\nthe KnowGL model available at https://huggingface.co/ibm/knowgl-large.",
        "date": "2022-10-25"
    },
    "https://arxiv.org/abs/1011.4088": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "conditional random field",
            "andrew mccallum"
        ],
        "title": "[1011.4088] An Introduction to Conditional Random Fields",
        "summary": "Often we wish to predict a large number of variables that depend on each\nother as well as on other observed variables. Structured prediction methods are\nessentially a combination of classification and graphical modeling, combining\nthe ability of graphical models to compactly model multivariate data with the\nability of classification methods to perform prediction using large sets of\ninput features. This tutorial describes conditional random fields, a popular\nprobabilistic method for structured prediction. CRFs have seen wide application\nin natural language processing, computer vision, and bioinformatics. We\ndescribe methods for inference and parameter estimation for CRFs, including\npractical issues for implementing large scale CRFs. We do not assume previous\nknowledge of graphical modeling, so this tutorial is intended to be useful to\npractitioners in a wide variety of fields.",
        "date": "2010-11-17"
    },
    "https://arxiv.org/abs/2007.00814": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "colbert"
        ],
        "title": "[2007.00814] Relevance-guided Supervision for OpenQA with ColBERT",
        "summary": "Systems for Open-Domain Question Answering (OpenQA) generally depend on a\nretriever for finding candidate passages in a large corpus and a reader for\nextracting answers from those passages. In much recent work, the retriever is a\nlearned component that uses coarse-grained vector representations of questions\nand passages. We argue that this modeling choice is insufficiently expressive\nfor dealing with the complexity of natural language questions. To address this,\nwe define ColBERT-QA, which adapts the scalable neural retrieval model ColBERT\nto OpenQA. ColBERT creates fine-grained interactions between questions and\npassages. We propose an efficient weak supervision strategy that iteratively\nuses ColBERT to create its own training data. This greatly improves OpenQA\nretrieval on Natural Questions, SQuAD, and TriviaQA, and the resulting system\nattains state-of-the-art extractive OpenQA performance on all three datasets.",
        "date": "2020-07-01"
    },
    "https://arxiv.org/abs/2210.16637": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "cluster analysis",
            "zero shot",
            "pre trained language models"
        ],
        "title": "[2210.16637] Beyond Prompting: Making Pre-trained Language Models Better Zero-shot Learners by Clustering Representations",
        "summary": "Recent work has demonstrated that pre-trained language models (PLMs) are\nzero-shot learners. However, most existing zero-shot methods involve heavy\nhuman engineering or complicated self-training pipelines, hindering their\napplication to new situations. In this work, we show that zero-shot text\nclassification can be improved simply by clustering texts in the embedding\nspaces of PLMs. Specifically, we fit the unlabeled texts with a Bayesian\nGaussian Mixture Model after initializing cluster positions and shapes using\nclass names. Despite its simplicity, this approach achieves superior or\ncomparable performance on both topic and sentiment classification datasets and\noutperforms prior works significantly on unbalanced datasets. We further\nexplore the applicability of our clustering approach by evaluating it on 14\ndatasets with more diverse topics, text lengths, and numbers of classes. Our\napproach achieves an average of 20% absolute improvement over prompt-based\nzero-shot learning. Finally, we compare different PLM embedding spaces and find\nthat texts are well-clustered by topics even if the PLM is not explicitly\npre-trained to generate meaningful sentence embeddings. This work indicates\nthat PLM embeddings can categorize texts without task-specific fine-tuning,\nthus providing a new way to analyze and utilize their knowledge and zero-shot\nlearning ability.",
        "date": "2022-10-29"
    },
    "https://arxiv.org/abs/2109.06304": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "phrase embeddings",
            "bert"
        ],
        "title": "[2109.06304] Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration",
        "summary": "Phrase representations derived from BERT often do not exhibit complex phrasal\ncompositionality, as the model relies instead on lexical similarity to\ndetermine semantic relatedness. In this paper, we propose a contrastive\nfine-tuning objective that enables BERT to produce more powerful phrase\nembeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal\nparaphrases, which is automatically generated using a paraphrase generation\nmodel, as well as a large-scale dataset of phrases in context mined from the\nBooks3 corpus. Phrase-BERT outperforms baselines across a variety of\nphrase-level similarity tasks, while also demonstrating increased lexical\ndiversity between nearest neighbors in the vector space. Finally, as a case\nstudy, we show that Phrase-BERT embeddings can be easily integrated with a\nsimple autoencoder to build a phrase-based neural topic model that interprets\ntopics as mixtures of words and phrases by performing a nearest neighbor search\nin the embedding space. Crowdsourced evaluations demonstrate that this\nphrase-based topic model produces more coherent and meaningful topics than\nbaseline word and phrase-level topic models, further validating the utility of\nPhrase-BERT.",
        "date": "2021-09-13"
    },
    "https://arxiv.org/abs/2004.14958": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "cross lingual nlp",
            "ml evaluation",
            "sebastian ruder",
            "unsupervised machine translation"
        ],
        "title": "[2004.14958] A Call for More Rigor in Unsupervised Cross-lingual Learning",
        "summary": "We review motivations, definition, approaches, and methodology for\nunsupervised cross-lingual learning and call for a more rigorous position in\neach of them. An existing rationale for such research is based on the lack of\nparallel data for many of the world's languages. However, we argue that a\nscenario without any parallel data and abundant monolingual data is unrealistic\nin practice. We also discuss different training signals that have been used in\nprevious work, which depart from the pure unsupervised setting. We then\ndescribe common methodological issues in tuning and evaluation of unsupervised\ncross-lingual models and present best practices. Finally, we provide a unified\noutlook for different types of research in this area (i.e., cross-lingual word\nembeddings, deep multilingual pretraining, and unsupervised machine\ntranslation) and argue for comparable evaluation of these models.",
        "date": "2020-04-30"
    },
    "https://arxiv.org/abs/1905.11852": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp facebook",
            "explainable nlp",
            "ludovic denoyer"
        ],
        "title": "[1905.11852] EDUCE: Explaining model Decisions through Unsupervised Concepts Extraction",
        "summary": "Providing explanations along with predictions is crucial in some text\nprocessing tasks. Therefore, we propose a new self-interpretable model that\nperforms output prediction and simultaneously provides an explanation in terms\nof the presence of particular concepts in the input. To do so, our model's\nprediction relies solely on a low-dimensional binary representation of the\ninput, where each feature denotes the presence or absence of concepts. The\npresence of a concept is decided from an excerpt i.e. a small sequence of\nconsecutive words in the text. Relevant concepts for the prediction task at\nhand are automatically defined by our model, avoiding the need for\nconcept-level annotations. To ease interpretability, we enforce that for each\nconcept, the corresponding excerpts share similar semantics and are\ndifferentiable from each others. We experimentally demonstrate the relevance of\nour approach on text classification and multi-sentiment analysis tasks.",
        "date": "2019-05-28"
    },
    "https://arxiv.org/abs/1803.07828": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "knowledge graph embeddings"
        ],
        "title": "[1803.07828] Expeditious Generation of Knowledge Graph Embeddings",
        "summary": "Knowledge Graph Embedding methods aim at representing entities and relations\nin a knowledge base as points or vectors in a continuous vector space. Several\napproaches using embeddings have shown promising results on tasks such as link\nprediction, entity recommendation, question answering, and triplet\nclassification. However, only a few methods can compute low-dimensional\nembeddings of very large knowledge bases without needing state-of-the-art\ncomputational resources. In this paper, we propose KG2Vec, a simple and fast\napproach to Knowledge Graph Embedding based on the skip-gram model. Instead of\nusing a predefined scoring function, we learn it relying on Long Short-Term\nMemories. We show that our embeddings achieve results comparable with the most\nscalable approaches on knowledge graph completion as well as on a new metric.\nYet, KG2Vec can embed large graphs in lesser time by processing more than 250\nmillion triples in less than 7 hours on common hardware.",
        "date": "2018-03-21"
    },
    "https://arxiv.org/abs/1802.07569": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "catastrophic forgetting",
            "continual learning"
        ],
        "title": "[1802.07569] Continual Lifelong Learning with Neural Networks: A Review",
        "summary": "Humans and animals have the ability to continually acquire, fine-tune, and\ntransfer knowledge and skills throughout their lifespan. This ability, referred\nto as lifelong learning, is mediated by a rich set of neurocognitive mechanisms\nthat together contribute to the development and specialization of our\nsensorimotor skills as well as to long-term memory consolidation and retrieval.\nConsequently, lifelong learning capabilities are crucial for autonomous agents\ninteracting in the real world and processing continuous streams of information.\nHowever, lifelong learning remains a long-standing challenge for machine\nlearning and neural network models since the continual acquisition of\nincrementally available information from non-stationary data distributions\ngenerally leads to catastrophic forgetting or interference. This limitation\nrepresents a major drawback for state-of-the-art deep neural network models\nthat typically learn representations from stationary batches of training data,\nthus without accounting for situations in which information becomes\nincrementally available over time. In this review, we critically summarize the\nmain challenges linked to lifelong learning for artificial learning systems and\ncompare existing neural network approaches that alleviate, to different\nextents, catastrophic forgetting. We discuss well-established and emerging\nresearch motivated by lifelong learning factors in biological systems such as\nstructural plasticity, memory replay, curriculum and transfer learning,\nintrinsic motivation, and multisensory integration.",
        "date": "2018-02-21"
    },
    "https://arxiv.org/abs/2202.10054": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "fine tuning"
        ],
        "title": "[2202.10054] Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution",
        "summary": "When transferring a pretrained model to a downstream task, two popular\nmethods are full fine-tuning (updating all the model parameters) and linear\nprobing (updating only the last linear layer -- the \"head\"). It is well known\nthat fine-tuning leads to better accuracy in-distribution (ID). However, in\nthis paper, we find that fine-tuning can achieve worse accuracy than linear\nprobing out-of-distribution (OOD) when the pretrained features are good and the\ndistribution shift is large. On 10 distribution shift datasets\n(Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR $\\to$ STL, CIFAR10.1, FMoW,\nImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on\naverage 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We\nshow theoretically that this tradeoff between ID and OOD accuracy arises even\nin a simple setting: fine-tuning overparameterized two-layer linear networks.\nWe prove that the OOD error of fine-tuning is high when we initialize with a\nfixed or random head -- this is because while fine-tuning learns the head, the\nlower layers of the neural network change simultaneously and distort the\npretrained features. Our analysis suggests that the easy two-step strategy of\nlinear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning\nheuristic, combines the benefits of both fine-tuning and linear probing.\nEmpirically, LP-FT outperforms both fine-tuning and linear probing on the above\ndatasets (1% better ID, 10% better OOD than full fine-tuning).",
        "date": "2022-02-21"
    },
    "https://arxiv.org/abs/2002.11402": {
        "extra-tags": [],
        "tags": [
            "named entity recognition",
            "arxiv doc",
            "bert",
            "conditional random field",
            "wikipedia"
        ],
        "title": "[2002.11402] Detecting Potential Topics In News Using BERT, CRF and Wikipedia",
        "summary": "For a news content distribution platform like Dailyhunt, Named Entity\nRecognition is a pivotal task for building better user recommendation and\nnotification algorithms. Apart from identifying names, locations, organisations\nfrom the news for 13+ Indian languages and use them in algorithms, we also need\nto identify n-grams which do not necessarily fit in the definition of\nNamed-Entity, yet they are important. For example, \"me too movement\", \"beef\nban\", \"alwar mob lynching\". In this exercise, given an English language text,\nwe are trying to detect case-less n-grams which convey important information\nand can be used as topics and/or hashtags for a news. Model is built using\nWikipedia titles data, private English news corpus and BERT-Multilingual\npre-trained model, Bi-GRU and CRF architecture. It shows promising results when\ncompared with industry best Flair, Spacy and Stanford-caseless-NER in terms of\nF1 and especially Recall.",
        "date": "2020-02-26"
    },
    "https://arxiv.org/abs/2008.12813": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "emnlp 2021",
            "nlp microsoft",
            "transformer based kg embeddings",
            "link prediction"
        ],
        "title": "[2008.12813] HittER: Hierarchical Transformers for Knowledge Graph Embeddings",
        "summary": "This paper examines the challenging problem of learning representations of\nentities and relations in a complex multi-relational knowledge graph. We\npropose HittER, a Hierarchical Transformer model to jointly learn\nEntity-relation composition and Relational contextualization based on a source\nentity's neighborhood. Our proposed model consists of two different Transformer\nblocks: the bottom block extracts features of each entity-relation pair in the\nlocal neighborhood of the source entity and the top block aggregates the\nrelational information from outputs of the bottom block. We further design a\nmasked entity prediction task to balance information from the relational\ncontext and the source entity itself. Experimental results show that HittER\nachieves new state-of-the-art results on multiple link prediction datasets. We\nadditionally propose a simple approach to integrate HittER into BERT and\ndemonstrate its effectiveness on two Freebase factoid question answering\ndatasets.",
        "date": "2020-08-28"
    },
    "https://arxiv.org/abs/2001.09522": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp microsoft",
            "text aware kg embedding",
            "taxonomy expansion task",
            "taxonomies",
            "knowledge graph completion",
            "discute avec raphael",
            "graph neural networks",
            "thewebconf 2020",
            "microsoft research"
        ],
        "title": "[2001.09522] TaxoExpan: Self-supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network",
        "summary": "Taxonomies consist of machine-interpretable semantics and provide valuable\nknowledge for many web applications. For example, online retailers (e.g.,\nAmazon and eBay) use taxonomies for product recommendation, and web search\nengines (e.g., Google and Bing) leverage taxonomies to enhance query\nunderstanding. Enormous efforts have been made on constructing taxonomies\neither manually or semi-automatically. However, with the fast-growing volume of\nweb content, existing taxonomies will become outdated and fail to capture\nemerging knowledge. Therefore, in many applications, dynamic expansions of an\nexisting taxonomy are in great demand. In this paper, we study how to expand an\nexisting taxonomy by adding a set of new concepts. We propose a novel\nself-supervised framework, named TaxoExpan, which automatically generates a set\nof <query concept, anchor concept> pairs from the existing taxonomy as training\ndata. Using such self-supervision data, TaxoExpan learns a model to predict\nwhether a query concept is the direct hyponym of an anchor concept. We develop\ntwo innovative techniques in TaxoExpan: (1) a position-enhanced graph neural\nnetwork that encodes the local structure of an anchor concept in the existing\ntaxonomy, and (2) a noise-robust training objective that enables the learned\nmodel to be insensitive to the label noise in the self-supervision data.\nExtensive experiments on three large-scale datasets from different domains\ndemonstrate both the effectiveness and the efficiency of TaxoExpan for taxonomy\nexpansion.",
        "date": "2020-01-26"
    },
    "https://arxiv.org/abs/2010.05234": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "graph neural networks",
            "sample code",
            "tutorial"
        ],
        "title": "[2010.05234] A Practical Guide to Graph Neural Networks",
        "summary": "Graph neural networks (GNNs) have recently grown in popularity in the field\nof artificial intelligence due to their unique ability to ingest relatively\nunstructured data types as input data. Although some elements of the GNN\narchitecture are conceptually similar in operation to traditional neural\nnetworks (and neural network variants), other elements represent a departure\nfrom traditional deep learning techniques. This tutorial exposes the power and\nnovelty of GNNs to the average deep learning enthusiast by collating and\npresenting details on the motivations, concepts, mathematics, and applications\nof the most common types of GNNs. Importantly, we present this tutorial\nconcisely, alongside worked code examples, and at an introductory pace, thus\nproviding a practical and accessible guide to understanding and using GNNs.",
        "date": "2020-10-11"
    },
    "https://arxiv.org/abs/2109.08133": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp princeton",
            "emnlp 2021",
            "phrase embeddings",
            "discute avec raphael",
            "dense passage retrieval"
        ],
        "title": "[2109.08133] Phrase Retrieval Learns Passage Retrieval, Too",
        "summary": "Dense retrieval methods have shown great promise over sparse retrieval\nmethods in a range of NLP problems. Among them, dense phrase retrieval-the most\nfine-grained retrieval unit-is appealing because phrases can be directly used\nas the output for question answering and slot filling tasks. In this work, we\nfollow the intuition that retrieving phrases naturally entails retrieving\nlarger text blocks and study whether phrase retrieval can serve as the basis\nfor coarse-level retrieval including passages and documents. We first observe\nthat a dense phrase-retrieval system, without any retraining, already achieves\nbetter passage retrieval accuracy (+3-5% in top-5 accuracy) compared to passage\nretrievers, which also helps achieve superior end-to-end QA performance with\nfewer passages. Then, we provide an interpretation for why phrase-level\nsupervision helps learn better fine-grained entailment compared to\npassage-level supervision, and also show that phrase retrieval can be improved\nto achieve competitive performance in document-retrieval tasks such as entity\nlinking and knowledge-grounded dialogue. Finally, we demonstrate how phrase\nfiltering and vector quantization can reduce the size of our index by 4-10x,\nmaking dense phrase retrieval a practical and versatile solution in\nmulti-granularity retrieval.",
        "date": "2021-09-16"
    },
    "https://arxiv.org/abs/2205.15952": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "knowledge graph deep learning",
            "sbert",
            "question answering"
        ],
        "title": "[2205.15952] Knowledge Graph -- Deep Learning: A Case Study in Question Answering in Aviation Safety Domain",
        "summary": "In the commercial aviation domain, there are a large number of documents,\nlike, accident reports (NTSB, ASRS) and regulatory directives (ADs). There is a\nneed for a system to access these diverse repositories efficiently in order to\nservice needs in the aviation industry, like maintenance, compliance, and\nsafety. In this paper, we propose a Knowledge Graph (KG) guided Deep Learning\n(DL) based Question Answering (QA) system for aviation safety. We construct a\nKnowledge Graph from Aircraft Accident reports and contribute this resource to\nthe community of researchers. The efficacy of this resource is tested and\nproved by the aforesaid QA system. Natural Language Queries constructed from\nthe documents mentioned above are converted into SPARQL (the interface language\nof the RDF graph database) queries and answered. On the DL side, we have two\ndifferent QA models: (i) BERT QA which is a pipeline of Passage Retrieval\n(Sentence-BERT based) and Question Answering (BERT based), and (ii) the\nrecently released GPT-3. We evaluate our system on a set of queries created\nfrom the accident reports. Our combined QA system achieves 9.3% increase in\naccuracy over GPT-3 and 40.3% increase over BERT QA. Thus, we infer that KG-DL\nperforms better than either singly.",
        "date": "2022-05-31"
    },
    "https://arxiv.org/abs/2211.09110": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "language model",
            "chris manning"
        ],
        "title": "[2211.09110] Holistic Evaluation of Language Models",
        "summary": "Language models (LMs) are becoming the foundation for almost all major\nlanguage technologies, but their capabilities, limitations, and risks are not\nwell understood. We present Holistic Evaluation of Language Models (HELM) to\nimprove the transparency of language models. First, we taxonomize the vast\nspace of potential scenarios (i.e. use cases) and metrics (i.e. desiderata)\nthat are of interest for LMs. Then we select a broad subset based on coverage\nand feasibility, noting what's missing or underrepresented (e.g. question\nanswering for neglected English dialects, metrics for trustworthiness). Second,\nwe adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration,\nrobustness, fairness, bias, toxicity, and efficiency) for each of 16 core\nscenarios when possible (87.5% of the time). This ensures metrics beyond\naccuracy don't fall to the wayside, and that trade-offs are clearly exposed. We\nalso perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze\nspecific aspects (e.g. reasoning, disinformation). Third, we conduct a\nlarge-scale evaluation of 30 prominent language models (spanning open,\nlimited-access, and closed models) on all 42 scenarios, 21 of which were not\npreviously used in mainstream LM evaluation. Prior to HELM, models on average\nwere evaluated on just 17.9% of the core HELM scenarios, with some prominent\nmodels not sharing a single scenario in common. We improve this to 96.0%: now\nall 30 models have been densely benchmarked on the same core scenarios and\nmetrics under standardized conditions. Our evaluation surfaces 25 top-level\nfindings. For full transparency, we release all raw model prompts and\ncompletions publicly for further analysis, as well as a general modular\ntoolkit. We intend for HELM to be a living benchmark for the community,\ncontinuously updated with new scenarios, metrics, and models.",
        "date": "2022-11-16"
    },
    "https://arxiv.org/abs/2002.01808": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp microsoft",
            "adapter modules finetuning",
            "language models knowledge"
        ],
        "title": "[2002.01808] K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters",
        "summary": "We study the problem of injecting knowledge into large pre-trained models\nlike BERT and RoBERTa. Existing methods typically update the original\nparameters of pre-trained models when injecting knowledge. However, when\nmultiple kinds of knowledge are injected, the historically injected knowledge\nwould be flushed away. To address this, we propose K-Adapter, a framework that\nretains the original parameters of the pre-trained model fixed and supports the\ndevelopment of versatile knowledge-infused model. Taking RoBERTa as the\nbackbone model, K-Adapter has a neural adapter for each kind of infused\nknowledge, like a plug-in connected to RoBERTa. There is no information flow\nbetween different adapters, thus multiple adapters can be efficiently trained\nin a distributed way. As a case study, we inject two kinds of knowledge in this\nwork, including (1) factual knowledge obtained from automatically aligned\ntext-triplets on Wikipedia and Wikidata and (2) linguistic knowledge obtained\nvia dependency parsing. Results on three knowledge-driven tasks, including\nrelation classification, entity typing, and question answering, demonstrate\nthat each adapter improves the performance and the combination of both adapters\nbrings further improvements. Further analysis indicates that K-Adapter captures\nversatile knowledge than RoBERTa.",
        "date": "2020-02-05"
    },
    "https://arxiv.org/abs/2212.01340": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "benchmark",
            "nlp ibm",
            "nlp stanford",
            "information retrieval"
        ],
        "title": "[2212.01340] Moving Beyond Downstream Task Accuracy for Information Retrieval Benchmarking",
        "summary": "Neural information retrieval (IR) systems have progressed rapidly in recent\nyears, in large part due to the release of publicly available benchmarking\ntasks. Unfortunately, some dimensions of this progress are illusory: the\nmajority of the popular IR benchmarks today focus exclusively on downstream\ntask accuracy and thus conceal the costs incurred by systems that trade away\nefficiency for quality. Latency, hardware cost, and other efficiency\nconsiderations are paramount to the deployment of IR systems in user-facing\nsettings. We propose that IR benchmarks structure their evaluation methodology\nto include not only metrics of accuracy, but also efficiency considerations\nsuch as a query latency and the corresponding cost budget for a reproducible\nhardware setting. For the popular IR benchmarks MS MARCO and XOR-TyDi, we show\nhow the best choice of IR system varies according to how these efficiency\nconsiderations are chosen and weighed. We hope that future benchmarks will\nadopt these guidelines toward more holistic IR evaluation.",
        "date": "2022-12-02"
    },
    "https://arxiv.org/abs/2010.00711": {
        "extra-tags": [],
        "tags": [
            "confiance ai",
            "arxiv doc",
            "survey",
            "nlp ibm",
            "explainable nlp"
        ],
        "title": "[2010.00711] A Survey of the State of Explainable AI for Natural Language Processing",
        "summary": "Recent years have seen important advances in the quality of state-of-the-art\nmodels, but this has come at the expense of models becoming less interpretable.\nThis survey presents an overview of the current state of Explainable AI (XAI),\nconsidered within the domain of Natural Language Processing (NLP). We discuss\nthe main categorization of explanations, as well as the various ways\nexplanations can be arrived at and visualized. We detail the operations and\nexplainability techniques currently available for generating explanations for\nNLP model predictions, to serve as a resource for model developers in the\ncommunity. Finally, we point out the current gaps and encourage directions for\nfuture work in this important research area.",
        "date": "2020-10-01"
    },
    "https://arxiv.org/abs/2008.07267": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "uncertainty in deep learning",
            "nlp text classification",
            "active learning"
        ],
        "title": "[2008.07267] A Survey of Active Learning for Text Classification using Deep Neural Networks",
        "summary": "Natural language processing (NLP) and neural networks (NNs) have both\nundergone significant changes in recent years. For active learning (AL)\npurposes, NNs are, however, less commonly used -- despite their current\npopularity. By using the superior text classification performance of NNs for\nAL, we can either increase a model's performance using the same amount of data\nor reduce the data and therefore the required annotation efforts while keeping\nthe same performance. We review AL for text classification using deep neural\nnetworks (DNNs) and elaborate on two main causes which used to hinder the\nadoption: (a) the inability of NNs to provide reliable uncertainty estimates,\non which the most commonly used query strategies rely, and (b) the challenge of\ntraining DNNs on small data. To investigate the former, we construct a taxonomy\nof query strategies, which distinguishes between data-based, model-based, and\nprediction-based instance selection, and investigate the prevalence of these\nclasses in recent research. Moreover, we review recent NN-based advances in NLP\nlike word embeddings or language models in the context of (D)NNs, survey the\ncurrent state-of-the-art at the intersection of AL, text classification, and\nDNNs and relate recent advances in NLP to AL. Finally, we analyze recent work\nin AL for text classification, connect the respective query strategies to the\ntaxonomy, and outline commonalities and shortcomings. As a result, we highlight\ngaps in current research and present open research questions.",
        "date": "2020-08-17"
    },
    "https://arxiv.org/abs/2010.03496": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "attention knowledge graphs",
            "blp",
            "thewebconf 2021",
            "text aware kg embedding",
            "entity embeddings",
            "discute avec raphael",
            "link prediction",
            "good"
        ],
        "title": "[2010.03496] Inductive Entity Representations from Text via Link Prediction",
        "summary": "We present a method for learning representations of entities, that uses a\nTransformer-based architecture as an entity encoder, and link prediction\ntraining on a knowledge graph with textual entity descriptions. We demonstrate\nthat our approach can be applied effectively for link prediction in different\ninductive settings involving entities not seen during training, outperforming\nrelated state-of-the-art methods (22% MRR improvement on average). We provide\nevidence that the learned representations transfer to other tasks that do not\nrequire fine-tuning the entity encoder. In an entity classification task we\nobtain an average improvement of 16% accuracy compared with baselines that also\nemploy pre-trained models. For an information retrieval task, significant\nimprovements of up to 8.8% in NDCG@10 were obtained for natural language\nqueries.",
        "date": "2020-10-07"
    },
    "https://arxiv.org/abs/1906.07241": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "kg and nlp",
            "knowledge graph augmented language models",
            "allen institute for ai a2i",
            "kd mkb biblio"
        ],
        "title": "[1906.07241] Barack's Wife Hillary: Using Knowledge-Graphs for Fact-Aware Language Modeling",
        "summary": "Modeling human language requires the ability to not only generate fluent text\nbut also encode factual knowledge. However, traditional language models are\nonly capable of remembering facts seen at training time, and often have\ndifficulty recalling them. To address this, we introduce the knowledge graph\nlanguage model (KGLM), a neural language model with mechanisms for selecting\nand copying facts from a knowledge graph that are relevant to the context.\nThese mechanisms enable the model to render information it has never seen\nbefore, as well as generate out-of-vocabulary tokens. We also introduce the\nLinked WikiText-2 dataset, a corpus of annotated text aligned to the Wikidata\nknowledge graph whose contents (roughly) match the popular WikiText-2\nbenchmark. In experiments, we demonstrate that the KGLM achieves significantly\nbetter performance than a strong baseline language model. We additionally\ncompare different language model's ability to complete sentences requiring\nfactual knowledge, showing that the KGLM outperforms even very large language\nmodels in generating facts.",
        "date": "2019-06-17"
    },
    "https://arxiv.org/abs/2208.01066": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp stanford",
            "prompted models",
            "attention is all you need"
        ],
        "title": "[2208.01066] What Can Transformers Learn In-Context? A Case Study of Simple Function Classes",
        "summary": "In-context learning refers to the ability of a model to condition on a prompt\nsequence consisting of in-context examples (input-output pairs corresponding to\nsome task) along with a new query input, and generate the corresponding output.\nCrucially, in-context learning happens only at inference time without any\nparameter updates to the model. While large language models such as GPT-3\nexhibit some ability to perform in-context learning, it is unclear what the\nrelationship is between tasks on which this succeeds and what is present in the\ntraining data. To make progress towards understanding in-context learning, we\nconsider the well-defined problem of training a model to in-context learn a\nfunction class (e.g., linear functions): that is, given data derived from some\nfunctions in the class, can we train a model to in-context learn \"most\"\nfunctions from this class? We show empirically that standard Transformers can\nbe trained from scratch to perform in-context learning of linear functions --\nthat is, the trained model is able to learn unseen linear functions from\nin-context examples with performance comparable to the optimal least squares\nestimator. In fact, in-context learning is possible even under two forms of\ndistribution shift: (i) between the training data of the model and\ninference-time prompts, and (ii) between the in-context examples and the query\ninput during inference. We also show that we can train Transformers to\nin-context learn more complex function classes -- namely sparse linear\nfunctions, two-layer neural networks, and decision trees -- with performance\nthat matches or exceeds task-specific learning algorithms. Our code and models\nare available at https://github.com/dtsip/in-context-learning .",
        "date": "2022-08-01"
    },
    "https://arxiv.org/abs/2206.02743": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp microsoft",
            "discute avec raphael",
            "learned index structures"
        ],
        "title": "[2206.02743] A Neural Corpus Indexer for Document Retrieval",
        "summary": "Current state-of-the-art document retrieval solutions mainly follow an\nindex-retrieve paradigm, where the index is hard to be directly optimized for\nthe final retrieval target. In this paper, we aim to show that an end-to-end\ndeep neural network unifying training and indexing stages can significantly\nimprove the recall performance of traditional methods. To this end, we propose\nNeural Corpus Indexer (NCI), a sequence-to-sequence network that generates\nrelevant document identifiers directly for a designated query. To optimize the\nrecall performance of NCI, we invent a prefix-aware weight-adaptive decoder\narchitecture, and leverage tailored techniques including query generation,\nsemantic document identifiers, and consistency-based regularization. Empirical\nstudies demonstrated the superiority of NCI on two commonly used academic\nbenchmarks, achieving +17.6% and +16.8% relative enhancement for Recall@1 on\nNQ320k dataset and R-Precision on TriviaQA dataset, respectively, compared to\nthe best baseline method.",
        "date": "2022-06-06"
    },
    "https://arxiv.org/abs/1712.05972": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "zero shot learning",
            "zero shot text classifier",
            "nlp text classification"
        ],
        "title": "[1712.05972] Train Once, Test Anywhere: Zero-Shot Learning for Text Classification",
        "summary": "Zero-shot Learners are models capable of predicting unseen classes. In this\nwork, we propose a Zero-shot Learning approach for text categorization. Our\nmethod involves training model on a large corpus of sentences to learn the\nrelationship between a sentence and embedding of sentence's tags. Learning such\nrelationship makes the model generalize to unseen sentences, tags, and even new\ndatasets provided they can be put into same embedding space. The model learns\nto predict whether a given sentence is related to a tag or not; unlike other\nclassifiers that learn to classify the sentence as one of the possible classes.\nWe propose three different neural networks for the task and report their\naccuracy on the test set of the dataset used for training them as well as two\nother standard datasets for which no retraining was done. We show that our\nmodels generalize well across new unseen classes in both cases. Although the\nmodels do not achieve the accuracy level of the state of the art supervised\nmodels, yet it evidently is a step forward towards general intelligence in\nnatural language processing.",
        "date": "2017-12-16"
    },
    "https://arxiv.org/abs/2012.12624": {
        "extra-tags": [],
        "tags": [
            "nlp princeton",
            "arxiv doc",
            "phrase embeddings",
            "discute avec raphael",
            "open domain question answering"
        ],
        "title": "[2012.12624] Learning Dense Representations of Phrases at Scale",
        "summary": "Open-domain question answering can be reformulated as a phrase retrieval\nproblem, without the need for processing documents on-demand during inference\n(Seo et al., 2019). However, current phrase retrieval models heavily depend on\nsparse representations and still underperform retriever-reader approaches. In\nthis work, we show for the first time that we can learn dense representations\nof phrases alone that achieve much stronger performance in open-domain QA. We\npresent an effective method to learn phrase representations from the\nsupervision of reading comprehension tasks, coupled with novel negative\nsampling methods. We also propose a query-side fine-tuning strategy, which can\nsupport transfer learning and reduce the discrepancy between training and\ninference. On five popular open-domain QA datasets, our model DensePhrases\nimproves over previous phrase retrieval models by 15%-25% absolute accuracy and\nmatches the performance of state-of-the-art retriever-reader models. Our model\nis easy to parallelize due to pure dense representations and processes more\nthan 10 questions per second on CPUs. Finally, we directly use our pre-indexed\ndense phrase representations for two slot filling tasks, showing the promise of\nutilizing DensePhrases as a dense knowledge base for downstream tasks.",
        "date": "2020-12-23"
    },
    "https://arxiv.org/abs/1911.11506": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp text classification",
            "label embedding"
        ],
        "title": "[1911.11506] Word-Class Embeddings for Multiclass Text Classification",
        "summary": "Pre-trained word embeddings encode general word semantics and lexical\nregularities of natural language, and have proven useful across many NLP tasks,\nincluding word sense disambiguation, machine translation, and sentiment\nanalysis, to name a few. In supervised tasks such as multiclass text\nclassification (the focus of this article) it seems appealing to enhance word\nrepresentations with ad-hoc embeddings that encode task-specific information.\nWe propose (supervised) word-class embeddings (WCEs), and show that, when\nconcatenated to (unsupervised) pre-trained word embeddings, they substantially\nfacilitate the training of deep-learning models in multiclass classification by\ntopic. We show empirical evidence that WCEs yield a consistent improvement in\nmulticlass classification accuracy, using four popular neural architectures and\nsix widely used and publicly available datasets for multiclass text\nclassification. Our code that implements WCEs is publicly available at\nhttps://github.com/AlexMoreo/word-class-embeddings",
        "date": "2019-11-26"
    },
    "https://arxiv.org/abs/1503.08677": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "label embedding",
            "image classification"
        ],
        "title": "[1503.08677] Label-Embedding for Image Classification",
        "summary": "Attributes act as intermediate representations that enable parameter sharing\nbetween classes, a must when training data is scarce. We propose to view\nattribute-based image classification as a label-embedding problem: each class\nis embedded in the space of attribute vectors. We introduce a function that\nmeasures the compatibility between an image and a label embedding. The\nparameters of this function are learned on a training set of labeled samples to\nensure that, given an image, the correct classes rank higher than the incorrect\nones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets\nshow that the proposed framework outperforms the standard Direct Attribute\nPrediction baseline in a zero-shot learning scenario. Label embedding enjoys a\nbuilt-in ability to leverage alternative sources of information instead of or\nin addition to attributes, such as e.g. class hierarchies or textual\ndescriptions. Moreover, label embedding encompasses the whole range of learning\nsettings from zero-shot learning to regular learning with a large number of\nlabeled examples.",
        "date": "2015-03-30"
    },
    "https://arxiv.org/abs/2208.09982": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "topic modeling",
            "nlp long documents",
            "lm kb",
            "extractive summarization"
        ],
        "title": "[2208.09982] GRETEL: Graph Contrastive Topic Enhanced Language Model for Long Document Extractive Summarization",
        "summary": "Recently, neural topic models (NTMs) have been incorporated into pre-trained\nlanguage models (PLMs), to capture the global semantic information for text\nsummarization. However, in these methods, there remain limitations in the way\nthey capture and integrate the global semantic information. In this paper, we\npropose a novel model, the graph contrastive topic enhanced language model\n(GRETEL), that incorporates the graph contrastive topic model with the\npre-trained language model, to fully leverage both the global and local\ncontextual semantics for long document extractive summarization. To better\ncapture and incorporate the global semantic information into PLMs, the graph\ncontrastive topic model integrates the hierarchical transformer encoder and the\ngraph contrastive learning to fuse the semantic information from the global\ndocument context and the gold summary. To this end, GRETEL encourages the model\nto efficiently extract salient sentences that are topically related to the gold\nsummary, rather than redundant sentences that cover sub-optimal topics.\nExperimental results on both general domain and biomedical datasets demonstrate\nthat our proposed method outperforms SOTA methods.",
        "date": "2022-08-21"
    },
    "https://arxiv.org/abs/2110.06176": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "attention knowledge graphs",
            "attention is all you need",
            "entities",
            "interesting idea"
        ],
        "title": "[2110.06176] Mention Memory: incorporating textual knowledge into Transformers through entity mention attention",
        "summary": "Natural language understanding tasks such as open-domain question answering\noften require retrieving and assimilating factual information from multiple\nsources. We propose to address this problem by integrating a semi-parametric\nrepresentation of a large text corpus into a Transformer model as a source of\nfactual knowledge. Specifically, our method represents knowledge with `mention\nmemory', a table of dense vector representations of every entity mention in a\ncorpus. The proposed model - TOME - is a Transformer that accesses the\ninformation through internal memory layers in which each entity mention in the\ninput passage attends to the mention memory. This approach enables synthesis of\nand reasoning over many disparate sources of information within a single\nTransformer model. In experiments using a memory of 150 million Wikipedia\nmentions, TOME achieves strong performance on several open-domain\nknowledge-intensive tasks, including the claim verification benchmarks HoVer\nand FEVER and several entity-based QA benchmarks. We also show that the model\nlearns to attend to informative mentions without any direct supervision.\nFinally we demonstrate that the model can generalize to new unseen entities by\nupdating the memory without retraining.",
        "date": "2021-10-12"
    },
    "https://arxiv.org/abs/1902.10909": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "intent classification and slot filling",
            "nlp alibaba",
            "bert"
        ],
        "title": "[1902.10909] BERT for Joint Intent Classification and Slot Filling",
        "summary": "Intent classification and slot filling are two essential tasks for natural\nlanguage understanding. They often suffer from small-scale human-labeled\ntraining data, resulting in poor generalization capability, especially for rare\nwords. Recently a new language representation model, BERT (Bidirectional\nEncoder Representations from Transformers), facilitates pre-training deep\nbidirectional representations on large-scale unlabeled corpora, and has created\nstate-of-the-art models for a wide variety of natural language processing tasks\nafter simple fine-tuning. However, there has not been much effort on exploring\nBERT for natural language understanding. In this work, we propose a joint\nintent classification and slot filling model based on BERT. Experimental\nresults demonstrate that our proposed model achieves significant improvement on\nintent classification accuracy, slot filling F1, and sentence-level semantic\nframe accuracy on several public benchmark datasets, compared to the\nattention-based recurrent neural network models and slot-gated models.",
        "date": "2019-02-28"
    },
    "https://arxiv.org/abs/2102.12627": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "geoffrey hinton"
        ],
        "title": "[2102.12627] How to represent part-whole hierarchies in a neural network",
        "summary": "This paper does not describe a working system. Instead, it presents a single\nidea about representation which allows advances made by several different\ngroups to be combined into an imaginary system called GLOM. The advances\ninclude transformers, neural fields, contrastive representation learning,\ndistillation and capsules. GLOM answers the question: How can a neural network\nwith a fixed architecture parse an image into a part-whole hierarchy which has\na different structure for each image? The idea is simply to use islands of\nidentical vectors to represent the nodes in the parse tree. If GLOM can be made\nto work, it should significantly improve the interpretability of the\nrepresentations produced by transformer-like systems when applied to vision or\nlanguage",
        "date": "2021-02-25"
    },
    "https://arxiv.org/abs/2112.01488": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "domain specific corpora",
            "colbert"
        ],
        "title": "[2112.01488] ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction",
        "summary": "Neural information retrieval (IR) has greatly advanced search and other\nknowledge-intensive language tasks. While many neural IR methods encode queries\nand documents into single-vector representations, late interaction models\nproduce multi-vector representations at the granularity of each token and\ndecompose relevance modeling into scalable token-level computations. This\ndecomposition has been shown to make late interaction more effective, but it\ninflates the space footprint of these models by an order of magnitude. In this\nwork, we introduce ColBERTv2, a retriever that couples an aggressive residual\ncompression mechanism with a denoised supervision strategy to simultaneously\nimprove the quality and space footprint of late interaction. We evaluate\nColBERTv2 across a wide range of benchmarks, establishing state-of-the-art\nquality within and outside the training domain while reducing the space\nfootprint of late interaction models by 5--8$\\times$.",
        "date": "2021-12-02"
    },
    "https://arxiv.org/abs/1806.06478": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "entity alignment",
            "cross lingual nlp",
            "knowledge graph embeddings",
            "co training",
            "using literal descriptions of entities"
        ],
        "title": "[1806.06478] Co-training Embeddings of Knowledge Graphs and Entity Descriptions for Cross-lingual Entity Alignment",
        "summary": "Multilingual knowledge graph (KG) embeddings provide latent semantic\nrepresentations of entities and structured knowledge with cross-lingual\ninferences, which benefit various knowledge-driven cross-lingual NLP tasks.\nHowever, precisely learning such cross-lingual inferences is usually hindered\nby the low coverage of entity alignment in many KGs. Since many multilingual\nKGs also provide literal descriptions of entities, in this paper, we introduce\nan embedding-based approach which leverages a weakly aligned multilingual KG\nfor semi-supervised cross-lingual learning using entity descriptions. Our\napproach performs co-training of two embedding models, i.e. a multilingual KG\nembedding model and a multilingual literal description embedding model. The\nmodels are trained on a large Wikipedia-based trilingual dataset where most\nentity alignment is unknown to training. Experimental results show that the\nperformance of the proposed approach on the entity alignment task improves at\neach iteration of co-training, and eventually reaches a stage at which it\nsignificantly surpasses previous approaches. We also show that our approach has\npromising abilities for zero-shot entity alignment, and cross-lingual KG\ncompletion.",
        "date": "2018-06-18"
    },
    "https://arxiv.org/abs/1912.12510": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "out of distribution detection"
        ],
        "title": "[1912.12510] Detecting Out-of-Distribution Examples with In-distribution Examples and Gram Matrices",
        "summary": "When presented with Out-of-Distribution (OOD) examples, deep neural networks\nyield confident, incorrect predictions. Detecting OOD examples is challenging,\nand the potential risks are high. In this paper, we propose to detect OOD\nexamples by identifying inconsistencies between activity patterns and class\npredicted. We find that characterizing activity patterns by Gram matrices and\nidentifying anomalies in gram matrix values can yield high OOD detection rates.\nWe identify anomalies in the gram matrices by simply comparing each value with\nits respective range observed over the training data. Unlike many approaches,\nthis can be used with any pre-trained softmax classifier and does not require\naccess to OOD data for fine-tuning hyperparameters, nor does it require OOD\naccess for inferring parameters. The method is applicable across a variety of\narchitectures and vision datasets and, for the important and surprisingly hard\ntask of detecting far-from-distribution out-of-distribution examples, it\ngenerally performs better than or equal to state-of-the-art OOD detection\nmethods (including those that do assume access to OOD examples).",
        "date": "2019-12-28"
    },
    "https://arxiv.org/abs/1904.08375": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "question answering",
            "dense passage retrieval"
        ],
        "title": "[1904.08375] Document Expansion by Query Prediction",
        "summary": "One technique to improve the retrieval effectiveness of a search engine is to\nexpand documents with terms that are related or representative of the\ndocuments' content.From the perspective of a question answering system, this\nmight comprise questions the document can potentially answer. Following this\nobservation, we propose a simple method that predicts which queries will be\nissued for a given document and then expands it with those predictions with a\nvanilla sequence-to-sequence model, trained using datasets consisting of pairs\nof query and relevant documents. By combining our method with a\nhighly-effective re-ranking component, we achieve the state of the art in two\nretrieval tasks. In a latency-critical regime, retrieval results alone (without\nre-ranking) approach the effectiveness of more computationally expensive neural\nre-rankers but are much faster.",
        "date": "2019-04-17"
    },
    "https://arxiv.org/abs/2004.09813": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nils reimers",
            "multilingual nlp",
            "sentence embeddings",
            "knowledge distillation"
        ],
        "title": "[2004.09813] Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation",
        "summary": "We present an easy and efficient method to extend existing sentence embedding\nmodels to new languages. This allows to create multilingual versions from\npreviously monolingual models. The training is based on the idea that a\ntranslated sentence should be mapped to the same location in the vector space\nas the original sentence. We use the original (monolingual) model to generate\nsentence embeddings for the source language and then train a new system on\ntranslated sentences to mimic the original model. Compared to other methods for\ntraining multilingual sentence embeddings, this approach has several\nadvantages: It is easy to extend existing models with relatively few samples to\nnew languages, it is easier to ensure desired properties for the vector space,\nand the hardware requirements for training is lower. We demonstrate the\neffectiveness of our approach for 50+ languages from various language families.\nCode to extend sentence embeddings models to more than 400 languages is\npublicly available.",
        "date": "2020-04-21"
    },
    "https://arxiv.org/abs/2001.03765": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "entity embeddings",
            "entity type representation"
        ],
        "title": "[2001.03765] Learning Cross-Context Entity Representations from Text",
        "summary": "Language modeling tasks, in which words, or word-pieces, are predicted on the\nbasis of a local context, have been very effective for learning word embeddings\nand context dependent representations of phrases. Motivated by the observation\nthat efforts to code world knowledge into machine readable knowledge bases or\nhuman readable encyclopedias tend to be entity-centric, we investigate the use\nof a fill-in-the-blank task to learn context independent representations of\nentities from the text contexts in which those entities were mentioned. We show\nthat large scale training of neural models allows us to learn high quality\nentity representations, and we demonstrate successful results on four domains:\n(1) existing entity-level typing benchmarks, including a 64% error reduction\nover previous work on TypeNet (Murty et al., 2018); (2) a novel few-shot\ncategory reconstruction task; (3) existing entity linking benchmarks, where we\nmatch the state-of-the-art on CoNLL-Aida without linking-specific features and\nobtain a score of 89.8% on TAC-KBP 2010 without using any alias table, external\nknowledge base or in domain training data and (4) answering trivia questions,\nwhich uniquely identify entities. Our global entity representations encode\nfine-grained type categories, such as Scottish footballers, and can answer\ntrivia questions such as: Who was the last inmate of Spandau jail in Berlin?",
        "date": "2020-01-11"
    },
    "https://arxiv.org/abs/2010.00904": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp facebook",
            "gautier izacard",
            "entity linking"
        ],
        "title": "[2010.00904] Autoregressive Entity Retrieval",
        "summary": "Entities are at the center of how we represent and aggregate knowledge. For\ninstance, Encyclopedias such as Wikipedia are structured by entities (e.g., one\nper article). The ability to retrieve such entities given a query is\nfundamental for knowledge-intensive tasks such as entity linking and\nopen-domain question answering. One way to understand current approaches is as\nclassifiers among atomic labels, one for each entity. Their weight vectors are\ndense entity representations produced by encoding entity information such as\ndescriptions. This approach leads to several shortcomings: i) context and\nentity affinity is mainly captured through a vector dot product, potentially\nmissing fine-grained interactions between the two; ii) a large memory footprint\nis needed to store dense representations when considering large entity sets;\niii) an appropriately hard set of negative data has to be subsampled at\ntraining time. We propose GENRE, the first system that retrieves entities by\ngenerating their unique names, left to right, token-by-token in an\nautoregressive fashion, and conditioned on the context. This enables to\nmitigate the aforementioned technical issues: i) the autoregressive formulation\nallows us to directly capture relations between context and entity name,\neffectively cross encoding both; ii) the memory footprint is greatly reduced\nbecause the parameters of our encoder-decoder architecture scale with\nvocabulary size, not entity count; iii) the exact softmax loss can be\nefficiently computed without the need to subsample negative data. We show the\nefficacy of the approach with more than 20 datasets on entity disambiguation,\nend-to-end entity linking and document retrieval tasks, achieving new SOTA, or\nvery competitive results while using a tiny fraction of the memory of competing\nsystems. Finally, we demonstrate that new entities can be added by simply\nspecifying their unambiguous name.",
        "date": "2020-10-02"
    },
    "https://arxiv.org/abs/1709.03933": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "feature hashing",
            "word embedding"
        ],
        "title": "[1709.03933] Hash Embeddings for Efficient Word Representations",
        "summary": "We present hash embeddings, an efficient method for representing words in a\ncontinuous vector form. A hash embedding may be seen as an interpolation\nbetween a standard word embedding and a word embedding created using a random\nhash function (the hashing trick). In hash embeddings each token is represented\nby $k$ $d$-dimensional embeddings vectors and one $k$ dimensional weight\nvector. The final $d$ dimensional representation of the token is the product of\nthe two. Rather than fitting the embedding vectors for each token these are\nselected by the hashing trick from a shared pool of $B$ embedding vectors. Our\nexperiments show that hash embeddings can easily deal with huge vocabularies\nconsisting of millions of tokens. When using a hash embedding there is no need\nto create a dictionary before training nor to perform any kind of vocabulary\npruning after training. We show that models trained using hash embeddings\nexhibit at least the same level of performance as models trained using regular\nembeddings across a wide range of tasks. Furthermore, the number of parameters\nneeded by such an embedding is only a fraction of what is required by a regular\nembedding. Since standard embeddings and embeddings constructed using the\nhashing trick are actually just special cases of a hash embedding, hash\nembeddings can be considered an extension and improvement over the existing\nregular embedding types.",
        "date": "2017-09-12"
    },
    "https://arxiv.org/abs/2012.04584": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "gautier izacard",
            "question answering",
            "retriever reader",
            "nlp facebook",
            "nlp ens",
            "knowledge distillation",
            "knowledge augmented language models",
            "open domain question answering",
            "neural models for information retrieval"
        ],
        "title": "[2012.04584] Distilling Knowledge from Reader to Retriever for Question Answering",
        "summary": "The task of information retrieval is an important component of many natural\nlanguage processing systems, such as open domain question answering. While\ntraditional methods were based on hand-crafted features, continuous\nrepresentations based on neural networks recently obtained competitive results.\nA challenge of using such methods is to obtain supervised data to train the\nretriever model, corresponding to pairs of query and support documents. In this\npaper, we propose a technique to learn retriever models for downstream tasks,\ninspired by knowledge distillation, and which does not require annotated pairs\nof query and documents. Our approach leverages attention scores of a reader\nmodel, used to solve the task based on retrieved documents, to obtain synthetic\nlabels for the retriever. We evaluate our method on question answering,\nobtaining state-of-the-art results.",
        "date": "2020-12-08"
    },
    "https://arxiv.org/abs/2103.12953": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "short text clustering",
            "contrastive learning"
        ],
        "title": "[2103.12953] Supporting Clustering with Contrastive Learning",
        "summary": "Unsupervised clustering aims at discovering the semantic categories of data\naccording to some distance measured in the representation space. However,\ndifferent categories often overlap with each other in the representation space\nat the beginning of the learning process, which poses a significant challenge\nfor distance-based clustering in achieving good separation between different\ncategories. To this end, we propose Supporting Clustering with Contrastive\nLearning (SCCL) -- a novel framework to leverage contrastive learning to\npromote better separation. We assess the performance of SCCL on short text\nclustering and show that SCCL significantly advances the state-of-the-art\nresults on most benchmark datasets with 3%-11% improvement on Accuracy and\n4%-15% improvement on Normalized Mutual Information. Furthermore, our\nquantitative analysis demonstrates the effectiveness of SCCL in leveraging the\nstrengths of both bottom-up instance discrimination and top-down clustering to\nachieve better intra-cluster and inter-cluster distances when evaluated with\nthe ground truth cluster labels",
        "date": "2021-03-24"
    },
    "https://arxiv.org/abs/1907.05242": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "guillaume lample",
            "memory in deep learning",
            "nlp facebook",
            "attention is all you need",
            "memory networks",
            "ludovic denoyer"
        ],
        "title": "[1907.05242] Large Memory Layers with Product Keys",
        "summary": "This paper introduces a structured memory which can be easily integrated into\na neural network. The memory is very large by design and significantly\nincreases the capacity of the architecture, by up to a billion parameters with\na negligible computational overhead. Its design and access pattern is based on\nproduct keys, which enable fast and exact nearest neighbor search. The ability\nto increase the number of parameters while keeping the same computational\nbudget lets the overall system strike a better trade-off between prediction\naccuracy and computation efficiency both at training and test time. This memory\nlayer allows us to tackle very large scale language modeling tasks. In our\nexperiments we consider a dataset with up to 30 billion words, and we plug our\nmemory layer in a state-of-the-art transformer-based architecture. In\nparticular, we found that a memory augmented model with only 12 layers\noutperforms a baseline transformer model with 24 layers, while being twice\nfaster at inference time. We release our code for reproducibility purposes.",
        "date": "2019-07-10"
    },
    "https://arxiv.org/abs/1609.02521": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "extreme multi label classification"
        ],
        "title": "[1609.02521] DiSMEC - Distributed Sparse Machines for Extreme Multi-label Classification",
        "summary": "Extreme multi-label classification refers to supervised multi-label learning\ninvolving hundreds of thousands or even millions of labels. Datasets in extreme\nclassification exhibit fit to power-law distribution, i.e. a large fraction of\nlabels have very few positive instances in the data distribution. Most\nstate-of-the-art approaches for extreme multi-label classification attempt to\ncapture correlation among labels by embedding the label matrix to a\nlow-dimensional linear sub-space. However, in the presence of power-law\ndistributed extremely large and diverse label spaces, structural assumptions\nsuch as low rank can be easily violated.\nIn this work, we present DiSMEC, which is a large-scale distributed framework\nfor learning one-versus-rest linear classifiers coupled with explicit capacity\ncontrol to control model size. Unlike most state-of-the-art methods, DiSMEC\ndoes not make any low rank assumptions on the label matrix. Using double layer\nof parallelization, DiSMEC can learn classifiers for datasets consisting\nhundreds of thousands labels within few hours. The explicit capacity control\nmechanism filters out spurious parameters which keep the model compact in size,\nwithout losing prediction accuracy. We conduct extensive empirical evaluation\non publicly available real-world datasets consisting upto 670,000 labels. We\ncompare DiSMEC with recent state-of-the-art approaches, including - SLEEC which\nis a leading approach for learning sparse local embeddings, and FastXML which\nis a tree-based approach optimizing ranking based loss function. On some of the\ndatasets, DiSMEC can significantly boost prediction accuracies - 10% better\ncompared to SLECC and 15% better compared to FastXML, in absolute terms.",
        "date": "2016-09-08"
    },
    "https://arxiv.org/abs/1908.10084": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "sbert",
            "emnlp 2019",
            "siamese network",
            "nearest neighbor search",
            "sentence similarity",
            "huggingface transformers"
        ],
        "title": "[1908.10084] Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "summary": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new\nstate-of-the-art performance on sentence-pair regression tasks like semantic\ntextual similarity (STS). However, it requires that both sentences are fed into\nthe network, which causes a massive computational overhead: Finding the most\nsimilar pair in a collection of 10,000 sentences requires about 50 million\ninference computations (~65 hours) with BERT. The construction of BERT makes it\nunsuitable for semantic similarity search as well as for unsupervised tasks\nlike clustering.\nIn this publication, we present Sentence-BERT (SBERT), a modification of the\npretrained BERT network that use siamese and triplet network structures to\nderive semantically meaningful sentence embeddings that can be compared using\ncosine-similarity. This reduces the effort for finding the most similar pair\nfrom 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while\nmaintaining the accuracy from BERT.\nWe evaluate SBERT and SRoBERTa on common STS tasks and transfer learning\ntasks, where it outperforms other state-of-the-art sentence embeddings methods.",
        "date": "2019-08-27"
    },
    "https://arxiv.org/abs/1901.04085": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp facebook",
            "bert",
            "machine learned ranking",
            "ranking information retrieval"
        ],
        "title": "[1901.04085] Passage Re-ranking with BERT",
        "summary": "Recently, neural models pretrained on a language modeling task, such as ELMo\n(Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et\nal., 2018), have achieved impressive results on various natural language\nprocessing tasks such as question-answering and natural language inference. In\nthis paper, we describe a simple re-implementation of BERT for query-based\npassage re-ranking. Our system is the state of the art on the TREC-CAR dataset\nand the top entry in the leaderboard of the MS MARCO passage retrieval task,\noutperforming the previous state of the art by 27% (relative) in MRR@10. The\ncode to reproduce our results is available at\nhttps://github.com/nyu-dl/dl4marco-bert",
        "date": "2019-01-13"
    },
    "https://arxiv.org/abs/2205.05638": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "prompted models",
            "parameter efficient fine tuning peft"
        ],
        "title": "[2205.05638] Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning",
        "summary": "Few-shot in-context learning (ICL) enables pre-trained language models to\nperform a previously-unseen task without any gradient-based training by feeding\na small number of training examples as part of the input. ICL incurs\nsubstantial computational, memory, and storage costs because it involves\nprocessing all of the training examples every time a prediction is made.\nParameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning,\nsparse update methods, etc.) offers an alternative paradigm where a small set\nof parameters are trained to enable a model to perform the new task. In this\npaper, we rigorously compare few-shot ICL and PEFT and demonstrate that the\nlatter offers better accuracy as well as dramatically lower computational\ncosts. Along the way, we introduce a new PEFT method called (IA)$^3$ that\nscales activations by learned vectors, attaining stronger performance while\nonly introducing a relatively tiny amount of new parameters. We also propose a\nsimple recipe based on the T0 model called T-Few that can be applied to new\ntasks without task-specific tuning or modifications. We validate the\neffectiveness of T-Few on completely unseen tasks by applying it to the RAFT\nbenchmark, attaining super-human performance for the first time and\noutperforming the state-of-the-art by 6% absolute. All of the code used in our\nexperiments is publicly available.",
        "date": "2022-05-11"
    },
    "https://www.aclweb.org/anthology/D19-1005/": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "entities and lm",
            "knowbert",
            "knowledge graph augmented language models",
            "knowledge driven embeddings",
            "multiple knowledge bases",
            "grounded language learning",
            "contextualised word representations",
            "knowledge augmented language models",
            "kd mkb biblio",
            "allen institute for ai a2i",
            "emnlp 2019",
            "good",
            "nlp using knowledge graphs"
        ],
        "title": "[1909.04164] Knowledge Enhanced Contextual Word Representations",
        "summary": "Contextual word representations, typically trained on unstructured, unlabeled\ntext, do not contain any explicit grounding to real world entities and are\noften unable to remember facts about those entities. We propose a general\nmethod to embed multiple knowledge bases (KBs) into large scale models, and\nthereby enhance their representations with structured, human-curated knowledge.\nFor each KB, we first use an integrated entity linker to retrieve relevant\nentity embeddings, then update contextual word representations via a form of\nword-to-entity attention. In contrast to previous approaches, the entity\nlinkers and self-supervised language modeling objective are jointly trained\nend-to-end in a multitask setting that combines a small amount of entity\nlinking supervision with a large amount of raw text. After integrating WordNet\nand a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert)\ndemonstrates improved perplexity, ability to recall facts as measured in a\nprobing task and downstream performance on relationship extraction, entity\ntyping, and word sense disambiguation. KnowBert's runtime is comparable to\nBERT's and it scales to large KBs.",
        "date": "2019-09-09"
    },
    "https://arxiv.org/abs/2003.00330": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "graph neural networks",
            "neural symbolic computing",
            "survey"
        ],
        "title": "[2003.00330] Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and Perspective",
        "summary": "Neural-symbolic computing has now become the subject of interest of both\nacademic and industry research laboratories. Graph Neural Networks (GNN) have\nbeen widely used in relational and symbolic domains, with widespread\napplication of GNNs in combinatorial optimization, constraint satisfaction,\nrelational reasoning and other scientific domains. The need for improved\nexplainability, interpretability and trust of AI systems in general demands\nprincipled methodologies, as suggested by neural-symbolic computing. In this\npaper, we review the state-of-the-art on the use of GNNs as a model of\nneural-symbolic computing. This includes the application of GNNs in several\ndomains as well as its relationship to current developments in neural-symbolic\ncomputing.",
        "date": "2020-02-29"
    },
    "https://arxiv.org/abs/2010.07835": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "self training",
            "weak supervision",
            "language model fine tuning"
        ],
        "title": "[2010.07835] Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach",
        "summary": "Fine-tuned pre-trained language models (LMs) have achieved enormous success\nin many natural language processing (NLP) tasks, but they still require\nexcessive labeled data in the fine-tuning stage. We study the problem of\nfine-tuning pre-trained LMs using only weak supervision, without any labeled\ndata. This problem is challenging because the high capacity of LMs makes them\nprone to overfitting the noisy labels generated by weak supervision. To address\nthis problem, we develop a contrastive self-training framework, COSINE, to\nenable fine-tuning LMs with weak supervision. Underpinned by contrastive\nregularization and confidence-based reweighting, this contrastive self-training\nframework can gradually improve model fitting while effectively suppressing\nerror propagation. Experiments on sequence, token, and sentence pair\nclassification tasks show that our model outperforms the strongest baseline by\nlarge margins on 7 benchmarks in 6 tasks, and achieves competitive performance\nwith fully-supervised fine-tuning methods.",
        "date": "2020-10-15"
    },
    "https://arxiv.org/abs/1911.05507": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "memory in deep learning",
            "nlp long documents",
            "sequence to sequence learning",
            "google deepmind",
            "attention is all you need"
        ],
        "title": "[1911.05507] Compressive Transformers for Long-Range Sequence Modelling",
        "summary": "We present the Compressive Transformer, an attentive sequence model which\ncompresses past memories for long-range sequence learning. We find the\nCompressive Transformer obtains state-of-the-art language modelling results in\nthe WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc\nrespectively. We also find it can model high-frequency speech effectively and\ncan be used as a memory mechanism for RL, demonstrated on an object matching\ntask. To promote the domain of long-range sequence learning, we propose a new\nopen-vocabulary language modelling benchmark derived from books, PG-19.",
        "date": "2019-11-13"
    },
    "https://arxiv.org/abs/2004.07180": {
        "extra-tags": [],
        "tags": [
            "document embeddings",
            "nlp for scientific documents",
            "attention is all you need",
            "arxiv doc"
        ],
        "title": "[2004.07180] SPECTER: Document-level Representation Learning using Citation-informed Transformers",
        "summary": "Representation learning is a critical ingredient for natural language\nprocessing systems. Recent Transformer language models like BERT learn powerful\ntextual representations, but these models are targeted towards token- and\nsentence-level training objectives and do not leverage information on\ninter-document relatedness, which limits their document-level representation\npower. For applications on scientific documents, such as classification and\nrecommendation, the embeddings power strong performance on end tasks. We\npropose SPECTER, a new method to generate document-level embedding of\nscientific documents based on pretraining a Transformer language model on a\npowerful signal of document-level relatedness: the citation graph. Unlike\nexisting pretrained language models, SPECTER can be easily applied to\ndownstream applications without task-specific fine-tuning. Additionally, to\nencourage further research on document-level models, we introduce SciDocs, a\nnew evaluation benchmark consisting of seven document-level tasks ranging from\ncitation prediction, to document classification and recommendation. We show\nthat SPECTER outperforms a variety of competitive baselines on the benchmark.",
        "date": "2020-04-15"
    },
    "https://arxiv.org/abs/2110.10778": {
        "extra-tags": [],
        "tags": [
            "document embeddings",
            "graph attention networks",
            "arxiv doc",
            "nlp amazon",
            "nlp long documents"
        ],
        "title": "[2110.10778] Contrastive Document Representation Learning with Graph Attention Networks",
        "summary": "Recent progress in pretrained Transformer-based language models has shown\ngreat success in learning contextual representation of text. However, due to\nthe quadratic self-attention complexity, most of the pretrained Transformers\nmodels can only handle relatively short text. It is still a challenge when it\ncomes to modeling very long documents. In this work, we propose to use a graph\nattention network on top of the available pretrained Transformers model to\nlearn document embeddings. This graph attention network allows us to leverage\nthe high-level semantic structure of the document. In addition, based on our\ngraph document model, we design a simple contrastive learning strategy to\npretrain our models on a large amount of unlabeled corpus. Empirically, we\ndemonstrate the effectiveness of our approaches in document classification and\ndocument retrieval tasks.",
        "date": "2021-10-20"
    },
    "https://arxiv.org/abs/2009.00318": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "rdf2vec",
            "knowledge graph completion",
            "knowledge graph embeddings"
        ],
        "title": "[2009.00318] More is not Always Better: The Negative Impact of A-box Materialization on RDF2vec Knowledge Graph Embeddings",
        "summary": "RDF2vec is an embedding technique for representing knowledge graph entities\nin a continuous vector space. In this paper, we investigate the effect of\nmaterializing implicit A-box axioms induced by subproperties, as well as\nsymmetric and transitive properties. While it might be a reasonable assumption\nthat such a materialization before computing embeddings might lead to better\nembeddings, we conduct a set of experiments on DBpedia which demonstrate that\nthe materialization actually has a negative effect on the performance of\nRDF2vec. In our analysis, we argue that despite the huge body of work devoted\non completing missing information in knowledge graphs, such missing implicit\ninformation is actually a signal, not a defect, and we show examples\nillustrating that assumption.",
        "date": "2020-09-01"
    },
    "https://arxiv.org/abs/2002.04688": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "fast ai",
            "jeremy howard",
            "api"
        ],
        "title": "[2002.04688] fastai: A Layered API for Deep Learning",
        "summary": "fastai is a deep learning library which provides practitioners with\nhigh-level components that can quickly and easily provide state-of-the-art\nresults in standard deep learning domains, and provides researchers with\nlow-level components that can be mixed and matched to build new approaches. It\naims to do both things without substantial compromises in ease of use,\nflexibility, or performance. This is possible thanks to a carefully layered\narchitecture, which expresses common underlying patterns of many deep learning\nand data processing techniques in terms of decoupled abstractions. These\nabstractions can be expressed concisely and clearly by leveraging the dynamism\nof the underlying Python language and the flexibility of the PyTorch library.\nfastai includes: a new type dispatch system for Python along with a semantic\ntype hierarchy for tensors; a GPU-optimized computer vision library which can\nbe extended in pure Python; an optimizer which refactors out the common\nfunctionality of modern optimizers into two basic pieces, allowing optimization\nalgorithms to be implemented in 4-5 lines of code; a novel 2-way callback\nsystem that can access any part of the data, model, or optimizer and change it\nat any point during training; a new data block API; and much more. We have used\nthis library to successfully create a complete deep learning course, which we\nwere able to write more quickly than using previous approaches, and the code\nwas more clear. The library is already in wide use in research, industry, and\nteaching. NB: This paper covers fastai v2, which is currently in pre-release at\nhttp://dev.fast.ai/",
        "date": "2020-02-11"
    },
    "https://arxiv.org/abs/1904.04458": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "entity type",
            "nlp facebook",
            "rnn based language model",
            "unsupervised named entity recognition",
            "knowledge augmented language models"
        ],
        "title": "[1904.04458] Knowledge-Augmented Language Model and its Application to Unsupervised Named-Entity Recognition",
        "summary": "Traditional language models are unable to efficiently model entity names\nobserved in text. All but the most popular named entities appear infrequently\nin text providing insufficient context. Recent efforts have recognized that\ncontext can be generalized between entity names that share the same type (e.g.,\n\\emph{person} or \\emph{location}) and have equipped language models with access\nto an external knowledge base (KB). Our Knowledge-Augmented Language Model\n(KALM) continues this line of work by augmenting a traditional model with a KB.\nUnlike previous methods, however, we train with an end-to-end predictive\nobjective optimizing the perplexity of text. We do not require any additional\ninformation such as named entity tags. In addition to improving language\nmodeling performance, KALM learns to recognize named entities in an entirely\nunsupervised way by using entity type information latent in the model. On a\nNamed Entity Recognition (NER) task, KALM achieves performance comparable with\nstate-of-the-art supervised models. Our work demonstrates that named entities\n(and possibly other types of world knowledge) can be modeled successfully using\npredictive learning and training on large corpora of text without any\nadditional information.",
        "date": "2019-04-09"
    },
    "https://arxiv.org/abs/2006.00632": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "unsupervised domain adaptation nlp"
        ],
        "title": "[2006.00632] Neural Unsupervised Domain Adaptation in NLP---A Survey",
        "summary": "Deep neural networks excel at learning from labeled data and achieve\nstate-of-the-art resultson a wide array of Natural Language Processing tasks.\nIn contrast, learning from unlabeled data, especially under domain shift,\nremains a challenge. Motivated by the latest advances, in this survey we review\nneural unsupervised domain adaptation techniques which do not require labeled\ntarget domain data. This is a more challenging yet a more widely applicable\nsetup. We outline methods, from early traditional non-neural methods to\npre-trained model transfer. We also revisit the notion of domain, and we\nuncover a bias in the type of Natural Language Processing tasks which received\nmost attention. Lastly, we outline future directions, particularly the broader\nneed for out-of-distribution generalization of future NLP.",
        "date": "2020-05-31"
    },
    "https://arxiv.org/abs/2110.08207": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "zero shot",
            "huggingface bigscience"
        ],
        "title": "[2110.08207] Multitask Prompted Training Enables Zero-Shot Task Generalization",
        "summary": "Large language models have recently been shown to attain reasonable zero-shot\ngeneralization on a diverse set of tasks. It has been hypothesized that this is\na consequence of implicit multitask learning in language model training. Can\nzero-shot generalization instead be directly induced by explicit multitask\nlearning? To test this question at scale, we develop a system for easily\nmapping general natural language tasks into a human-readable prompted form. We\nconvert a large set of supervised datasets, each with multiple prompts using\nvarying natural language. These prompted datasets allow for benchmarking the\nability of a model to perform completely unseen tasks specified in natural\nlanguage. We fine-tune a pretrained encoder-decoder model on this multitask\nmixture covering a wide variety of tasks. The model attains strong zero-shot\nperformance on several standard datasets, often outperforming models 16x its\nsize. Further, our approach attains strong performance on a subset of tasks\nfrom the BIG-Bench benchmark, outperforming models 6x its size. All prompts and\ntrained models are available at github.com/bigscience-workshop/promptsource/.",
        "date": "2021-10-15"
    },
    "https://arxiv.org/abs/2301.08210": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "graph neural networks",
            "survey",
            "google deepmind"
        ],
        "title": "[2301.08210] Everything is Connected: Graph Neural Networks",
        "summary": "In many ways, graphs are the main modality of data we receive from nature.\nThis is due to the fact that most of the patterns we see, both in natural and\nartificial systems, are elegantly representable using the language of graph\nstructures. Prominent examples include molecules (represented as graphs of\natoms and bonds), social networks and transportation networks. This potential\nhas already been seen by key scientific and industrial groups, with\nalready-impacted application areas including traffic forecasting, drug\ndiscovery, social network analysis and recommender systems. Further, some of\nthe most successful domains of application for machine learning in previous\nyears -- images, text and speech processing -- can be seen as special cases of\ngraph representation learning, and consequently there has been significant\nexchange of information between these areas. The main aim of this short survey\nis to enable the reader to assimilate the key concepts in the area, and\nposition graph representation learning in a proper context with related fields.",
        "date": "2023-01-19"
    },
    "https://arxiv.org/abs/2008.08995": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "virtual knowledge graph",
            "kg and nlp",
            "knowledge graph construction"
        ],
        "title": "[2008.08995] Constructing a Knowledge Graph from Unstructured Documents without External Alignment",
        "summary": "Knowledge graphs (KGs) are relevant to many NLP tasks, but building a\nreliable domain-specific KG is time-consuming and expensive. A number of\nmethods for constructing KGs with minimized human intervention have been\nproposed, but still require a process to align into the human-annotated\nknowledge base. To overcome this issue, we propose a novel method to\nautomatically construct a KG from unstructured documents that does not require\nexternal alignment and explore its use to extract desired information. To\nsummarize our approach, we first extract knowledge tuples in their surface form\nfrom unstructured documents, encode them using a pre-trained language model,\nand link the surface-entities via the encoding to form the graph structure. We\nperform experiments with benchmark datasets such as WikiMovies and MetaQA. The\nexperimental results show that our method can successfully create and search a\nKG with 18K documents and achieve 69.7% hits@10 (close to an oracle model) on a\nquery retrieval task.",
        "date": "2020-08-20"
    },
    "https://arxiv.org/abs/1903.11279": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp alibaba",
            "2d nlp",
            "information extraction",
            "visually rich documents"
        ],
        "title": "[1903.11279] Graph Convolution for Multimodal Information Extraction from Visually Rich Documents",
        "summary": "Visually rich documents (VRDs) are ubiquitous in daily business and life.\nExamples are purchase receipts, insurance policy documents, custom declaration\nforms and so on. In VRDs, visual and layout information is critical for\ndocument understanding, and texts in such documents cannot be serialized into\nthe one-dimensional sequence without losing information. Classic information\nextraction models such as BiLSTM-CRF typically operate on text sequences and do\nnot incorporate visual features. In this paper, we introduce a graph\nconvolution based model to combine textual and visual information presented in\nVRDs. Graph embeddings are trained to summarize the context of a text segment\nin the document, and further combined with text embeddings for entity\nextraction. Extensive experiments have been conducted to show that our method\noutperforms BiLSTM-CRF baselines by significant margins, on two real-world\ndatasets. Additionally, ablation studies are also performed to evaluate the\neffectiveness of each component of our model.",
        "date": "2019-03-27"
    },
    "https://arxiv.org/abs/2208.11663": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "language model",
            "gautier izacard"
        ],
        "title": "[2208.11663] PEER: A Collaborative Language Model",
        "summary": "Textual content is often the output of a collaborative writing process: We\nstart with an initial draft, ask for suggestions, and repeatedly make changes.\nAgnostic of this process, today's language models are trained to generate only\nthe final result. As a consequence, they lack several abilities crucial for\ncollaborative writing: They are unable to update existing texts, difficult to\ncontrol and incapable of verbally planning or explaining their actions. To\naddress these shortcomings, we introduce PEER, a collaborative language model\nthat is trained to imitate the entire writing process itself: PEER can write\ndrafts, add suggestions, propose edits and provide explanations for its\nactions. Crucially, we train multiple instances of PEER able to infill various\nparts of the writing process, enabling the use of self-training techniques for\nincreasing the quality, amount and diversity of training data. This unlocks\nPEER's full potential by making it applicable in domains for which no edit\nhistories are available and improving its ability to follow instructions, to\nwrite useful comments, and to explain its actions. We show that PEER achieves\nstrong performance across various domains and editing tasks.",
        "date": "2022-08-24"
    },
    "https://arxiv.org/abs/1902.06006": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "allen institute for ai a2i",
            "word embeddings introduction",
            "good"
        ],
        "title": "[1902.06006] Contextual Word Representations: A Contextual Introduction",
        "summary": "This introduction aims to tell the story of how we put words into computers.\nIt is part of the story of the field of natural language processing (NLP), a\nbranch of artificial intelligence. It targets a wide audience with a basic\nunderstanding of computer programming, but avoids a detailed mathematical\ntreatment, and it does not present any algorithms. It also does not focus on\nany particular application of NLP such as translation, question answering, or\ninformation extraction. The ideas presented here were developed by many\nresearchers over many decades, so the citations are not exhaustive but rather\ndirect the reader to a handful of papers that are, in the author's view,\nseminal. After reading this document, you should have a general understanding\nof word vectors (also known as word embeddings): why they exist, what problems\nthey solve, where they come from, how they have changed over time, and what\nsome of the open questions about them are. Readers already familiar with word\nvectors are advised to skip to Section 5 for the discussion of the most recent\nadvance, contextual word vectors.",
        "date": "2019-02-15"
    },
    "https://arxiv.org/abs/2106.00882": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "learning to hash",
            "ikuya yamada",
            "dense passage retrieval",
            "open domain question answering"
        ],
        "title": "[2106.00882] Efficient Passage Retrieval with Hashing for Open-domain Question Answering",
        "summary": "Most state-of-the-art open-domain question answering systems use a neural\nretrieval model to encode passages into continuous vectors and extract them\nfrom a knowledge source. However, such retrieval models often require large\nmemory to run because of the massive size of their passage index. In this\npaper, we introduce Binary Passage Retriever (BPR), a memory-efficient neural\nretrieval model that integrates a learning-to-hash technique into the\nstate-of-the-art Dense Passage Retriever (DPR) to represent the passage index\nusing compact binary codes rather than continuous vectors. BPR is trained with\na multi-task objective over two tasks: efficient candidate generation based on\nbinary codes and accurate reranking based on continuous vectors. Compared with\nDPR, BPR substantially reduces the memory cost from 65GB to 2GB without a loss\nof accuracy on two standard open-domain question answering benchmarks: Natural\nQuestions and TriviaQA. Our code and trained models are available at\nhttps://github.com/studio-ousia/bpr.",
        "date": "2021-06-02"
    },
    "https://arxiv.org/abs/2210.09338": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "kg and nlp",
            "chris manning",
            "lm link prediction",
            "nlp stanford",
            "jure leskovec"
        ],
        "title": "[2210.09338] Deep Bidirectional Language-Knowledge Graph Pretraining",
        "summary": "Pretraining a language model (LM) on text has been shown to help various\ndownstream NLP tasks. Recent works show that a knowledge graph (KG) can\ncomplement text data, offering structured background knowledge that provides a\nuseful scaffold for reasoning. However, these works are not pretrained to learn\na deep fusion of the two modalities at scale, limiting the potential to acquire\nfully joint representations of text and KG. Here we propose DRAGON (Deep\nBidirectional Language-Knowledge Graph Pretraining), a self-supervised approach\nto pretraining a deeply joint language-knowledge foundation model from text and\nKG at scale. Specifically, our model takes pairs of text segments and relevant\nKG subgraphs as input and bidirectionally fuses information from both\nmodalities. We pretrain this model by unifying two self-supervised reasoning\ntasks, masked language modeling and KG link prediction. DRAGON outperforms\nexisting LM and LM+KG models on diverse downstream tasks including question\nanswering across general and biomedical domains, with +5% absolute gain on\naverage. In particular, DRAGON achieves notable performance on complex\nreasoning about language and knowledge (+10% on questions involving long\ncontexts or multi-step reasoning) and low-resource QA (+8% on OBQA and\nRiddleSense), and new state-of-the-art results on various BioNLP tasks. Our\ncode and trained models are available at\nhttps://github.com/michiyasunaga/dragon.",
        "date": "2022-10-17"
    },
    "https://arxiv.org/abs/1503.03832": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "face recognition",
            "ml google",
            "siamese network"
        ],
        "title": "[1503.03832] FaceNet: A Unified Embedding for Face Recognition and Clustering",
        "summary": "Despite significant recent advances in the field of face recognition,\nimplementing face verification and recognition efficiently at scale presents\nserious challenges to current approaches. In this paper we present a system,\ncalled FaceNet, that directly learns a mapping from face images to a compact\nEuclidean space where distances directly correspond to a measure of face\nsimilarity. Once this space has been produced, tasks such as face recognition,\nverification and clustering can be easily implemented using standard techniques\nwith FaceNet embeddings as feature vectors.\nOur method uses a deep convolutional network trained to directly optimize the\nembedding itself, rather than an intermediate bottleneck layer as in previous\ndeep learning approaches. To train, we use triplets of roughly aligned matching\n/ non-matching face patches generated using a novel online triplet mining\nmethod. The benefit of our approach is much greater representational\nefficiency: we achieve state-of-the-art face recognition performance using only\n128-bytes per face.\nOn the widely used Labeled Faces in the Wild (LFW) dataset, our system\nachieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves\n95.12%. Our system cuts the error rate in comparison to the best published\nresult by 30% on both datasets.\nWe also introduce the concept of harmonic embeddings, and a harmonic triplet\nloss, which describe different versions of face embeddings (produced by\ndifferent networks) that are compatible to each other and allow for direct\ncomparison between each other.",
        "date": "2015-03-12"
    },
    "https://arxiv.org/abs/2003.08271": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "pre trained language models"
        ],
        "title": "[2003.08271] Pre-trained Models for Natural Language Processing: A Survey",
        "summary": "Recently, the emergence of pre-trained models (PTMs) has brought natural\nlanguage processing (NLP) to a new era. In this survey, we provide a\ncomprehensive review of PTMs for NLP. We first briefly introduce language\nrepresentation learning and its research progress. Then we systematically\ncategorize existing PTMs based on a taxonomy with four perspectives. Next, we\ndescribe how to adapt the knowledge of PTMs to the downstream tasks. Finally,\nwe outline some potential directions of PTMs for future research. This survey\nis purposed to be a hands-on guide for understanding, using, and developing\nPTMs for various NLP tasks.",
        "date": "2020-03-18"
    },
    "https://arxiv.org/abs/2002.02925": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp microsoft",
            "knowledge distillation",
            "bert"
        ],
        "title": "[2002.02925] BERT-of-Theseus: Compressing BERT by Progressive Module Replacing",
        "summary": "In this paper, we propose a novel model compression approach to effectively\ncompress BERT by progressive module replacing. Our approach first divides the\noriginal BERT into several modules and builds their compact substitutes. Then,\nwe randomly replace the original modules with their substitutes to train the\ncompact modules to mimic the behavior of the original modules. We progressively\nincrease the probability of replacement through the training. In this way, our\napproach brings a deeper level of interaction between the original and compact\nmodels, and smooths the training process. Compared to the previous knowledge\ndistillation approaches for BERT compression, our approach leverages only one\nloss function and one hyper-parameter, liberating human effort from\nhyper-parameter tuning. Our approach outperforms existing knowledge\ndistillation approaches on GLUE benchmark, showing a new perspective of model\ncompression.",
        "date": "2020-02-07"
    },
    "https://arxiv.org/abs/2205.04260": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "entity type",
            "sbert",
            "entities and lm",
            "sentence embeddings",
            "ikuya yamada",
            "entities",
            "contrastive learning"
        ],
        "title": "[2205.04260] EASE: Entity-Aware Contrastive Learning of Sentence Embedding",
        "summary": "We present EASE, a novel method for learning sentence embeddings via\ncontrastive learning between sentences and their related entities. The\nadvantage of using entity supervision is twofold: (1) entities have been shown\nto be a strong indicator of text semantics and thus should provide rich\ntraining signals for sentence embeddings; (2) entities are defined\nindependently of languages and thus offer useful cross-lingual alignment\nsupervision. We evaluate EASE against other unsupervised models both in\nmonolingual and multilingual settings. We show that EASE exhibits competitive\nor better performance in English semantic textual similarity (STS) and short\ntext clustering (STC) tasks and it significantly outperforms baseline methods\nin multilingual settings on a variety of tasks. Our source code, pre-trained\nmodels, and newly constructed multilingual STC dataset are available at\nhttps://github.com/studio-ousia/ease.",
        "date": "2022-05-09"
    },
    "https://arxiv.org/abs/1810.04882": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "word embedding"
        ],
        "title": "[1810.04882] Towards Understanding Linear Word Analogies",
        "summary": "A surprising property of word vectors is that word analogies can often be\nsolved with vector arithmetic. However, it is unclear why arithmetic operators\ncorrespond to non-linear embedding models such as skip-gram with negative\nsampling (SGNS). We provide a formal explanation of this phenomenon without\nmaking the strong assumptions that past theories have made about the vector\nspace and word distribution. Our theory has several implications. Past work has\nconjectured that linear substructures exist in vector spaces because relations\ncan be represented as ratios; we prove that this holds for SGNS. We provide\nnovel justification for the addition of SGNS word vectors by showing that it\nautomatically down-weights the more frequent word, as weighting schemes do ad\nhoc. Lastly, we offer an information theoretic interpretation of Euclidean\ndistance in vector spaces, justifying its use in capturing word dissimilarity.",
        "date": "2018-10-11"
    },
    "https://arxiv.org/abs/1911.09419": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "discute avec raphael",
            "hierarchy aware knowledge graph embeddings"
        ],
        "title": "[1911.09419] Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction",
        "summary": "Knowledge graph embedding, which aims to represent entities and relations as\nlow dimensional vectors (or matrices, tensors, etc.), has been shown to be a\npowerful technique for predicting missing links in knowledge graphs. Existing\nknowledge graph embedding models mainly focus on modeling relation patterns\nsuch as symmetry/antisymmetry, inversion, and composition. However, many\nexisting approaches fail to model semantic hierarchies, which are common in\nreal-world applications. To address this challenge, we propose a novel\nknowledge graph embedding model---namely, Hierarchy-Aware Knowledge Graph\nEmbedding (HAKE)---which maps entities into the polar coordinate system. HAKE\nis inspired by the fact that concentric circles in the polar coordinate system\ncan naturally reflect the hierarchy. Specifically, the radial coordinate aims\nto model entities at different levels of the hierarchy, and entities with\nsmaller radii are expected to be at higher levels; the angular coordinate aims\nto distinguish entities at the same level of the hierarchy, and these entities\nare expected to have roughly the same radii but different angles. Experiments\ndemonstrate that HAKE can effectively model the semantic hierarchies in\nknowledge graphs, and significantly outperforms existing state-of-the-art\nmethods on benchmark datasets for the link prediction task.",
        "date": "2019-11-21"
    },
    "https://arxiv.org/abs/2012.15156": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "gautier izacard",
            "nlp facebook",
            "nlp ens",
            "open domain question answering"
        ],
        "title": "[2012.15156] A Memory Efficient Baseline for Open Domain Question Answering",
        "summary": "Recently, retrieval systems based on dense representations have led to\nimportant improvements in open-domain question answering, and related tasks.\nWhile very effective, this approach is also memory intensive, as the dense\nvectors for the whole knowledge source need to be kept in memory. In this\npaper, we study how the memory footprint of dense retriever-reader systems can\nbe reduced. We consider three strategies to reduce the index size: dimension\nreduction, vector quantization and passage filtering. We evaluate our approach\non two question answering benchmarks: TriviaQA and NaturalQuestions, showing\nthat it is possible to get competitive systems using less than 6Gb of memory.",
        "date": "2020-12-30"
    },
    "https://arxiv.org/abs/1906.01195": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "acl 2019",
            "attention knowledge graphs",
            "knowledge graph embeddings"
        ],
        "title": "[1906.01195] Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs",
        "summary": "The recent proliferation of knowledge graphs (KGs) coupled with incomplete or\npartial information, in the form of missing relations (links) between entities,\nhas fueled a lot of research on knowledge base completion (also known as\nrelation prediction). Several recent works suggest that convolutional neural\nnetwork (CNN) based models generate richer and more expressive feature\nembeddings and hence also perform well on relation prediction. However, we\nobserve that these KG embeddings treat triples independently and thus fail to\ncover the complex and hidden information that is inherently implicit in the\nlocal neighborhood surrounding a triple. To this effect, our paper proposes a\nnovel attention based feature embedding that captures both entity and relation\nfeatures in any given entity's neighborhood. Additionally, we also encapsulate\nrelation clusters and multihop relations in our model. Our empirical study\noffers insights into the efficacy of our attention based model and we show\nmarked performance gains in comparison to state of the art methods on all\ndatasets.",
        "date": "2019-06-04"
    },
    "https://arxiv.org/abs/2006.07264": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "low resource languages"
        ],
        "title": "[2006.07264] Low-resource Languages: A Review of Past Work and Future Challenges",
        "summary": "A current problem in NLP is massaging and processing low-resource languages\nwhich lack useful training attributes such as supervised data, number of native\nspeakers or experts, etc. This review paper concisely summarizes previous\ngroundbreaking achievements made towards resolving this problem, and analyzes\npotential improvements in the context of the overall future research direction.",
        "date": "2020-06-12"
    },
    "https://arxiv.org/abs/1912.08422": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "ai facebook",
            "kd mkb related",
            "recommender systems",
            "knowledge distillation",
            "explainable ai"
        ],
        "title": "[1912.08422] Distilling Structured Knowledge into Embeddings for Explainable and Accurate Recommendation",
        "summary": "Recently, the embedding-based recommendation models (e.g., matrix\nfactorization and deep models) have been prevalent in both academia and\nindustry due to their effectiveness and flexibility. However, they also have\nsuch intrinsic limitations as lacking explainability and suffering from data\nsparsity. In this paper, we propose an end-to-end joint learning framework to\nget around these limitations without introducing any extra overhead by\ndistilling structured knowledge from a differentiable path-based recommendation\nmodel. Through extensive experiments, we show that our proposed framework can\nachieve state-of-the-art recommendation performance and meanwhile provide\ninterpretable recommendation reasons.",
        "date": "2019-12-18"
    },
    "https://arxiv.org/abs/2209.00099": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "language models size",
            "nlp low resource scenarios",
            "language model fine tuning"
        ],
        "title": "[2209.00099] Efficient Methods for Natural Language Processing: A Survey",
        "summary": "Getting the most out of limited resources allows advances in natural language\nprocessing (NLP) research and practice while being conservative with resources.\nThose resources may be data, time, storage, or energy. Recent work in NLP has\nyielded interesting results from scaling; however, using only scale to improve\nresults means that resource consumption also scales. That relationship\nmotivates research into efficient methods that require less resources to\nachieve similar results. This survey relates and synthesises methods and\nfindings in those efficiencies in NLP, aiming to guide new researchers in the\nfield and inspire the development of new methods.",
        "date": "2022-08-31"
    },
    "https://arxiv.org/abs/1910.03524": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "poincare embeddings",
            "vector space model",
            "geometry of language embeddings",
            "yandex",
            "graphs machine learning"
        ],
        "title": "[1910.03524] Beyond Vector Spaces: Compact Data Representation as Differentiable Weighted Graphs",
        "summary": "Learning useful representations is a key ingredient to the success of modern\nmachine learning. Currently, representation learning mostly relies on embedding\ndata into Euclidean space. However, recent work has shown that data in some\ndomains is better modeled by non-euclidean metric spaces, and inappropriate\ngeometry can result in inferior performance. In this paper, we aim to eliminate\nthe inductive bias imposed by the embedding space geometry. Namely, we propose\nto map data into more general non-vector metric spaces: a weighted graph with a\nshortest path distance. By design, such graphs can model arbitrary geometry\nwith a proper configuration of edges and weights. Our main contribution is\nPRODIGE: a method that learns a weighted graph representation of data\nend-to-end by gradient descent. Greater generality and fewer model assumptions\nmake PRODIGE more powerful than existing embedding-based approaches. We confirm\nthe superiority of our method via extensive experiments on a wide range of\ntasks, including classification, compression, and collaborative filtering.",
        "date": "2019-10-08"
    },
    "https://arxiv.org/abs/1911.03814": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "zero shot entity linking",
            "retriever reader",
            "nlp facebook",
            "entity discovery and linking",
            "blink",
            "bert based entity linking"
        ],
        "title": "[1911.03814] Scalable Zero-shot Entity Linking with Dense Entity Retrieval",
        "summary": "This paper introduces a conceptually simple, scalable, and highly effective\nBERT-based entity linking model, along with an extensive evaluation of its\naccuracy-speed trade-off. We present a two-stage zero-shot linking algorithm,\nwhere each entity is defined only by a short textual description. The first\nstage does retrieval in a dense space defined by a bi-encoder that\nindependently embeds the mention context and the entity descriptions. Each\ncandidate is then re-ranked with a cross-encoder, that concatenates the mention\nand entity text. Experiments demonstrate that this approach is state of the art\non recent zero-shot benchmarks (6 point absolute gains) and also on more\nestablished non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative\nsimplicity (e.g. no explicit entity embeddings or manually engineered mention\ntables). We also show that bi-encoder linking is very fast with nearest\nneighbour search (e.g. linking with 5.9 million candidates in 2 milliseconds),\nand that much of the accuracy gain from the more expensive cross-encoder can be\ntransferred to the bi-encoder via knowledge distillation. Our code and models\nare available at https://github.com/facebookresearch/BLINK.",
        "date": "2019-11-10"
    },
    "https://arxiv.org/abs/2007.15779": {
        "extra-tags": [],
        "tags": [
            "domain specific bert",
            "nlp microsoft",
            "biomedical nlp",
            "arxiv doc"
        ],
        "title": "[2007.15779] Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
        "summary": "Pretraining large neural language models, such as BERT, has led to impressive\ngains on many natural language processing (NLP) tasks. However, most\npretraining efforts focus on general domain corpora, such as newswire and Web.\nA prevailing assumption is that even domain-specific pretraining can benefit by\nstarting from general-domain language models. In this paper, we challenge this\nassumption by showing that for domains with abundant unlabeled text, such as\nbiomedicine, pretraining language models from scratch results in substantial\ngains over continual pretraining of general-domain language models. To\nfacilitate this investigation, we compile a comprehensive biomedical NLP\nbenchmark from publicly-available datasets. Our experiments show that\ndomain-specific pretraining serves as a solid foundation for a wide range of\nbiomedical NLP tasks, leading to new state-of-the-art results across the board.\nFurther, in conducting a thorough evaluation of modeling choices, both for\npretraining and task-specific fine-tuning, we discover that some common\npractices are unnecessary with BERT models, such as using complex tagging\nschemes in named entity recognition (NER). To help accelerate research in\nbiomedical NLP, we have released our state-of-the-art pretrained and\ntask-specific models for the community, and created a leaderboard featuring our\nBLURB benchmark (short for Biomedical Language Understanding & Reasoning\nBenchmark) at https://aka.ms/BLURB.",
        "date": "2020-07-31"
    },
    "https://arxiv.org/abs/1405.5893": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "african languages",
            "jerma",
            "songhai",
            "haoussa",
            "nlp 4 africa"
        ],
        "title": "[1405.5893] Computerization of African languages-French dictionaries",
        "summary": "This paper relates work done during the DiLAF project. It consists in\nconverting 5 bilingual African language-French dictionaries originally in Word\nformat into XML following the LMF model. The languages processed are Bambara,\nHausa, Kanuri, Tamajaq and Songhai-zarma, still considered as under-resourced\nlanguages concerning Natural Language Processing tools. Once converted, the\ndictionaries are available online on the Jibiki platform for lookup and\nmodification. The DiLAF project is first presented. A description of each\ndictionary follows. Then, the conversion methodology from .doc format to XML\nfiles is presented. A specific point on the usage of Unicode follows. Then,\neach step of the conversion into XML and LMF is detailed. The last part\npresents the Jibiki lexical resources management platform used for the project.",
        "date": "2014-05-22"
    },
    "https://arxiv.org/abs/2004.06842": {
        "extra-tags": [],
        "tags": [
            "graph embeddings",
            "arxiv doc",
            "entity recommendation",
            "discute avec raphael",
            "yahoo",
            "recommender systems",
            "brad pitt"
        ],
        "title": "[2004.06842] Layered Graph Embedding for Entity Recommendation using Wikipedia in the Yahoo! Knowledge Graph",
        "summary": "In this paper, we describe an embedding-based entity recommendation framework\nfor Wikipedia that organizes Wikipedia into a collection of graphs layered on\ntop of each other, learns complementary entity representations from their\ntopology and content, and combines them with a lightweight learning-to-rank\napproach to recommend related entities on Wikipedia. Through offline and online\nevaluations, we show that the resulting embeddings and recommendations perform\nwell in terms of quality and user engagement. Balancing simplicity and quality,\nthis framework provides default entity recommendations for English and other\nlanguages in the Yahoo! Knowledge Graph, which Wikipedia is a core subset of.",
        "date": "2020-04-15"
    },
    "https://arxiv.org/abs/2104.12016": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp stanford",
            "retrieval based nlp"
        ],
        "title": "[2104.12016] Learning Passage Impacts for Inverted Indexes",
        "summary": "Neural information retrieval systems typically use a cascading pipeline, in\nwhich a first-stage model retrieves a candidate set of documents and one or\nmore subsequent stages re-rank this set using contextualized language models\nsuch as BERT. In this paper, we propose DeepImpact, a new document\nterm-weighting scheme suitable for efficient retrieval using a standard\ninverted index. Compared to existing methods, DeepImpact improves impact-score\nmodeling and tackles the vocabulary-mismatch problem. In particular, DeepImpact\nleverages DocT5Query to enrich the document collection and, using a\ncontextualized language model, directly estimates the semantic importance of\ntokens in a document, producing a single-value representation for each token in\neach document. Our experiments show that DeepImpact significantly outperforms\nprior first-stage retrieval approaches by up to 17% on effectiveness metrics\nw.r.t. DocT5Query, and, when deployed in a re-ranking scenario, can reach the\nsame effectiveness of state-of-the-art approaches with up to 5.1x speedup in\nefficiency.",
        "date": "2021-04-24"
    },
    "https://arxiv.org/abs/1909.04939": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "time series"
        ],
        "title": "[1909.04939] InceptionTime: Finding AlexNet for Time Series Classification",
        "summary": "Time series classification (TSC) is the area of machine learning interested\nin learning how to assign labels to time series. The last few decades of work\nin this area have led to significant progress in the accuracy of classifiers,\nwith the state of the art now represented by the HIVE-COTE algorithm. While\nextremely accurate, HIVE-COTE is infeasible to use in many applications because\nof its very high training time complexity in O(N^2*T^4) for a dataset with N\ntime series of length T. For example, it takes HIVE-COTE more than 72,000s to\nlearn from a small dataset with N=700 time series of short length T=46. Deep\nlearning, on the other hand, has now received enormous attention because of its\nhigh scalability and state-of-the-art accuracy in computer vision and natural\nlanguage processing tasks. Deep learning for TSC has only very recently started\nto be explored, with the first few architectures developed over the last 3\nyears only. The accuracy of deep learning for TSC has been raised to a\ncompetitive level, but has not quite reached the level of HIVE-COTE. This is\nwhat this paper achieves: outperforming HIVE-COTE's accuracy together with\nscalability. We take an important step towards finding the AlexNet network for\nTSC by presenting InceptionTime---an ensemble of deep Convolutional Neural\nNetwork (CNN) models, inspired by the Inception-v4 architecture. Our\nexperiments show that InceptionTime slightly outperforms HIVE-COTE with a\nwin/draw/loss on the UCR archive of 40/6/39. Not only is InceptionTime more\naccurate, but it is much faster: InceptionTime learns from that same dataset\nwith 700 time series in 2,300s but can also learn from a dataset with 8M time\nseries in 13 hours, a quantity of data that is fully out of reach of HIVE-COTE.",
        "date": "2019-09-11"
    },
    "https://arxiv.org/abs/2108.13854": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "question answering",
            "domain adaptation nlp",
            "synthetic data nlp",
            "nlp low resource scenarios"
        ],
        "title": "[2108.13854] Contrastive Domain Adaptation for Question Answering using Limited Text Corpora",
        "summary": "Question generation has recently shown impressive results in customizing\nquestion answering (QA) systems to new domains. These approaches circumvent the\nneed for manually annotated training data from the new domain and, instead,\ngenerate synthetic question-answer pairs that are used for training. However,\nexisting methods for question generation rely on large amounts of synthetically\ngenerated datasets and costly computational resources, which render these\ntechniques widely inaccessible when the text corpora is of limited size. This\nis problematic as many niche domains rely on small text corpora, which\nnaturally restricts the amount of synthetic data that can be generated. In this\npaper, we propose a novel framework for domain adaptation called contrastive\ndomain adaptation for QA (CAQA). Specifically, CAQA combines techniques from\nquestion generation and domain-invariant learning to answer out-of-domain\nquestions in settings with limited text corpora. Here, we train a QA system on\nboth source data and generated data from the target domain with a contrastive\nadaptation loss that is incorporated in the training objective. By combining\ntechniques from question generation and domain-invariant learning, our model\nachieved considerable improvements compared to state-of-the-art baselines.",
        "date": "2021-08-31"
    },
    "https://arxiv.org/abs/2205.08184": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "text to text transfer transformer",
            "knowledge graph augmented language models",
            "structured knowledge",
            "nlp google"
        ],
        "title": "[2205.08184] SKILL: Structured Knowledge Infusion for Large Language Models",
        "summary": "Large language models (LLMs) have demonstrated human-level performance on a\nvast spectrum of natural language tasks. However, it is largely unexplored\nwhether they can better internalize knowledge from a structured data, such as a\nknowledge graph, or from text. In this work, we propose a method to infuse\nstructured knowledge into LLMs, by directly training T5 models on factual\ntriples of knowledge graphs (KGs). We show that models pre-trained on Wikidata\nKG with our method outperform the T5 baselines on FreebaseQA and WikiHop, as\nwell as the Wikidata-answerable subset of TriviaQA and NaturalQuestions. The\nmodels pre-trained on factual triples compare competitively with the ones on\nnatural language sentences that contain the same knowledge. Trained on a\nsmaller size KG, WikiMovies, we saw 3x improvement of exact match score on\nMetaQA task compared to T5 baseline. The proposed method has an advantage that\nno alignment between the knowledge graph and text corpus is required in\ncurating training data. This makes our method particularly useful when working\nwith industry-scale knowledge graphs.",
        "date": "2022-05-17"
    },
    "https://arxiv.org/abs/1910.12507": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "text aware kg embedding",
            "survey",
            "knowledge graph embeddings"
        ],
        "title": "[1910.12507] A Survey on Knowledge Graph Embeddings with Literals: Which model links better Literal-ly?",
        "summary": "Knowledge Graphs (KGs) are composed of structured information about a\nparticular domain in the form of entities and relations. In addition to the\nstructured information KGs help in facilitating interconnectivity and\ninteroperability between different resources represented in the Linked Data\nCloud. KGs have been used in a variety of applications such as entity linking,\nquestion answering, recommender systems, etc. However, KG applications suffer\nfrom high computational and storage costs. Hence, there arises the necessity\nfor a representation able to map the high dimensional KGs into low dimensional\nspaces, i.e., embedding space, preserving structural as well as relational\ninformation. This paper conducts a survey of KG embedding models which not only\nconsider the structured information contained in the form of entities and\nrelations in a KG but also the unstructured information represented as literals\nsuch as text, numerical values, images, etc. Along with a theoretical analysis\nand comparison of the methods proposed so far for generating KG embeddings with\nliterals, an empirical evaluation of the different methods under identical\nsettings has been performed for the general task of link prediction.",
        "date": "2019-10-28"
    },
    "https://arxiv.org/abs/2201.12431": {
        "extra-tags": [],
        "tags": [
            "arxiv doc"
        ],
        "title": "[2201.12431] Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval",
        "summary": "Retrieval-based language models (R-LM) model the probability of natural\nlanguage text by combining a standard language model (LM) with examples\nretrieved from an external datastore at test time. While effective, a major\nbottleneck of using these models in practice is the computationally costly\ndatastore search, which can be performed as frequently as every time step. In\nthis paper, we present RetoMaton - retrieval automaton - which approximates the\ndatastore search, based on (1) saving pointers between consecutive datastore\nentries, and (2) clustering of entries into \"states\". This effectively results\nin a weighted finite automaton built on top of the datastore, instead of\nrepresenting the datastore as a flat list. The creation of the automaton is\nunsupervised, and a RetoMaton can be constructed from any text collection:\neither the original training corpus or from another domain. Traversing this\nautomaton at inference time, in parallel to the LM inference, reduces its\nperplexity by up to 1.85, or alternatively saves up to 83% of the nearest\nneighbor searches over $k$NN-LM (Khandelwal et al., 2020) without hurting\nperplexity. Our code and trained models are available at\nhttps://github.com/neulab/retomaton .",
        "date": "2022-01-28"
    },
    "https://arxiv.org/abs/2104.10809": {
        "extra-tags": [],
        "tags": [
            "yoav goldberg",
            "grounded language learning",
            "arxiv doc"
        ],
        "title": "[2104.10809] Provable Limitations of Acquiring Meaning from Ungrounded Form: What will Future Language Models Understand?",
        "summary": "Language models trained on billions of tokens have recently led to\nunprecedented results on many NLP tasks. This success raises the question of\nwhether, in principle, a system can ever \"understand\" raw text without access\nto some form of grounding. We formally investigate the abilities of ungrounded\nsystems to acquire meaning. Our analysis focuses on the role of \"assertions\":\ncontexts within raw text that provide indirect clues about underlying\nsemantics. We study whether assertions enable a system to emulate\nrepresentations preserving semantic relations like equivalence. We find that\nassertions enable semantic emulation if all expressions in the language are\nreferentially transparent. However, if the language uses non-transparent\npatterns like variable binding, we show that emulation can become an\nuncomputable problem. Finally, we discuss differences between our formal model\nand natural language, exploring how our results generalize to a modal setting\nand other semantic relations. Together, our results suggest that assertions in\ncode or language do not provide sufficient signal to fully emulate semantic\nrepresentations. We formalize ways in which ungrounded language models appear\nto be fundamentally limited in their ability to \"understand\".",
        "date": "2021-04-22"
    },
    "https://arxiv.org/abs/2006.13365": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "knowledge graph embeddings"
        ],
        "title": "[2006.13365] Bringing Light Into the Dark: A Large-scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework",
        "summary": "The heterogeneity in recently published knowledge graph embedding models'\nimplementations, training, and evaluation has made fair and thorough\ncomparisons difficult. In order to assess the reproducibility of previously\npublished results, we re-implemented and evaluated 19 interaction models in the\nPyKEEN software package. Here, we outline which results could be reproduced\nwith their reported hyper-parameters, which could only be reproduced with\nalternate hyper-parameters, and which could not be reproduced at all as well as\nprovide insight as to why this might be the case.\nWe then performed a large-scale benchmarking on four datasets with several\nthousands of experiments and 21,246 GPU hours of computation time. We present\ninsights gained as to best practices, best configurations for each model, and\nwhere improvements could be made over previously published best configurations.\nOur results highlight that the combination of model architecture, training\napproach, loss function, and the explicit modeling of inverse relations is\ncrucial for a model's performances, and not only determined by the model\narchitecture. We provide evidence that several architectures can obtain results\ncompetitive to the state-of-the-art when configured carefully. We have made all\ncode, experimental configurations, results, and analyses that lead to our\ninterpretations available at https://github.com/pykeen/pykeen and\nhttps://github.com/pykeen/benchmarking",
        "date": "2020-06-23"
    },
    "https://arxiv.org/abs/1807.08447": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "graph embeddings",
            "kd mkb biblio",
            "combining knowledge graphs",
            "ai amazon"
        ],
        "title": "[1807.08447] LinkNBed: Multi-Graph Representation Learning with Entity Linkage",
        "summary": "Knowledge graphs have emerged as an important model for studying complex\nmulti-relational data. This has given rise to the construction of numerous\nlarge scale but incomplete knowledge graphs encoding information extracted from\nvarious resources. An effective and scalable approach to jointly learn over\nmultiple graphs and eventually construct a unified graph is a crucial next step\nfor the success of knowledge-based inference for many downstream applications.\nTo this end, we propose LinkNBed, a deep relational learning framework that\nlearns entity and relationship representations across multiple graphs. We\nidentify entity linkage across graphs as a vital component to achieve our goal.\nWe design a novel objective that leverage entity linkage and build an efficient\nmulti-task training procedure. Experiments on link prediction and entity\nlinkage demonstrate substantial improvements over the state-of-the-art\nrelational learning approaches.",
        "date": "2018-07-23"
    },
    "https://arxiv.org/abs/2104.09224": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "attention is all you need",
            "driverless car"
        ],
        "title": "[2104.09224] Multi-Modal Fusion Transformer for End-to-End Autonomous Driving",
        "summary": "How should representations from complementary sensors be integrated for\nautonomous driving? Geometry-based sensor fusion has shown great promise for\nperception tasks such as object detection and motion forecasting. However, for\nthe actual driving task, the global context of the 3D scene is key, e.g. a\nchange in traffic light state can affect the behavior of a vehicle\ngeometrically distant from that traffic light. Geometry alone may therefore be\ninsufficient for effectively fusing representations in end-to-end driving\nmodels. In this work, we demonstrate that imitation learning policies based on\nexisting sensor fusion methods under-perform in the presence of a high density\nof dynamic agents and complex scenarios, which require global contextual\nreasoning, such as handling traffic oncoming from multiple directions at\nuncontrolled intersections. Therefore, we propose TransFuser, a novel\nMulti-Modal Fusion Transformer, to integrate image and LiDAR representations\nusing attention. We experimentally validate the efficacy of our approach in\nurban settings involving complex scenarios using the CARLA urban driving\nsimulator. Our approach achieves state-of-the-art driving performance while\nreducing collisions by 76% compared to geometry-based fusion.",
        "date": "2021-04-19"
    },
    "https://arxiv.org/abs/1605.07723": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "snorkel"
        ],
        "title": "[1605.07723] Data Programming: Creating Large Training Sets, Quickly",
        "summary": "Large labeled training sets are the critical building blocks of supervised\nlearning methods and are key enablers of deep learning techniques. For some\napplications, creating labeled training sets is the most time-consuming and\nexpensive part of applying machine learning. We therefore propose a paradigm\nfor the programmatic creation of training sets called data programming in which\nusers express weak supervision strategies or domain heuristics as labeling\nfunctions, which are programs that label subsets of the data, but that are\nnoisy and may conflict. We show that by explicitly representing this training\nset labeling process as a generative model, we can \"denoise\" the generated\ntraining set, and establish theoretically that we can recover the parameters of\nthese generative models in a handful of settings. We then show how to modify a\ndiscriminative loss function to make it noise-aware, and demonstrate our method\nover a range of discriminative models including logistic regression and LSTMs.\nExperimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data\nprogramming would have led to a new winning score, and also show that applying\ndata programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points\nover a state-of-the-art LSTM baseline (and into second place in the\ncompetition). Additionally, in initial user studies we observed that data\nprogramming may be an easier way for non-experts to create machine learning\nmodels when training data is limited or unavailable.",
        "date": "2016-05-25"
    },
    "https://arxiv.org/abs/1905.05950": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "bertology"
        ],
        "title": "[1905.05950] BERT Rediscovers the Classical NLP Pipeline",
        "summary": "Pre-trained text encoders have rapidly advanced the state of the art on many\nNLP tasks. We focus on one such model, BERT, and aim to quantify where\nlinguistic information is captured within the network. We find that the model\nrepresents the steps of the traditional NLP pipeline in an interpretable and\nlocalizable way, and that the regions responsible for each step appear in the\nexpected sequence: POS tagging, parsing, NER, semantic roles, then coreference.\nQualitative analysis reveals that the model can and often does adjust this\npipeline dynamically, revising lower-level decisions on the basis of\ndisambiguating information from higher-level representations.",
        "date": "2019-05-15"
    },
    "https://arxiv.org/abs/2106.04098": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "entity type prediction",
            "acl 2021",
            "discute avec raphael"
        ],
        "title": "[2106.04098] Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model",
        "summary": "Recently, there is an effort to extend fine-grained entity typing by using a\nricher and ultra-fine set of types, and labeling noun phrases including\npronouns and nominal nouns instead of just named entity mentions. A key\nchallenge for this ultra-fine entity typing task is that human annotated data\nare extremely scarce, and the annotation ability of existing distant or weak\nsupervision approaches is very limited. To remedy this problem, in this paper,\nwe propose to obtain training data for ultra-fine entity typing by using a BERT\nMasked Language Model (MLM). Given a mention in a sentence, our approach\nconstructs an input for the BERT MLM so that it predicts context dependent\nhypernyms of the mention, which can be used as type labels. Experimental\nresults demonstrate that, with the help of these automatically generated\nlabels, the performance of an ultra-fine entity typing model can be improved\nsubstantially. We also show that our approach can be applied to improve\ntraditional fine-grained entity typing after performing simple type mapping.",
        "date": "2021-06-08"
    },
    "https://arxiv.org/abs/2009.00236": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "survey",
            "deep active learning"
        ],
        "title": "[2009.00236] A Survey of Deep Active Learning",
        "summary": "Active learning (AL) attempts to maximize the performance gain of the model\nby marking the fewest samples. Deep learning (DL) is greedy for data and\nrequires a large amount of data supply to optimize massive parameters, so that\nthe model learns how to extract high-quality features. In recent years, due to\nthe rapid development of internet technology, we are in an era of information\ntorrents and we have massive amounts of data. In this way, DL has aroused\nstrong interest of researchers and has been rapidly developed. Compared with\nDL, researchers have relatively low interest in AL. This is mainly because\nbefore the rise of DL, traditional machine learning requires relatively few\nlabeled samples. Therefore, early AL is difficult to reflect the value it\ndeserves. Although DL has made breakthroughs in various fields, most of this\nsuccess is due to the publicity of the large number of existing annotation\ndatasets. However, the acquisition of a large number of high-quality annotated\ndatasets consumes a lot of manpower, which is not allowed in some fields that\nrequire high expertise, especially in the fields of speech recognition,\ninformation extraction, medical images, etc. Therefore, AL has gradually\nreceived due attention. A natural idea is whether AL can be used to reduce the\ncost of sample annotations, while retaining the powerful learning capabilities\nof DL. Therefore, deep active learning (DAL) has emerged. Although the related\nresearch has been quite abundant, it lacks a comprehensive survey of DAL. This\narticle is to fill this gap, we provide a formal classification method for the\nexisting work, and a comprehensive and systematic overview. In addition, we\nalso analyzed and summarized the development of DAL from the perspective of\napplication. Finally, we discussed the confusion and problems in DAL, and gave\nsome possible development directions for DAL.",
        "date": "2020-08-30"
    },
    "https://arxiv.org/abs/2010.00402": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "hierarchical clustering",
            "ai stanford",
            "poincare embeddings",
            "ml google"
        ],
        "title": "[2010.00402] From Trees to Continuous Embeddings and Back: Hyperbolic Hierarchical Clustering",
        "summary": "Similarity-based Hierarchical Clustering (HC) is a classical unsupervised\nmachine learning algorithm that has traditionally been solved with heuristic\nalgorithms like Average-Linkage. Recently, Dasgupta reframed HC as a discrete\noptimization problem by introducing a global cost function measuring the\nquality of a given tree. In this work, we provide the first continuous\nrelaxation of Dasgupta's discrete optimization problem with provable quality\nguarantees. The key idea of our method, HypHC, is showing a direct\ncorrespondence from discrete trees to continuous representations (via the\nhyperbolic embeddings of their leaf nodes) and back (via a decoding algorithm\nthat maps leaf embeddings to a dendrogram), allowing us to search the space of\ndiscrete binary trees with continuous optimization. Building on analogies\nbetween trees and hyperbolic space, we derive a continuous analogue for the\nnotion of lowest common ancestor, which leads to a continuous relaxation of\nDasgupta's discrete objective. We can show that after decoding, the global\nminimizer of our continuous relaxation yields a discrete tree with a (1 +\nepsilon)-factor approximation for Dasgupta's optimal tree, where epsilon can be\nmade arbitrarily small and controls optimization challenges. We experimentally\nevaluate HypHC on a variety of HC benchmarks and find that even approximate\nsolutions found with gradient descent have superior clustering quality than\nagglomerative heuristics or other gradient based algorithms. Finally, we\nhighlight the flexibility of HypHC using end-to-end training in a downstream\nclassification task.",
        "date": "2020-10-01"
    },
    "https://arxiv.org/abs/2207.09980": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "graph neural networks",
            "dismult"
        ],
        "title": "[2207.09980] ReFactorGNNs: Revisiting Factorisation-based Models from a Message-Passing Perspective",
        "summary": "Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring\nsuccess for Knowledge Graph Completion (KGC) tasks, often outperforming Graph\nNeural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node\nfeatures and to generalise to unseen nodes in inductive settings. Our work\nbridges the gap between FMs and GNNs by proposing ReFactorGNNs. This new\narchitecture draws upon both modelling paradigms, which previously were largely\nthought of as disjoint. Concretely, using a message-passing formalism, we show\nhow FMs can be cast as GNNs by reformulating the gradient descent procedure as\nmessage-passing operations, which forms the basis of our ReFactorGNNs. Across a\nmultitude of well-established KGC benchmarks, our ReFactorGNNs achieve\ncomparable transductive performance to FMs, and state-of-the-art inductive\nperformance while using an order of magnitude fewer parameters.",
        "date": "2022-07-20"
    },
    "https://arxiv.org/abs/2103.12876": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "knowledge graph",
            "question answering"
        ],
        "title": "[2103.12876] Complex Factoid Question Answering with a Free-Text Knowledge Graph",
        "summary": "We introduce DELFT, a factoid question answering system which combines the\nnuance and depth of knowledge graph question answering approaches with the\nbroader coverage of free-text. DELFT builds a free-text knowledge graph from\nWikipedia, with entities as nodes and sentences in which entities co-occur as\nedges. For each question, DELFT finds the subgraph linking question entity\nnodes to candidates using text sentences as edges, creating a dense and high\ncoverage semantic graph. A novel graph neural network reasons over the\nfree-text graph-combining evidence on the nodes via information along edge\nsentences-to select a final answer. Experiments on three question answering\ndatasets show DELFT can answer entity-rich questions better than machine\nreading based models, bert-based answer ranking and memory networks. DELFT's\nadvantage comes from both the high coverage of its free-text knowledge\ngraph-more than double that of dbpedia relations-and the novel graph neural\nnetwork which reasons on the rich but noisy free-text evidence.",
        "date": "2021-03-23"
    },
    "https://arxiv.org/abs/1905.06316": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "probing ml",
            "contextualised word representations",
            "language model"
        ],
        "title": "[1905.06316] What do you learn from context? Probing for sentence structure in contextualized word representations",
        "summary": "Contextualized representation models such as ELMo (Peters et al., 2018a) and\nBERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a\ndiverse array of downstream NLP tasks. Building on recent token-level probing\nwork, we introduce a novel edge probing task design and construct a broad suite\nof sub-sentence tasks derived from the traditional structured NLP pipeline. We\nprobe word-level contextual representations from four recent models and\ninvestigate how they encode sentence structure across a range of syntactic,\nsemantic, local, and long-range phenomena. We find that existing models trained\non language modeling and translation produce strong representations for\nsyntactic phenomena, but only offer comparably small improvements on semantic\ntasks over a non-contextual baseline.",
        "date": "2019-05-15"
    },
    "https://arxiv.org/abs/1906.04341": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "what s encoded by a nn",
            "deep learning attention",
            "chris manning",
            "bert",
            "bertology"
        ],
        "title": "[1906.04341] What Does BERT Look At? An Analysis of BERT's Attention",
        "summary": "Large pre-trained neural networks such as BERT have had great recent success\nin NLP, motivating a growing body of research investigating what aspects of\nlanguage they are able to learn from unlabeled data. Most recent analysis has\nfocused on model outputs (e.g., language model surprisal) or internal vector\nrepresentations (e.g., probing classifiers). Complementary to these works, we\npropose methods for analyzing the attention mechanisms of pre-trained models\nand apply them to BERT. BERT's attention heads exhibit patterns such as\nattending to delimiter tokens, specific positional offsets, or broadly\nattending over the whole sentence, with heads in the same layer often\nexhibiting similar behaviors. We further show that certain attention heads\ncorrespond well to linguistic notions of syntax and coreference. For example,\nwe find heads that attend to the direct objects of verbs, determiners of nouns,\nobjects of prepositions, and coreferent mentions with remarkably high accuracy.\nLastly, we propose an attention-based probing classifier and use it to further\ndemonstrate that substantial syntactic information is captured in BERT's\nattention.",
        "date": "2019-06-11"
    },
    "https://arxiv.org/abs/2210.16773": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "emnlp 2022",
            "knowledge intensive nlp tasks"
        ],
        "title": "[2210.16773] An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks",
        "summary": "Access to external knowledge is essential for many natural language\nprocessing tasks, such as question answering and dialogue. Existing methods\noften rely on a parametric model that stores knowledge in its parameters, or\nuse a retrieval-augmented model that has access to an external knowledge\nsource. Parametric and retrieval-augmented models have complementary strengths\nin terms of computational efficiency and predictive accuracy. To combine the\nstrength of both approaches, we propose the Efficient Memory-Augmented\nTransformer (EMAT) -- it encodes external knowledge into a key-value memory and\nexploits the fast maximum inner product search for memory querying. We also\nintroduce pre-training tasks that allow EMAT to encode informative key-value\nrepresentations, and to learn an implicit strategy to integrate multiple memory\nslots into the transformer. Experiments on various knowledge-intensive tasks\nsuch as question answering and dialogue datasets show that, simply augmenting\nparametric models (T5-base) using our method produces more accurate results\n(e.g., 25.8 -> 44.3 EM on NQ) while retaining a high throughput (e.g., 1000\nqueries/s on NQ). Compared to retrieval-augmented models, EMAT runs\nsubstantially faster across the board and produces more accurate results on WoW\nand ELI5. Our code and datasets are available at https://github.\ncom/uclnlp/EMAT.",
        "date": "2022-10-30"
    },
    "https://arxiv.org/abs/1807.04905": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "entity type prediction",
            "acl 2018",
            "allen institute for ai a2i"
        ],
        "title": "[1807.04905] Ultra-Fine Entity Typing",
        "summary": "We introduce a new entity typing task: given a sentence with an entity\nmention, the goal is to predict a set of free-form phrases (e.g. skyscraper,\nsongwriter, or criminal) that describe appropriate types for the target entity.\nThis formulation allows us to use a new type of distant supervision at large\nscale: head words, which indicate the type of the noun phrases they appear in.\nWe show that these ultra-fine types can be crowd-sourced, and introduce new\nevaluation sets that are much more diverse and fine-grained than existing\nbenchmarks. We present a model that can predict open types, and is trained\nusing a multitask objective that pools our new head-word supervision with prior\nsupervision from entity linking. Experimental results demonstrate that our\nmodel is effective in predicting entity types at varying granularity; it\nachieves state of the art performance on an existing fine-grained entity typing\nbenchmark, and sets baselines for our newly-introduced datasets. Our data and\nmodel can be downloaded from: http://nlp.cs.washington.edu/entity_type",
        "date": "2018-07-13"
    },
    "https://arxiv.org/abs/1909.03186": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "automatic summarization",
            "rigolo",
            "attention is all you need"
        ],
        "title": "[1909.03186] On Extractive and Abstractive Neural Document Summarization with Transformer Language Models",
        "summary": "We present a method to produce abstractive summaries of long documents that\nexceed several thousand words via neural abstractive summarization. We perform\na simple extractive step before generating a summary, which is then used to\ncondition the transformer language model on relevant information before being\ntasked with generating a summary. We show that this extractive step\nsignificantly improves summarization results. We also show that this approach\nproduces more abstractive summaries compared to prior work that employs a copy\nmechanism while still achieving higher rouge scores. Note: The abstract above\nwas not written by the authors, it was generated by one of the models presented\nin this paper.",
        "date": "2019-09-07"
    },
    "https://arxiv.org/abs/2006.09462": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "question answering",
            "uncertainty in deep learning",
            "softmax",
            "trust in nlp",
            "nlp stanford"
        ],
        "title": "[2006.09462] Selective Question Answering under Domain Shift",
        "summary": "To avoid giving wrong answers, question answering (QA) models need to know\nwhen to abstain from answering. Moreover, users often ask questions that\ndiverge from the model's training data, making errors more likely and thus\nabstention more critical. In this work, we propose the setting of selective\nquestion answering under domain shift, in which a QA model is tested on a\nmixture of in-domain and out-of-domain data, and must answer (i.e., not abstain\non) as many questions as possible while maintaining high accuracy. Abstention\npolicies based solely on the model's softmax probabilities fare poorly, since\nmodels are overconfident on out-of-domain inputs. Instead, we train a\ncalibrator to identify inputs on which the QA model errs, and abstain when it\npredicts an error is likely. Crucially, the calibrator benefits from observing\nthe model's behavior on out-of-domain data, even if from a different domain\nthan the test data. We combine this method with a SQuAD-trained QA model and\nevaluate on mixtures of SQuAD and five other QA datasets. Our method answers\n56% of questions while maintaining 80% accuracy; in contrast, directly using\nthe model's probabilities only answers 48% at 80% accuracy.",
        "date": "2020-06-16"
    },
    "https://arxiv.org/abs/1903.04197": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "pixelwise dense prediction"
        ],
        "title": "[1903.04197] Structured Knowledge Distillation for Dense Prediction",
        "summary": "In this paper, we consider transferring the structure information from large\nnetworks to small ones for dense prediction tasks. Previous knowledge\ndistillation strategies used for dense prediction tasks often directly borrow\nthe distillation scheme for image classification and perform knowledge\ndistillation for each pixel separately, leading to sub-optimal performance.\nHere we propose to distill structured knowledge from large networks to small\nnetworks, taking into account the fact that dense prediction is a structured\nprediction problem. Specifically, we study two structured distillation schemes:\ni)pair-wise distillation that distills the pairwise similarities by building a\nstatic graph, and ii)holistic distillation that uses adversarial training to\ndistill holistic knowledge. The effectiveness of our knowledge distillation\napproaches is demonstrated by extensive experiments on three dense prediction\ntasks: semantic segmentation, depth estimation, and object detection.",
        "date": "2019-03-11"
    },
    "https://arxiv.org/abs/1910.09760": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "knowledge graph",
            "question answering"
        ],
        "title": "[1910.09760] Question Answering over Knowledge Graphs via Structural Query Patterns",
        "summary": "Natural language question answering over knowledge graphs is an important and\ninteresting task as it enables common users to gain accurate answers in an easy\nand intuitive manner. However, it remains a challenge to bridge the gap between\nunstructured questions and structured knowledge graphs. To address the problem,\na natural discipline is building a structured query to represent the input\nquestion. Searching the structured query over the knowledge graph can produce\nanswers to the question. Distinct from the existing methods that are based on\nsemantic parsing or templates, we propose an effective approach powered by a\nnovel notion, structural query pattern, in this paper. Given an input question,\nwe first generate its query sketch that is compatible with the underlying\nstructure of the knowledge graph. Then, we complete the query graph by labeling\nthe nodes and edges under the guidance of the structural query pattern.\nFinally, answers can be retrieved by executing the constructed query graph over\nthe knowledge graph. Evaluations on three question answering benchmarks show\nthat our proposed approach outperforms state-of-the-art methods significantly.",
        "date": "2019-10-22"
    },
    "https://arxiv.org/abs/1802.05930": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "deep learning attention",
            "text kg and embeddings"
        ],
        "title": "[1802.05930] Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing",
        "summary": "Machine Learning has been the quintessential solution for many AI problems,\nbut learning is still heavily dependent on the specific training data. Some\nlearning models can be incorporated with a prior knowledge in the Bayesian set\nup, but these learning models do not have the ability to access any organised\nworld knowledge on demand. In this work, we propose to enhance learning models\nwith world knowledge in the form of Knowledge Graph (KG) fact triples for\nNatural Language Processing (NLP) tasks. Our aim is to develop a deep learning\nmodel that can extract relevant prior support facts from knowledge graphs\ndepending on the task using attention mechanism. We introduce a\nconvolution-based model for learning representations of knowledge graph entity\nand relation clusters in order to reduce the attention space. We show that the\nproposed method is highly scalable to the amount of prior information that has\nto be processed and can be applied to any generic NLP task. Using this method\nwe show significant improvement in performance for text classification with\nNews20, DBPedia datasets and natural language inference with Stanford Natural\nLanguage Inference (SNLI) dataset. We also demonstrate that a deep learning\nmodel can be trained well with substantially less amount of labeled training\ndata, when it has access to organised world knowledge in the form of knowledge\ngraph.",
        "date": "2018-02-16"
    },
    "https://arxiv.org/abs/2204.08491": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "ai stanford",
            "pretrained models",
            "active learning"
        ],
        "title": "[2204.08491] Active Learning Helps Pretrained Models Learn the Intended Task",
        "summary": "Models can fail in unpredictable ways during deployment due to task\nambiguity, when multiple behaviors are consistent with the provided training\ndata. An example is an object classifier trained on red squares and blue\ncircles: when encountering blue squares, the intended behavior is undefined. We\ninvestigate whether pretrained models are better active learners, capable of\ndisambiguating between the possible tasks a user may be trying to specify.\nIntriguingly, we find that better active learning is an emergent property of\nthe pretraining process: pretrained models require up to 5 times fewer labels\nwhen using uncertainty-based active learning, while non-pretrained models see\nno or even negative benefit. We find these gains come from an ability to select\nexamples with attributes that disambiguate the intended behavior, such as rare\nproduct categories or atypical backgrounds. These attributes are far more\nlinearly separable in pretrained model's representation spaces vs\nnon-pretrained models, suggesting a possible mechanism for this behavior.",
        "date": "2022-04-18"
    },
    "https://arxiv.org/abs/1607.00653": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "node2vec",
            "jure leskovec"
        ],
        "title": "[1607.00653] node2vec: Scalable Feature Learning for Networks",
        "summary": "Prediction tasks over nodes and edges in networks require careful effort in\nengineering features used by learning algorithms. Recent research in the\nbroader field of representation learning has led to significant progress in\nautomating prediction by learning the features themselves. However, present\nfeature learning approaches are not expressive enough to capture the diversity\nof connectivity patterns observed in networks. Here we propose node2vec, an\nalgorithmic framework for learning continuous feature representations for nodes\nin networks. In node2vec, we learn a mapping of nodes to a low-dimensional\nspace of features that maximizes the likelihood of preserving network\nneighborhoods of nodes. We define a flexible notion of a node's network\nneighborhood and design a biased random walk procedure, which efficiently\nexplores diverse neighborhoods. Our algorithm generalizes prior work which is\nbased on rigid notions of network neighborhoods, and we argue that the added\nflexibility in exploring neighborhoods is the key to learning richer\nrepresentations. We demonstrate the efficacy of node2vec over existing\nstate-of-the-art techniques on multi-label classification and link prediction\nin several real-world networks from diverse domains. Taken together, our work\nrepresents a new way for efficiently learning state-of-the-art task-independent\nrepresentations in complex networks.",
        "date": "2016-07-03"
    },
    "https://arxiv.org/abs/2004.14545": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "explainable ai",
            "survey"
        ],
        "title": "[2004.14545] Explainable Deep Learning: A Field Guide for the Uninitiated",
        "summary": "Deep neural network (DNN) is an indispensable machine learning tool for\nachieving human-level performance on many learning tasks. Yet, due to its\nblack-box nature, it is inherently difficult to understand which aspects of the\ninput data drive the decisions of the network. There are various real-world\nscenarios in which humans need to make actionable decisions based on the output\nDNNs. Such decision support systems can be found in critical domains, such as\nlegislation, law enforcement, etc. It is important that the humans making\nhigh-level decisions can be sure that the DNN decisions are driven by\ncombinations of data features that are appropriate in the context of the\ndeployment of the decision support system and that the decisions made are\nlegally or ethically defensible. Due to the incredible pace at which DNN\ntechnology is being developed, the development of new methods and studies on\nexplaining the decision-making process of DNNs has blossomed into an active\nresearch field. A practitioner beginning to study explainable deep learning may\nbe intimidated by the plethora of orthogonal directions the field is taking.\nThis complexity is further exacerbated by the general confusion that exists in\ndefining what it means to be able to explain the actions of a deep learning\nsystem and to evaluate a system's \"ability to explain\". To alleviate this\nproblem, this article offers a \"field guide\" to deep learning explainability\nfor those uninitiated in the field. The field guide: i) Discusses the traits of\na deep learning system that researchers enhance in explainability research, ii)\nplaces explainability in the context of other related deep learning research\nareas, and iii) introduces three simple dimensions defining the space of\nfoundational methods that contribute to explainable deep learning. The guide is\ndesigned as an easy-to-digest starting point for those just embarking in the\nfield.",
        "date": "2020-04-30"
    },
    "https://arxiv.org/abs/2009.07938": {
        "extra-tags": [],
        "tags": [
            "ai ibm",
            "knowledge graph completion",
            "link prediction",
            "arxiv doc"
        ],
        "title": "[2009.07938] Type-augmented Relation Prediction in Knowledge Graphs",
        "summary": "Knowledge graphs (KGs) are of great importance to many real world\napplications, but they generally suffer from incomplete information in the form\nof missing relations between entities. Knowledge graph completion (also known\nas relation prediction) is the task of inferring missing facts given existing\nones. Most of the existing work is proposed by maximizing the likelihood of\nobserved instance-level triples. Not much attention, however, is paid to the\nontological information, such as type information of entities and relations. In\nthis work, we propose a type-augmented relation prediction (TaRP) method, where\nwe apply both the type information and instance-level information for relation\nprediction. In particular, type information and instance-level information are\nencoded as prior probabilities and likelihoods of relations respectively, and\nare combined by following Bayes' rule. Our proposed TaRP method achieves\nsignificantly better performance than state-of-the-art methods on three\nbenchmark datasets: FB15K, YAGO26K-906, and DB111K-174. In addition, we show\nthat TaRP achieves significantly improved data efficiency. More importantly,\nthe type information extracted from a specific dataset can generalize well to\nother datasets through the proposed TaRP model.",
        "date": "2020-09-16"
    },
    "https://arxiv.org/abs/2004.09095": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp microsoft",
            "low resource languages"
        ],
        "title": "[2004.09095] The State and Fate of Linguistic Diversity and Inclusion in the NLP World",
        "summary": "Language technologies contribute to promoting multilingualism and linguistic\ndiversity around the world. However, only a very small number of the over 7000\nlanguages of the world are represented in the rapidly evolving language\ntechnologies and applications. In this paper we look at the relation between\nthe types of languages, resources, and their representation in NLP conferences\nto understand the trajectory that different languages have followed over time.\nOur quantitative investigation underlines the disparity between languages,\nespecially in terms of their resources, and calls into question the \"language\nagnostic\" status of current models and systems. Through this paper, we attempt\nto convince the ACL community to prioritise the resolution of the predicaments\nhighlighted here, so that no language is left behind.",
        "date": "2020-04-20"
    },
    "https://arxiv.org/abs/1904.09078": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "multimodal classification"
        ],
        "title": "[1904.09078] EmbraceNet: A robust deep learning architecture for multimodal classification",
        "summary": "Classification using multimodal data arises in many machine learning\napplications. It is crucial not only to model cross-modal relationship\neffectively but also to ensure robustness against loss of part of data or\nmodalities. In this paper, we propose a novel deep learning-based multimodal\nfusion architecture for classification tasks, which guarantees compatibility\nwith any kind of learning models, deals with cross-modal information carefully,\nand prevents performance degradation due to partial absence of data. We employ\ntwo datasets for multimodal classification tasks, build models based on our\narchitecture and other state-of-the-art models, and analyze their performance\non various situations. The results show that our architecture outperforms the\nother multimodal fusion architectures when some parts of data are not\navailable.",
        "date": "2019-04-19"
    },
    "https://arxiv.org/abs/1707.00306": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "cluster analysis"
        ],
        "title": "[1707.00306] Variable Selection Methods for Model-based Clustering",
        "summary": "Model-based clustering is a popular approach for clustering multivariate data\nwhich has seen applications in numerous fields. Nowadays, high-dimensional data\nare more and more common and the model-based clustering approach has adapted to\ndeal with the increasing dimensionality. In particular, the development of\nvariable selection techniques has received a lot of attention and research\neffort in recent years. Even for small size problems, variable selection has\nbeen advocated to facilitate the interpretation of the clustering results. This\nreview provides a summary of the methods developed for variable selection in\nmodel-based clustering. Existing R packages implementing the different methods\nare indicated and illustrated in application to two data analysis examples.",
        "date": "2017-07-02"
    },
    "https://arxiv.org/abs/2206.10658": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp facebook",
            "retriever",
            "open domain question answering",
            "nlp google"
        ],
        "title": "[2206.10658] Questions Are All You Need to Train a Dense Passage Retriever",
        "summary": "We introduce ART, a new corpus-level autoencoding approach for training dense\nretrieval models that does not require any labeled training data. Dense\nretrieval is a central challenge for open-domain tasks, such as Open QA, where\nstate-of-the-art methods typically require large supervised datasets with\ncustom hard-negative mining and denoising of positive examples. ART, in\ncontrast, only requires access to unpaired inputs and outputs (e.g. questions\nand potential answer documents). It uses a new document-retrieval autoencoding\nscheme, where (1) an input question is used to retrieve a set of evidence\ndocuments, and (2) the documents are then used to compute the probability of\nreconstructing the original question. Training for retrieval based on question\nreconstruction enables effective unsupervised learning of both document and\nquestion encoders, which can be later incorporated into complete Open QA\nsystems without any further finetuning. Extensive experiments demonstrate that\nART obtains state-of-the-art results on multiple QA retrieval benchmarks with\nonly generic initialization from a pre-trained language model, removing the\nneed for labeled data and task-specific losses.",
        "date": "2022-06-21"
    },
    "https://arxiv.org/abs/1602.01137": {
        "extra-tags": [],
        "tags": [
            "bhaskar mitra",
            "embeddings in ir",
            "arxiv doc",
            "ranking information retrieval"
        ],
        "title": "[1602.01137] A Dual Embedding Space Model for Document Ranking",
        "summary": "A fundamental goal of search engines is to identify, given a query, documents\nthat have relevant text. This is intrinsically difficult because the query and\nthe document may use different vocabulary, or the document may contain query\nwords without being relevant. We investigate neural word embeddings as a source\nof evidence in document ranking. We train a word2vec embedding model on a large\nunlabelled query corpus, but in contrast to how the model is commonly used, we\nretain both the input and the output projections, allowing us to leverage both\nthe embedding spaces to derive richer distributional relationships. During\nranking we map the query words into the input space and the document words into\nthe output space, and compute a query-document relevance score by aggregating\nthe cosine similarities across all the query-document word pairs.\nWe postulate that the proposed Dual Embedding Space Model (DESM) captures\nevidence on whether a document is about a query term in addition to what is\nmodelled by traditional term-frequency based approaches. Our experiments show\nthat the DESM can re-rank top documents returned by a commercial Web search\nengine, like Bing, better than a term-matching based signal like TF-IDF.\nHowever, when ranking a larger set of candidate documents, we find the\nembeddings-based approach is prone to false positives, retrieving documents\nthat are only loosely related to the query. We demonstrate that this problem\ncan be solved effectively by ranking based on a linear mixture of the DESM and\nthe word counting features.",
        "date": "2016-02-02"
    },
    "https://arxiv.org/abs/1912.03263": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "statistical classification"
        ],
        "title": "[1912.03263] Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One",
        "summary": "We propose to reinterpret a standard discriminative classifier of p(y|x) as\nan energy based model for the joint distribution p(x,y). In this setting, the\nstandard class probabilities can be easily computed as well as unnormalized\nvalues of p(x) and p(x|y). Within this framework, standard discriminative\narchitectures may beused and the model can also be trained on unlabeled data.\nWe demonstrate that energy based training of the joint distribution improves\ncalibration, robustness, andout-of-distribution detection while also enabling\nour models to generate samplesrivaling the quality of recent GAN approaches. We\nimprove upon recently proposed techniques for scaling up the training of energy\nbased models and presentan approach which adds little overhead compared to\nstandard classification training. Our approach is the first to achieve\nperformance rivaling the state-of-the-artin both generative and discriminative\nlearning within one hybrid model.",
        "date": "2019-12-06"
    },
    "https://arxiv.org/abs/2001.04451": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "reformer",
            "google research"
        ],
        "title": "[2001.04451] Reformer: The Efficient Transformer",
        "summary": "Large Transformer models routinely achieve state-of-the-art results on a\nnumber of tasks but training these models can be prohibitively costly,\nespecially on long sequences. We introduce two techniques to improve the\nefficiency of Transformers. For one, we replace dot-product attention by one\nthat uses locality-sensitive hashing, changing its complexity from O($L^2$) to\nO($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use\nreversible residual layers instead of the standard residuals, which allows\nstoring activations only once in the training process instead of $N$ times,\nwhere $N$ is the number of layers. The resulting model, the Reformer, performs\non par with Transformer models while being much more memory-efficient and much\nfaster on long sequences.",
        "date": "2020-01-13"
    },
    "https://arxiv.org/abs/2211.03318": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "chris manning",
            "emnlp 2022"
        ],
        "title": "[2211.03318] Fixing Model Bugs with Natural Language Patches",
        "summary": "Current approaches for fixing systematic problems in NLP models (e.g. regex\npatches, finetuning on more data) are either brittle, or labor-intensive and\nliable to shortcuts. In contrast, humans often provide corrections to each\nother through natural language. Taking inspiration from this, we explore\nnatural language patches -- declarative statements that allow developers to\nprovide corrective feedback at the right level of abstraction, either\noverriding the model (``if a review gives 2 stars, the sentiment is negative'')\nor providing additional information the model may lack (``if something is\ndescribed as the bomb, then it is good''). We model the task of determining if\na patch applies separately from the task of integrating patch information, and\nshow that with a small amount of synthetic data, we can teach models to\neffectively use real patches on real data -- 1 to 7 patches improve accuracy by\n~1-4 accuracy points on different slices of a sentiment analysis dataset, and\nF1 by 7 points on a relation extraction dataset. Finally, we show that\nfinetuning on as many as 100 labeled examples may be needed to match the\nperformance of a small set of language patches.",
        "date": "2022-11-07"
    },
    "https://arxiv.org/abs/2212.10380": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "dual encoders ir",
            "dense passage retrieval"
        ],
        "title": "[2212.10380] What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary",
        "summary": "Dual encoders are now the dominant architecture for dense retrieval. Yet, we\nhave little understanding of how they represent text, and why this leads to\ngood performance. In this work, we shed light on this question via\ndistributions over the vocabulary. We propose to interpret the vector\nrepresentations produced by dual encoders by projecting them into the model's\nvocabulary space. We show that the resulting distributions over vocabulary\ntokens are intuitive and contain rich semantic information. We find that this\nview can explain some of the failure cases of dense retrievers. For example,\nthe inability of models to handle tail entities can be explained via a tendency\nof the token distributions to forget some of the tokens of those entities. We\nleverage this insight and propose a simple way to enrich query and passage\nrepresentations with lexical information at inference time, and show that this\nsignificantly improves performance compared to the original model in\nout-of-domain settings.",
        "date": "2022-12-20"
    },
    "https://arxiv.org/abs/1911.03903": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "critical evaluation",
            "knowledge graph completion"
        ],
        "title": "[1911.03903] A Re-evaluation of Knowledge Graph Completion Methods",
        "summary": "Knowledge Graph Completion (KGC) aims at automatically predicting missing\nlinks for large-scale knowledge graphs. A vast number of state-of-the-art KGC\ntechniques have got published at top conferences in several research fields,\nincluding data mining, machine learning, and natural language processing.\nHowever, we notice that several recent papers report very high performance,\nwhich largely outperforms previous state-of-the-art methods. In this paper, we\nfind that this can be attributed to the inappropriate evaluation protocol used\nby them and propose a simple evaluation protocol to address this problem. The\nproposed protocol is robust to handle bias in the model, which can\nsubstantially affect the final results. We conduct extensive experiments and\nreport the performance of several existing methods using our protocol. The\nreproducible code has been made publicly available",
        "date": "2019-11-10"
    },
    "https://arxiv.org/abs/1911.02655": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp microsoft",
            "domain adaptation nlp",
            "nlp automotive",
            "nlp low resource scenarios",
            "extractive question answering"
        ],
        "title": "[1911.02655] Towards Domain Adaptation from Limited Data for Question Answering Using Deep Neural Networks",
        "summary": "This paper explores domain adaptation for enabling question answering (QA)\nsystems to answer questions posed against documents in new specialized domains.\nCurrent QA systems using deep neural network (DNN) technology have proven\neffective for answering general purpose factoid-style questions. However,\ncurrent general purpose DNN models tend to be ineffective for use in new\nspecialized domains. This paper explores the effectiveness of transfer learning\ntechniques for this problem. In experiments on question answering in the\nautomobile manual domain we demonstrate that standard DNN transfer learning\ntechniques work surprisingly well in adapting DNN models to a new domain using\nlimited amounts of annotated training data in the new domain.",
        "date": "2019-11-06"
    },
    "https://arxiv.org/abs/2205.11498": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nils reimers",
            "domain adaptation",
            "dense retriever",
            "gpl generative pseudo labeling"
        ],
        "title": "[2205.11498] Domain Adaptation for Memory-Efficient Dense Retrieval",
        "summary": "Dense retrievers encode documents into fixed dimensional embeddings. However,\nstoring all the document embeddings within an index produces bulky indexes\nwhich are expensive to serve. Recently, BPR (Yamada et al., 2021) and JPQ (Zhan\net al., 2021a) have been proposed which train the model to produce binary\ndocument vectors, which reduce the index 32x and more. The authors showed these\nbinary embedding models significantly outperform more traditional index\ncompression techniques like Product Quantization (PQ). Previous work evaluated\nthese approaches just in-domain, i.e. the methods were evaluated on tasks for\nwhich training data is available. In practice, retrieval models are often used\nin an out-of-domain setting, where they have been trained on a publicly\navailable dataset, like MS MARCO, but are then used for some custom dataset for\nwhich no training data is available.\nIn this work, we show that binary embedding models like BPR and JPQ can\nperform significantly worse than baselines once there is a domain-shift\ninvolved. We propose a modification to the training procedure of BPR and JPQ\nand combine it with a corpus specific generative procedure which allow the\nadaptation of BPR and JPQ to any corpus without requiring labeled training\ndata. Our domain-adapted strategy known as GPL is model agnostic, achieves an\nimprovement by up-to 19.3 and 11.6 points in nDCG@10 across the BEIR benchmark\nin comparison to BPR and JPQ while maintaining its 32x memory efficiency.\nJPQ+GPL even outperforms our upper baseline: uncompressed TAS-B model on\naverage by 2.0 points.",
        "date": "2022-05-23"
    },
    "https://arxiv.org/abs/1911.02116": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp facebook",
            "multilingual embeddings",
            "cross lingual nlp"
        ],
        "title": "[1911.02116] Unsupervised Cross-lingual Representation Learning at Scale",
        "summary": "This paper shows that pretraining multilingual language models at scale leads\nto significant performance gains for a wide range of cross-lingual transfer\ntasks. We train a Transformer-based masked language model on one hundred\nlanguages, using more than two terabytes of filtered CommonCrawl data. Our\nmodel, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a\nvariety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI,\n+13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs\nparticularly well on low-resource languages, improving 15.7% in XNLI accuracy\nfor Swahili and 11.4% for Urdu over previous XLM models. We also present a\ndetailed empirical analysis of the key factors that are required to achieve\nthese gains, including the trade-offs between (1) positive transfer and\ncapacity dilution and (2) the performance of high and low resource languages at\nscale. Finally, we show, for the first time, the possibility of multilingual\nmodeling without sacrificing per-language performance; XLM-R is very\ncompetitive with strong monolingual models on the GLUE and XNLI benchmarks. We\nwill make our code, data and models publicly available.",
        "date": "2019-11-05"
    },
    "https://arxiv.org/abs/1911.01464": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "pre trained language models",
            "nlp facebook",
            "cross lingual nlp",
            "bertology"
        ],
        "title": "[1911.01464] Emerging Cross-lingual Structure in Pretrained Language Models",
        "summary": "We study the problem of multilingual masked language modeling, i.e. the\ntraining of a single model on concatenated text from multiple languages, and\npresent a detailed study of several factors that influence why these models are\nso effective for cross-lingual transfer. We show, contrary to what was\npreviously hypothesized, that transfer is possible even when there is no shared\nvocabulary across the monolingual corpora and also when the text comes from\nvery different domains. The only requirement is that there are some shared\nparameters in the top layers of the multi-lingual encoder. To better understand\nthis result, we also show that representations from independently trained\nmodels in different languages can be aligned post-hoc quite effectively,\nstrongly suggesting that, much like for non-contextual word embeddings, there\nare universal latent symmetries in the learned embedding spaces. For\nmultilingual masked language modeling, these symmetries seem to be\nautomatically discovered and aligned during the joint training process.",
        "date": "2019-11-04"
    },
    "https://arxiv.org/abs/2004.12832": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "colbert"
        ],
        "title": "[2004.12832] ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT",
        "summary": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced\nadvances in Information Retrieval (IR), largely owed to fine-tuning deep\nlanguage models (LMs) for document ranking. While remarkably effective, the\nranking models based on these LMs increase computational cost by orders of\nmagnitude over prior approaches, particularly as they must feed each\nquery-document pair through a massive neural network to compute a single\nrelevance score. To tackle this, we present ColBERT, a novel ranking model that\nadapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT\nintroduces a late interaction architecture that independently encodes the query\nand the document using BERT and then employs a cheap yet powerful interaction\nstep that models their fine-grained similarity. By delaying and yet retaining\nthis fine-granular interaction, ColBERT can leverage the expressiveness of deep\nLMs while simultaneously gaining the ability to pre-compute document\nrepresentations offline, considerably speeding up query processing. Beyond\nreducing the cost of re-ranking the documents retrieved by a traditional model,\nColBERT's pruning-friendly interaction mechanism enables leveraging\nvector-similarity indexes for end-to-end retrieval directly from a large\ndocument collection. We extensively evaluate ColBERT using two recent passage\nsearch datasets. Results show that ColBERT's effectiveness is competitive with\nexisting BERT-based models (and outperforms every non-BERT baseline), while\nexecuting two orders-of-magnitude faster and requiring four orders-of-magnitude\nfewer FLOPs per query.",
        "date": "2020-04-27"
    },
    "https://arxiv.org/abs/1910.01348": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "knowledge distillation",
            "critical evaluation"
        ],
        "title": "[1910.01348] On the Efficacy of Knowledge Distillation",
        "summary": "In this paper, we present a thorough evaluation of the efficacy of knowledge\ndistillation and its dependence on student and teacher architectures. Starting\nwith the observation that more accurate teachers often don't make good\nteachers, we attempt to tease apart the factors that affect knowledge\ndistillation performance. We find crucially that larger models do not often\nmake better teachers. We show that this is a consequence of mismatched\ncapacity, and that small students are unable to mimic large teachers. We find\ntypical ways of circumventing this (such as performing a sequence of knowledge\ndistillation steps) to be ineffective. Finally, we show that this effect can be\nmitigated by stopping the teacher's training early. Our results generalize\nacross datasets and models.",
        "date": "2019-10-03"
    },
    "https://arxiv.org/abs/1912.01412": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "guillaume lample",
            "facebook fair",
            "nlp facebook",
            "connectionist vs symbolic debate",
            "deep learning",
            "mathematiques"
        ],
        "title": "[1912.01412] Deep Learning for Symbolic Mathematics",
        "summary": "Neural networks have a reputation for being better at solving statistical or\napproximate problems than at performing calculations or working with symbolic\ndata. In this paper, we show that they can be surprisingly good at more\nelaborated tasks in mathematics, such as symbolic integration and solving\ndifferential equations. We propose a syntax for representing mathematical\nproblems, and methods for generating large datasets that can be used to train\nsequence-to-sequence models. We achieve results that outperform commercial\nComputer Algebra Systems such as Matlab or Mathematica.",
        "date": "2019-12-02"
    },
    "https://arxiv.org/abs/1709.07604": {
        "extra-tags": [],
        "tags": [
            "graph embeddings",
            "survey",
            "arxiv doc"
        ],
        "title": "[1709.07604] A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications",
        "summary": "Graph is an important data representation which appears in a wide diversity\nof real-world scenarios. Effective graph analytics provides users a deeper\nunderstanding of what is behind the data, and thus can benefit a lot of useful\napplications such as node classification, node recommendation, link prediction,\netc. However, most graph analytics methods suffer the high computation and\nspace cost. Graph embedding is an effective yet efficient way to solve the\ngraph analytics problem. It converts the graph data into a low dimensional\nspace in which the graph structural information and graph properties are\nmaximally preserved. In this survey, we conduct a comprehensive review of the\nliterature in graph embedding. We first introduce the formal definition of\ngraph embedding as well as the related concepts. After that, we propose two\ntaxonomies of graph embedding which correspond to what challenges exist in\ndifferent graph embedding problem settings and how the existing work address\nthese challenges in their solutions. Finally, we summarize the applications\nthat graph embedding enables and suggest four promising future research\ndirections in terms of computation efficiency, problem settings, techniques and\napplication scenarios.",
        "date": "2017-09-22"
    },
    "https://arxiv.org/abs/2006.15020": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp facebook",
            "sequence to sequence learning",
            "nlp pretraining",
            "zero shot learning"
        ],
        "title": "[2006.15020] Pre-training via Paraphrasing",
        "summary": "We introduce MARGE, a pre-trained sequence-to-sequence model learned with an\nunsupervised multi-lingual multi-document paraphrasing objective. MARGE\nprovides an alternative to the dominant masked language modeling paradigm,\nwhere we self-supervise the reconstruction of target text by retrieving a set\nof related texts (in many languages) and conditioning on them to maximize the\nlikelihood of generating the original. We show it is possible to jointly learn\nto do retrieval and reconstruction, given only a random initialization. The\nobjective noisily captures aspects of paraphrase, translation, multi-document\nsummarization, and information retrieval, allowing for strong zero-shot\nperformance on several tasks. For example, with no additional task-specific\ntraining we achieve BLEU scores of up to 35.8 for document translation. We\nfurther show that fine-tuning gives strong performance on a range of\ndiscriminative and generative tasks in many languages, making MARGE the most\ngenerally applicable pre-training method to date.",
        "date": "2020-06-26"
    },
    "https://arxiv.org/abs/1907.07355": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "bert",
            "bertology"
        ],
        "title": "[1907.07355] Probing Neural Network Comprehension of Natural Language Arguments",
        "summary": "We are surprised to find that BERT's peak performance of 77% on the Argument\nReasoning Comprehension Task reaches just three points below the average\nuntrained human baseline. However, we show that this result is entirely\naccounted for by exploitation of spurious statistical cues in the dataset. We\nanalyze the nature of these cues and demonstrate that a range of models all\nexploit them. This analysis informs the construction of an adversarial dataset\non which all models achieve random accuracy. Our adversarial dataset provides a\nmore robust assessment of argument comprehension and should be adopted as the\nstandard in future work.",
        "date": "2019-07-17"
    },
    "https://arxiv.org/abs/2208.05388": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "catastrophic forgetting"
        ],
        "title": "[2208.05388] ATLAS: Universal Function Approximator for Memory Retention",
        "summary": "Artificial neural networks (ANNs), despite their universal function\napproximation capability and practical success, are subject to catastrophic\nforgetting. Catastrophic forgetting refers to the abrupt unlearning of a\nprevious task when a new task is learned. It is an emergent phenomenon that\nhinders continual learning. Existing universal function approximation theorems\nfor ANNs guarantee function approximation ability, but do not predict\ncatastrophic forgetting. This paper presents a novel universal approximation\ntheorem for multi-variable functions using only single-variable functions and\nexponential functions. Furthermore, we present ATLAS: a novel ANN architecture\nbased on the new theorem. It is shown that ATLAS is a universal function\napproximator capable of some memory retention, and continual learning. The\nmemory of ATLAS is imperfect, with some off-target effects during continual\nlearning, but it is well-behaved and predictable. An efficient implementation\nof ATLAS is provided. Experiments are conducted to evaluate both the function\napproximation and memory retention capabilities of ATLAS.",
        "date": "2022-08-10"
    },
    "https://arxiv.org/abs/2002.06504": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "top k"
        ],
        "title": "[2002.06504] Differentiable Top-k Operator with Optimal Transport",
        "summary": "The top-k operation, i.e., finding the k largest or smallest elements from a\ncollection of scores, is an important model component, which is widely used in\ninformation retrieval, machine learning, and data mining. However, if the top-k\noperation is implemented in an algorithmic way, e.g., using bubble algorithm,\nthe resulting model cannot be trained in an end-to-end way using prevalent\ngradient descent algorithms. This is because these implementations typically\ninvolve swapping indices, whose gradient cannot be computed. Moreover, the\ncorresponding mapping from the input scores to the indicator vector of whether\nthis element belongs to the top-k set is essentially discontinuous. To address\nthe issue, we propose a smoothed approximation, namely the SOFT (Scalable\nOptimal transport-based diFferenTiable) top-k operator. Specifically, our SOFT\ntop-k operator approximates the output of the top-k operation as the solution\nof an Entropic Optimal Transport (EOT) problem. The gradient of the SOFT\noperator can then be efficiently approximated based on the optimality\nconditions of EOT problem. We apply the proposed operator to the k-nearest\nneighbors and beam search algorithms, and demonstrate improved performance.",
        "date": "2020-02-16"
    },
    "https://arxiv.org/abs/1905.07854": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "attention knowledge graphs",
            "recommender systems"
        ],
        "title": "[1905.07854] KGAT: Knowledge Graph Attention Network for Recommendation",
        "summary": "To provide more accurate, diverse, and explainable recommendation, it is\ncompulsory to go beyond modeling user-item interactions and take side\ninformation into account. Traditional methods like factorization machine (FM)\ncast it as a supervised learning problem, which assumes each interaction as an\nindependent instance with side information encoded. Due to the overlook of the\nrelations among instances or items (e.g., the director of a movie is also an\nactor of another movie), these methods are insufficient to distill the\ncollaborative signal from the collective behaviors of users. In this work, we\ninvestigate the utility of knowledge graph (KG), which breaks down the\nindependent interaction assumption by linking items with their attributes. We\nargue that in such a hybrid structure of KG and user-item graph, high-order\nrelations --- which connect two items with one or multiple linked attributes\n--- are an essential factor for successful recommendation. We propose a new\nmethod named Knowledge Graph Attention Network (KGAT) which explicitly models\nthe high-order connectivities in KG in an end-to-end fashion. It recursively\npropagates the embeddings from a node's neighbors (which can be users, items,\nor attributes) to refine the node's embedding, and employs an attention\nmechanism to discriminate the importance of the neighbors. Our KGAT is\nconceptually advantageous to existing KG-based recommendation methods, which\neither exploit high-order relations by extracting paths or implicitly modeling\nthem with regularization. Empirical results on three public benchmarks show\nthat KGAT significantly outperforms state-of-the-art methods like Neural FM and\nRippleNet. Further studies verify the efficacy of embedding propagation for\nhigh-order relation modeling and the interpretability benefits brought by the\nattention mechanism.",
        "date": "2019-05-20"
    },
    "https://arxiv.org/abs/1911.03876": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "zero shot",
            "allennlp",
            "commonsense question answering"
        ],
        "title": "[1911.03876] Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering",
        "summary": "Understanding narratives requires reasoning about implicit world knowledge\nrelated to the causes, effects, and states of situations described in text. At\nthe core of this challenge is how to access contextually relevant knowledge on\ndemand and reason over it.\nIn this paper, we present initial studies toward zero-shot commonsense\nquestion answering by formulating the task as inference over dynamically\ngenerated commonsense knowledge graphs. In contrast to previous studies for\nknowledge integration that rely on retrieval of existing knowledge from static\nknowledge graphs, our study requires commonsense knowledge integration where\ncontextually relevant knowledge is often not present in existing knowledge\nbases. Therefore, we present a novel approach that generates\ncontextually-relevant symbolic knowledge structures on demand using generative\nneural commonsense knowledge models.\nEmpirical results on two datasets demonstrate the efficacy of our\nneuro-symbolic approach for dynamically constructing knowledge graphs for\nreasoning. Our approach achieves significant performance boosts over pretrained\nlanguage models and vanilla knowledge models, all while providing interpretable\nreasoning paths for its predictions.",
        "date": "2019-11-10"
    },
    "https://arxiv.org/abs/2101.00345": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "andrew mccallum",
            "discute avec raphael",
            "entity type representation"
        ],
        "title": "[2101.00345] Modeling Fine-Grained Entity Types with Box Embeddings",
        "summary": "Neural entity typing models typically represent fine-grained entity types as\nvectors in a high-dimensional space, but such spaces are not well-suited to\nmodeling these types' complex interdependencies. We study the ability of box\nembeddings, which embed concepts as d-dimensional hyperrectangles, to capture\nhierarchies of types even when these relationships are not defined explicitly\nin the ontology. Our model represents both types and entity mentions as boxes.\nEach mention and its context are fed into a BERT-based model to embed that\nmention in our box space; essentially, this model leverages typological clues\npresent in the surface text to hypothesize a type representation for the\nmention. Box containment can then be used to derive both the posterior\nprobability of a mention exhibiting a given type and the conditional\nprobability relations between types themselves. We compare our approach with a\nvector-based typing model and observe state-of-the-art performance on several\nentity typing benchmarks. In addition to competitive typing performance, our\nbox-based model shows better performance in prediction consistency (predicting\na supertype and a subtype together) and confidence (i.e., calibration),\ndemonstrating that the box-based model captures the latent type hierarchies\nbetter than the vector-based model does.",
        "date": "2021-01-02"
    },
    "https://arxiv.org/abs/1705.06476": {
        "extra-tags": [],
        "tags": [
            "parlai",
            "antoine bordes"
        ],
        "title": "[1705.06476] ParlAI: A Dialog Research Software Platform",
        "summary": "We introduce ParlAI (pronounced \"par-lay\"), an open-source software platform\nfor dialog research implemented in Python, available at http://parl.ai. Its\ngoal is to provide a unified framework for sharing, training and testing of\ndialog models, integration of Amazon Mechanical Turk for data collection, human\nevaluation, and online/reinforcement learning; and a repository of machine\nlearning models for comparing with others' models, and improving upon existing\narchitectures. Over 20 tasks are supported in the first release, including\npopular datasets such as SQuAD, bAbI tasks, MCTest, WikiQA, QACNN, QADailyMail,\nCBT, bAbI Dialog, Ubuntu, OpenSubtitles and VQA. Several models are integrated,\nincluding neural models such as memory networks, seq2seq and attentive LSTMs.",
        "date": "2017-05-18"
    },
    "https://arxiv.org/abs/1905.07129": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "ernie",
            "acl 2019",
            "discute avec raphael",
            "bert",
            "attention is all you need",
            "text kg and embeddings",
            "nlp using knowledge graphs"
        ],
        "title": "[1905.07129] ERNIE: Enhanced Language Representation with Informative Entities",
        "summary": "Neural language representation models such as BERT pre-trained on large-scale\ncorpora can well capture rich semantic patterns from plain text, and be\nfine-tuned to consistently improve the performance of various NLP tasks.\nHowever, the existing pre-trained language models rarely consider incorporating\nknowledge graphs (KGs), which can provide rich structured knowledge facts for\nbetter language understanding. We argue that informative entities in KGs can\nenhance language representation with external knowledge. In this paper, we\nutilize both large-scale textual corpora and KGs to train an enhanced language\nrepresentation model (ERNIE), which can take full advantage of lexical,\nsyntactic, and knowledge information simultaneously. The experimental results\nhave demonstrated that ERNIE achieves significant improvements on various\nknowledge-driven tasks, and meanwhile is comparable with the state-of-the-art\nmodel BERT on other common NLP tasks. The source code of this paper can be\nobtained from https://github.com/thunlp/ERNIE.",
        "date": "2019-05-17"
    },
    "https://arxiv.org/abs/2209.11055": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nils reimers",
            "moshe wasserblat",
            "few shot learning",
            "setfit sbert fine tuning"
        ],
        "title": "[2209.11055] Efficient Few-Shot Learning Without Prompts",
        "summary": "Recent few-shot methods, such as parameter-efficient fine-tuning (PEFT) and\npattern exploiting training (PET), have achieved impressive results in\nlabel-scarce settings. However, they are difficult to employ since they are\nsubject to high variability from manually crafted prompts, and typically\nrequire billion-parameter language models to achieve high accuracy. To address\nthese shortcomings, we propose SetFit (Sentence Transformer Fine-tuning), an\nefficient and prompt-free framework for few-shot fine-tuning of Sentence\nTransformers (ST). SetFit works by first fine-tuning a pretrained ST on a small\nnumber of text pairs, in a contrastive Siamese manner. The resulting model is\nthen used to generate rich text embeddings, which are used to train a\nclassification head. This simple framework requires no prompts or verbalizers,\nand achieves high accuracy with orders of magnitude less parameters than\nexisting techniques. Our experiments show that SetFit obtains comparable\nresults with PEFT and PET techniques, while being an order of magnitude faster\nto train. We also show that SetFit can be applied in multilingual settings by\nsimply switching the ST body. Our code is available at\nhttps://github.com/huggingface/setfit and our datasets at\nhttps://huggingface.co/setfit .",
        "date": "2022-09-22"
    },
    "https://arxiv.org/abs/2104.08821": {
        "extra-tags": [],
        "tags": [
            "nlp princeton",
            "arxiv doc",
            "sentence embeddings",
            "contrastive learning",
            "simcse"
        ],
        "title": "[2104.08821] SimCSE: Simple Contrastive Learning of Sentence Embeddings",
        "summary": "This paper presents SimCSE, a simple contrastive learning framework that\ngreatly advances state-of-the-art sentence embeddings. We first describe an\nunsupervised approach, which takes an input sentence and predicts itself in a\ncontrastive objective, with only standard dropout used as noise. This simple\nmethod works surprisingly well, performing on par with previous supervised\ncounterparts. We find that dropout acts as minimal data augmentation, and\nremoving it leads to a representation collapse. Then, we propose a supervised\napproach, which incorporates annotated pairs from natural language inference\ndatasets into our contrastive learning framework by using \"entailment\" pairs as\npositives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on\nstandard semantic textual similarity (STS) tasks, and our unsupervised and\nsupervised models using BERT base achieve an average of 76.3% and 81.6%\nSpearman's correlation respectively, a 4.2% and 2.2% improvement compared to\nthe previous best results. We also show -- both theoretically and empirically\n-- that the contrastive learning objective regularizes pre-trained embeddings'\nanisotropic space to be more uniform, and it better aligns positive pairs when\nsupervised signals are available.",
        "date": "2021-04-18"
    },
    "https://arxiv.org/abs/2010.02194": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp facebook",
            "knowledge distillation",
            "self training",
            "nlp pretraining"
        ],
        "title": "[2010.02194] Self-training Improves Pre-training for Natural Language Understanding",
        "summary": "Unsupervised pre-training has led to much recent progress in natural language\nunderstanding. In this paper, we study self-training as another way to leverage\nunlabeled data through semi-supervised learning. To obtain additional data for\na specific task, we introduce SentAugment, a data augmentation method which\ncomputes task-specific query embeddings from labeled data to retrieve sentences\nfrom a bank of billions of unlabeled sentences crawled from the web. Unlike\nprevious semi-supervised methods, our approach does not require in-domain\nunlabeled data and is therefore more generally applicable. Experiments show\nthat self-training is complementary to strong RoBERTa baselines on a variety of\ntasks. Our augmentation approach leads to scalable and effective self-training\nwith improvements of up to 2.6% on standard text classification benchmarks.\nFinally, we also show strong gains on knowledge-distillation and few-shot\nlearning.",
        "date": "2020-10-05"
    },
    "https://arxiv.org/abs/2208.00635": {
        "extra-tags": [],
        "tags": [
            "arxiv doc",
            "nlp alibaba",
            "knowledge augmented language models"
        ],
        "title": "[2208.00635] DictBERT: Dictionary Description Knowledge Enhanced Language Model Pre-training via Contrastive Learning",
        "summary": "Although pre-trained language models (PLMs) have achieved state-of-the-art\nperformance on various natural language processing (NLP) tasks, they are shown\nto be lacking in knowledge when dealing with knowledge driven tasks. Despite\nthe many efforts made for injecting knowledge into PLMs, this problem remains\nopen. To address the challenge, we propose \\textbf{DictBERT}, a novel approach\nthat enhances PLMs with dictionary knowledge which is easier to acquire than\nknowledge graph (KG). During pre-training, we present two novel pre-training\ntasks to inject dictionary knowledge into PLMs via contrastive learning:\n\\textit{dictionary entry prediction} and \\textit{entry description\ndiscrimination}. In fine-tuning, we use the pre-trained DictBERT as a plugin\nknowledge base (KB) to retrieve implicit knowledge for identified entries in an\ninput sequence, and infuse the retrieved knowledge into the input to enhance\nits representation via a novel extra-hop attention mechanism. We evaluate our\napproach on a variety of knowledge driven and language understanding tasks,\nincluding NER, relation extraction, CommonsenseQA, OpenBookQA and GLUE.\nExperimental results demonstrate that our model can significantly improve\ntypical PLMs: it gains a substantial improvement of 0.5\\%, 2.9\\%, 9.0\\%, 7.1\\%\nand 3.3\\% on BERT-large respectively, and is also effective on RoBERTa-large.",
        "date": "2022-08-01"
    },
    "https://github.com/JCSDA-internal/FSOI": {
        "extra-tags": [
            "forecast"
        ],
        "date": "2017-07-13",
        "title": "FSOI",
        "summary": "Forecast Sensitivity to Observations and Observation Impact",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/jdwittenauer/ipython-notebooks": {
        "extra-tags": [],
        "date": "2014-06-01",
        "title": "ipython-notebooks",
        "summary": "A collection of IPython notebooks covering various topics.",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://arxiv.org/abs/1909.04164": {
        "extra-tags": [],
        "tags": [
            "entities and lm",
            "knowledge augmented language models",
            "multiple knowledge bases",
            "good",
            "grounded language learning",
            "kd mkb biblio",
            "arxiv doc",
            "knowbert",
            "allen institute for ai a2i",
            "contextualised word representations",
            "nlp using knowledge graphs",
            "emnlp 2019",
            "knowledge driven embeddings",
            "knowledge graph augmented language models"
        ],
        "title": "[1909.04164] Knowledge Enhanced Contextual Word Representations",
        "summary": "Contextual word representations, typically trained on unstructured, unlabeled\ntext, do not contain any explicit grounding to real world entities and are\noften unable to remember facts about those entities. We propose a general\nmethod to embed multiple knowledge bases (KBs) into large scale models, and\nthereby enhance their representations with structured, human-curated knowledge.\nFor each KB, we first use an integrated entity linker to retrieve relevant\nentity embeddings, then update contextual word representations via a form of\nword-to-entity attention. In contrast to previous approaches, the entity\nlinkers and self-supervised language modeling objective are jointly trained\nend-to-end in a multitask setting that combines a small amount of entity\nlinking supervision with a large amount of raw text. After integrating WordNet\nand a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert)\ndemonstrates improved perplexity, ability to recall facts as measured in a\nprobing task and downstream performance on relationship extraction, entity\ntyping, and word sense disambiguation. KnowBert's runtime is comparable to\nBERT's and it scales to large KBs.",
        "date": "2019-09-09"
    },
    "https://arxiv.org/abs/1910.00163": {
        "extra-tags": [],
        "tags": [
            "word embedding",
            "emnlp 2019",
            "information bottleneck method",
            "arxiv doc"
        ],
        "title": "[1910.00163] Specializing Word Embeddings (for Parsing) by Information Bottleneck",
        "summary": "Pre-trained word embeddings like ELMo and BERT contain rich syntactic and\nsemantic information, resulting in state-of-the-art performance on various\ntasks. We propose a very fast variational information bottleneck (VIB) method\nto nonlinearly compress these embeddings, keeping only the information that\nhelps a discriminative parser. We compress each word embedding to either a\ndiscrete tag or a continuous vector. In the discrete version, our automatically\ncompressed tags form an alternative tag set: we show experimentally that our\ntags capture most of the information in traditional POS tag annotations, but\nour tag sequences can be parsed more accurately at the same level of tag\ngranularity. In the continuous version, we show experimentally that moderately\ncompressing the word embeddings by our method yields a more accurate parser in\n8 of 9 languages, unlike simple dimensionality reduction.",
        "date": "2019-10-01"
    },
    "http://openaccess.thecvf.com/content_ICCV_2019/html/Cho_On_the_Efficacy_of_Knowledge_Distillation_ICCV_2019_paper.html": {
        "extra-tags": [],
        "tags": [
            "critical evaluation",
            "knowledge distillation",
            "arxiv doc"
        ],
        "title": "[1910.01348] On the Efficacy of Knowledge Distillation",
        "summary": "In this paper, we present a thorough evaluation of the efficacy of knowledge\ndistillation and its dependence on student and teacher architectures. Starting\nwith the observation that more accurate teachers often don't make good\nteachers, we attempt to tease apart the factors that affect knowledge\ndistillation performance. We find crucially that larger models do not often\nmake better teachers. We show that this is a consequence of mismatched\ncapacity, and that small students are unable to mimic large teachers. We find\ntypical ways of circumventing this (such as performing a sequence of knowledge\ndistillation steps) to be ineffective. Finally, we show that this effect can be\nmitigated by stopping the teacher's training early. Our results generalize\nacross datasets and models.",
        "date": "2019-10-03"
    },
    "https://twitter.com/abacaj/status/1635355642289618944": {
        "extra-tags": [],
        "date": "2023-03-13",
        "title": "Twitter @abacaj",
        "summary": "LLaMA has been fine-tuned by stanford, \n\n\"We performed a blind pairwise comparison between text-davinci-003 and Alpaca 7B, and we found that these two models have very similar performance: Alpaca wins 90 versus 89 comparisons against text-davinci-003.\" https://t.co/Ut3RPXaoLL",
        "tags": [
            "alpaca",
            "davinci",
            "twitter",
            "alpaca 7b",
            "stanford",
            "llama"
        ]
    },
    "https://twitter.com/ptrblck_de/status/1634991080101007360": {
        "extra-tags": [],
        "date": "2023-03-12",
        "title": "Twitter @ptrblck_de",
        "summary": "@StasBekman You are most likely using CUDA 11.7+, which ships with lazy kernel/module loading and is enabled by default in PyTorch 1.13.1+. (Run `export CUDA_MODULE_LOADING=EAGER` to disable it as a test)",
        "tags": [
            "cuda 11.",
            "twitter",
            "pytorch",
            "cuda"
        ]
    },
    "https://twitter.com/Exponenciel13/status/1634575782935666688": {
        "extra-tags": [],
        "date": "2023-03-11",
        "title": "Twitter @Exponenciel13",
        "summary": "Pour celles et ceux qui ne le savent pas, vous pouvez faire des figures sur G\u00e9og\u00e9bra et importer le code sur Overleaf pour vous \u00e9viter de devoir coder en LaTeX. Il y'a quelques modifications \u00e0 faire mais vous gagnez un temps fous. https://t.co/hRIu4FlQcR",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/josh_wills/status/1633989488077860865": {
        "extra-tags": [],
        "date": "2023-03-10",
        "title": "Twitter @josh_wills",
        "summary": "@vboykis Max is my hero",
        "tags": [
            "max",
            "twitter"
        ]
    },
    "https://twitter.com/josh_wills/status/1634033187189055488": {
        "extra-tags": [
            "time"
        ],
        "date": "2023-03-10",
        "title": "Twitter @josh_wills",
        "summary": "@DSMoxon @vboykis Naw I\u2019ve been a fan of https://t.co/e8nwk6GBkZ for a long time",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/vboykis/status/1633985937758732288": {
        "extra-tags": [],
        "date": "2023-03-10",
        "title": "Twitter @vboykis",
        "summary": "This man is dangerous https://t.co/NdLW2x45LS",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1633789312457211904": {
        "extra-tags": [],
        "date": "2023-03-09",
        "title": "Twitter @fishnets88",
        "summary": "With the baby doing some actual sleep, it seems that I can do some recreational programming again. \n\nGotta say. @willmcgugan textual is recreational indeed. https://t.co/ntAbq2DCFG",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/mervenoyann/status/1633243957973995520": {
        "extra-tags": [
            "emoji"
        ],
        "date": "2023-03-07",
        "title": "Twitter @mervenoyann",
        "summary": "who is going to tell him that this company already exists and has an emoji in it https://t.co/TjKF2TTxZl",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/JonSaadFalcon/status/1632815738766385153": {
        "extra-tags": [],
        "date": "2023-03-06",
        "title": "Twitter @JonSaadFalcon",
        "summary": "Do you need zero-shot document retrieval in new domains? You're in luck! We propose UDAPDR (\u201cYou-dapter\u201d) that efficiently leverages GPT-3.5 and Flan-T5 to address domain adaptation under annotation constraints and domain shifts, improving the SoTA ColBERTv2 by up to 13%. https://t.co/AgtFwImF5u",
        "tags": [
            "twitter",
            "sota colbertv2",
            "flan-t5",
            "you-dapter",
            "gpt",
            "udapdr"
        ]
    },
    "https://twitter.com/deepset_ai/status/1631637923463323650": {
        "extra-tags": [],
        "date": "2023-03-03",
        "title": "Twitter @deepset_ai",
        "summary": "A Haystack Blog for the growing Haystack community \ud83c\udf89\nWe're thrilled to announce this new space where we can learn from each other and share NLP, Haystack, open source and development content: https://t.co/hfVW7Wy7Ay\n\n#NLP #opensource #machinelearning https://t.co/KiUIZZxnON",
        "tags": [
            "nlp",
            "twitter",
            "haystack"
        ]
    },
    "https://twitter.com/EugeneVinitsky/status/1630270218319855616": {
        "extra-tags": [],
        "date": "2023-02-27",
        "title": "Twitter @EugeneVinitsky",
        "summary": "The real difference between PyTorch, TensorFlow, and Jax is one has ptrblck and the others don't",
        "tags": [
            "pytorch",
            "twitter",
            "jax",
            "tensorflow"
        ]
    },
    "https://twitter.com/halford_max/status/1630506439352483843": {
        "extra-tags": [],
        "date": "2023-02-28",
        "title": "Twitter @halford_max",
        "summary": "Prince has been downloaded more than a million times! \ud83d\udc51\n\nTo celebrate, I deeply refactored the codebase, which fixed longstanding issues. I also made a documentation website: https://t.co/Tk0gMJkjSW",
        "tags": [
            "twitter",
            "prince"
        ]
    },
    "https://twitter.com/dannypostmaa/status/1629842362775076865": {
        "extra-tags": [],
        "date": "2023-02-26",
        "title": "Twitter @dannypostmaa",
        "summary": "\u26a1 NEW: Personalized Meme Generator\n\nYour favorite meme, now with your face! \ud83e\udd29\n\n1\ufe0f\u20e3 Snap a few selfies\n2\ufe0f\u20e3 Let the AI magic happen\n\u2705 Get 100+ customized memes featuring YOU!\n\nTry it out: Link in next tweet \ud83d\udc47 https://t.co/R3aCerX3os",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/johnjnay/status/1629499355219599360": {
        "extra-tags": [],
        "date": "2023-02-25",
        "title": "Twitter @johnjnay",
        "summary": "Measuring Uncertainty in LLMs\n\n-Hard to know when to trust LLMs\n-Hard to measure uncertainty in natural lang b/c diff words can mean same thing\n\n-By considering shared meanings \"Semantic Entropy\" more predictive of LLM accuracy\n-Works out-of-the-box\n\nPaper https://t.co/f3w09yidH3 https://t.co/fuOvfaxRvR",
        "tags": [
            "semantic entropy",
            "llms",
            "twitter",
            "llm"
        ]
    },
    "https://twitter.com/mervenoyann/status/1629496200759062528": {
        "extra-tags": [],
        "date": "2023-02-25",
        "title": "Twitter @mervenoyann",
        "summary": "spent my saturday training the first NER model (as a baseline) with recently annotated data for https://t.co/EmpouDzQOY\nI realized I get in flow when I do something I believe in https://t.co/A5U7YX6YZg",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/Thom_Wolf/status/1629258494481137664": {
        "extra-tags": [],
        "date": "2023-02-24",
        "title": "Twitter @Thom_Wolf",
        "summary": "It's increasingly clear we've severely underestimated how much you can pack in a 10B parameters model https://t.co/WLTDrmZ121",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/ylecun/status/1629243179068268548": {
        "extra-tags": [],
        "date": "2023-02-24",
        "title": "Twitter @ylecun",
        "summary": "Generated by LLaMA.\n(prompt in bold).\n\nSee appendix of the LLaMA paper: https://t.co/0y2o2zTcqv https://t.co/wnYx7KobdV",
        "tags": [
            "twitter",
            "llama"
        ]
    },
    "https://twitter.com/vboykis/status/1629324357066448898": {
        "extra-tags": [],
        "date": "2023-02-25",
        "title": "Twitter @vboykis",
        "summary": "Women:  I can change him\nHim: https://t.co/hqnNR4BHeO",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/buitengebieden/status/1629043640491286528": {
        "extra-tags": [],
        "date": "2023-02-24",
        "title": "Twitter @buitengebieden",
        "summary": "This took me a second.. \ud83d\ude02 https://t.co/JgqVSUcYIb",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/rasbt/status/1628799272287182850": {
        "extra-tags": [],
        "date": "2023-02-23",
        "title": "Twitter @rasbt",
        "summary": "Wrote a new blog post outlining techniques for improving the training performance of your PyTorch model without compromising its accuracy.\nBy changing only a few lines of code, we are going from 22.63 minutes to 3.15 minutes when finetuning BERT:\nhttps://t.co/36ptnaFOzz",
        "tags": [
            "pytorch",
            "twitter"
        ]
    },
    "https://twitter.com/jobergum/status/1628519945544822786": {
        "extra-tags": [],
        "date": "2023-02-22",
        "title": "Twitter @jobergum",
        "summary": "This will be interesting https://t.co/AQw0aNIJej",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/pommedeterre33/status/1627938521062178817": {
        "extra-tags": [],
        "date": "2023-02-21",
        "title": "Twitter @pommedeterre33",
        "summary": "FlexGen's paper claims 100x speed-up for GPT-3 like models... but only for large batches. Single batch inference (e.g. chat task) still takes 50 secs/token (down from 3 mins). Key takeaway: the excitement around democratizing big language models, not just the tech itself. https://t.co/8NvHImA1j4",
        "tags": [
            "twitter",
            "gpt-",
            "flexgen"
        ]
    },
    "https://twitter.com/mr_cheu/status/1626261050566778880": {
        "extra-tags": [],
        "date": "2023-02-16",
        "title": "Twitter @mr_cheu",
        "summary": "Some \"in the trenches\" learnings from integrating vector search into an enterprise search system:\n\n1) As of Feb 2023, open source text embedding models on @huggingface are still cheaper and offer higher performance compared to other commercial providers\n\nWe compared the top\u2026 https://t.co/fdDE2eKzFh https://t.co/70R5Ifswr3",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/_akhaliq/status/1626405827870441472": {
        "extra-tags": [],
        "date": "2023-02-17",
        "title": "Twitter @_akhaliq",
        "summary": "Do We Still Need Clinical Language Models?\n\nabs: https://t.co/67mhZzLEt0 https://t.co/Z5ri49YcQK",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/hyperfp/status/1626163877896499201": {
        "extra-tags": [],
        "date": "2023-02-16",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty this could be used by https://t.co/Nq0WJlNpWx, couldn't it? https://t.co/ZWFr1l037e",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/pommedeterre33/status/1625784898182168578": {
        "extra-tags": [],
        "date": "2023-02-15",
        "title": "Twitter @pommedeterre33",
        "summary": "A simple optimizer faster than AdamW and with a lower memory footprint.\nIt seems too good to be true, and still...\nOfc  lucidrains has *already* pushed a PyTorch implementation (&lt; 100 LoC) \ud83e\udd73\nhttps://t.co/jJu7fomdjK https://t.co/mtaGp6DqeW https://t.co/8MJDNAOotw",
        "tags": [
            "pytorch",
            "twitter",
            "adamw"
        ]
    },
    "https://twitter.com/sourab_m/status/1624090029198020612": {
        "extra-tags": [],
        "date": "2023-02-10",
        "title": "Twitter @sourab_m",
        "summary": "Large models are expensive to fine-tune on downstream tasks. What if we could achieve the same performance with a small fraction of the trainable parameters \ud83e\udd11?\n\nIntroducing \ud83e\udd17 PEFT: library for \"parameter-efficient fine-tuning\".\nhttps://t.co/Rj8eM9tltR\nhttps://t.co/Mxs2BqSu5G\n\ud83e\uddf5 https://t.co/a4HAnf9aHV",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/pommedeterre33/status/1623593858679570433": {
        "extra-tags": [],
        "date": "2023-02-09",
        "title": "Twitter @pommedeterre33",
        "summary": "Unlock blazing fast GPU inference for your generative models! Our team achieved 2.3x improvement on @OpenAI Whisper (large) vs HuggingFace FP16 + PyTorch 2.0. CUDA Graphs &amp; a simple trick for avoiding OOM, follow this thread to find out more (links at the end) \ud83d\udca5",
        "tags": [
            "twitter",
            "cuda"
        ]
    },
    "https://twitter.com/MathisHammel/status/1623357107792670721": {
        "extra-tags": [
            "ai"
        ],
        "date": "2023-02-08",
        "title": "Twitter @MathisHammel",
        "summary": "Je suis \u00e0 Stockholm cette semaine pour la conf\u00e9rence @JFokus, et j'ai d\u00e9cid\u00e9 de participer au quiz de l'un des sponsors. Je crois que j'ai gagn\u00e9 \ud83d\ude07\n\nPetit thread de cybers\u00e9curit\u00e9 appliqu\u00e9e dans lequel je vous explique comment j'ai fait \u2935\ufe0f https://t.co/jEcAjTgtGq",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/TournesolBotFR/status/1621862941434716160": {
        "extra-tags": [],
        "date": "2023-02-04",
        "title": "Twitter @TournesolBotFR",
        "summary": "Merci \u00e0 tout\u00b7es nos contributeur\u00b7rice\u00b7s. Voici le top 10 du mois dernier. Et vous, combien de comparaisons avez-vous faites le mois dernier? \ud83c\udf3b https://t.co/ggkdDQzYXX",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/vboykis/status/1621542836482068481": {
        "extra-tags": [],
        "date": "2023-02-03",
        "title": "Twitter @vboykis",
        "summary": "anything can be formulated as a graph neural net problem, if it can't you're just not trying hard enough",
        "tags": [
            "twitter",
            "graph neural"
        ]
    },
    "https://twitter.com/ljvmiranda921/status/1621813818904154112": {
        "extra-tags": [],
        "date": "2023-02-04",
        "title": "Twitter @ljvmiranda921",
        "summary": "\ud83d\udce3 New blog post: I've been working on an NER pipeline for my native language, Tagalog, using @spacy_io!\n\nStill a work-in-progress, but happy to share my updates on getting structured evaluations on a low-resource language :)\n\nhttps://t.co/xKXOAOE0C4",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1621817491700719616": {
        "extra-tags": [],
        "date": "2023-02-04",
        "title": "Twitter @fishnets88",
        "summary": "This is easily be one of the best blog posts I'll read this year. \n\nPartially because training a NLP pipeline for a new language is no small feat, but also because there are some solid and well written tips/tricks on display in this one. https://t.co/N0uyixr58C",
        "tags": [
            "nlp",
            "twitter"
        ]
    },
    "https://twitter.com/_Guz_/status/1621131374328692737": {
        "extra-tags": [],
        "date": "2023-02-02",
        "title": "Twitter @_Guz_",
        "summary": "Really happy that my internship project from last year @SpotifyResearch got accepted to @TheWebConf #TheWebConf Preprint coming soon :)) https://t.co/fQLXb1eqDB",
        "tags": [
            "thewebconf",
            "twitter"
        ]
    },
    "https://twitter.com/ylecun/status/1621206612772851713": {
        "extra-tags": [],
        "date": "2023-02-02",
        "title": "Twitter @ylecun",
        "summary": "Data on the intellectual contribution to AI from various research organizations.\nSome of organizations publish knowledge and open-source code for the entire world to use.\nOthers just consume it. https://t.co/BGxTP1lkXB",
        "tags": [
            "twitter",
            "ai"
        ]
    },
    "https://twitter.com/ylecun/status/1621216089265901574": {
        "extra-tags": [],
        "date": "2023-02-02",
        "title": "Twitter @ylecun",
        "summary": "@JrKibs You don't seem to know very far.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/deepset_ai/status/1621161534243368961": {
        "extra-tags": [],
        "date": "2023-02-02",
        "title": "Twitter @deepset_ai",
        "summary": "\ud83e\uddf5 Generative models have taken the world of NLP by storm. But LLMs do not know about your personal data. This makes personal assistants, enterprise knowledge management and many other applications challenging. Retrieval augmented pipelines are the answer \ud83d\udc47\n#nlp #llm",
        "tags": [
            "nlp",
            "llms",
            "twitter"
        ]
    },
    "https://twitter.com/yoavgo/status/1620866335399084033": {
        "extra-tags": [],
        "date": "2023-02-01",
        "title": "Twitter @yoavgo",
        "summary": "a surprisingly large number of people seem to simultaneously believe that:\n\na) text2image models can not be able to memorize but a tiny fraction of their training data.\n\nb) Large language models can be trained on all the internet and only produce factual assertions.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/AkariAsai/status/1620115280268783616": {
        "extra-tags": [],
        "date": "2023-01-30",
        "title": "Twitter @AkariAsai",
        "summary": "@_lewtun @AiEleuther We have instruction-tuned LMs that learn to retrieve *documents* following users' instructions (e.g., retrieve corresponding code/an answer to a technical question) \n\nhttps://t.co/uQmIooUdIg\nhttps://t.co/kOukxdyqSW\n\npaper: https://t.co/uxACzsl11j \nGitHub: https://t.co/lYYNAcYeJY",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1620003872248053760": {
        "extra-tags": [],
        "date": "2023-01-30",
        "title": "Twitter @fishnets88",
        "summary": "Bulk version 0.2.0 is out!\n\nIf you're eager to get started with bulk labeling you can now also run the service and allow people to download by setting a `--download` flag! https://t.co/R3rw1WxCC1",
        "tags": [
            "bulk",
            "twitter",
            "2.0"
        ]
    },
    "https://twitter.com/alex_buraks/status/1618988134850785280": {
        "extra-tags": [],
        "date": "2023-01-27",
        "title": "Twitter @alex_buraks",
        "summary": "You probably heard about Yandex, it\u2019s the 4th biggest search engine by market share worldwide. Yesterday proprietary source code of Yandex was leaked. \n\nThe most interesting part for SEO community is: the list of all 1922 ranking factors used in the search algorithm \n\n[\ud83e\uddf5THREAD] https://t.co/6x82AAmbON",
        "tags": [
            "yandex",
            "twitter",
            "seo"
        ]
    },
    "https://twitter.com/rodrigfnogueira/status/1618671367142141952": {
        "extra-tags": [],
        "date": "2023-01-26",
        "title": "Twitter @rodrigfnogueira",
        "summary": "\ud83d\udea8 We got a new paper!\nExaRanker: Explanation-Augmented Neural Ranker\nArxiv: https://t.co/xBHFJ6uh66\nCode: https://t.co/XeU0mFBfmn https://t.co/mvVG4CQyY4",
        "tags": [
            "twitter",
            "exaranker",
            "ranker\narxiv"
        ]
    },
    "https://twitter.com/ClementDelangue/status/1618826521787142144": {
        "extra-tags": [],
        "date": "2023-01-27",
        "title": "Twitter @ClementDelangue",
        "summary": "If you're a software engineer or new to AI, these guides done by @mervenoyann and the community are AMAZING! https://t.co/OzpqK6R4Uy https://t.co/k4JTHdby7T",
        "tags": [
            "twitter",
            "ai"
        ]
    },
    "https://twitter.com/Nils_Reimers/status/1618708532102328320": {
        "extra-tags": [],
        "date": "2023-01-26",
        "title": "Twitter @Nils_Reimers",
        "summary": "@jsmarr @simonw @cromwellian @CohereAI Encoding long docs to a single dense vector doesn't work well, even if supported by the model. Embeddings are only great if you encode a single topic with them. Hence, encoding to paragraphs or chunks of eg 100 words makes the most sense. We have research upcoming to dive deeper",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/hyperfp/status/1618201220111544322": {
        "extra-tags": [],
        "date": "2023-01-25",
        "title": "Twitter @hyperfp",
        "summary": "My Personal #KnowledgeGraph: https://t.co/XthJ6aRqfe\nMy goal: improve it using AI\n\"Personal Knowledge With AI In The Loop\"",
        "tags": [
            "twitter",
            "ai"
        ]
    },
    "https://twitter.com/ylecun/status/1618011105707819009": {
        "extra-tags": [],
        "date": "2023-01-24",
        "title": "Twitter @ylecun",
        "summary": "@DigitalSecArch Well, OpenAI is not particularly open: they didn't publish nor open-sourced anything about chatGPT, nor about much of their recent projects.\nIn contrast, Meta-FAIR is famously open: everything is published and open sourced.",
        "tags": [
            "twitter",
            "openai",
            "chatgpt",
            "meta-fair"
        ]
    },
    "https://twitter.com/ylecun/status/1617254676428111872": {
        "extra-tags": [],
        "date": "2023-01-22",
        "title": "Twitter @ylecun",
        "summary": "LLMs are still making sh*t up.\nThat's fine if you use them as writing assistants.\nNot good as question answerers, search engines, etc.\nRLHF merely mitigates the most frequent mistakes without actually fixing the problem. https://t.co/XnDxF8Q9Zr",
        "tags": [
            "llms",
            "twitter",
            "rlhf"
        ]
    },
    "https://twitter.com/halford_max/status/1617086607818997761": {
        "extra-tags": [],
        "date": "2023-01-22",
        "title": "Twitter @halford_max",
        "summary": "8 haf tools with free tiers I used for data science projects (I usually dislike these kind of tweets, but I felt like listing these somewhere)",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/perplexity_ai/status/1616120452338036736": {
        "extra-tags": [],
        "date": "2023-01-19",
        "title": "Twitter @perplexity_ai",
        "summary": "Announcing a major update to Perplexity Ask: the world\u2019s first conversational search engine! Now, you can read answers with up-to-date sources and ask follow-up questions to dig deeper. In other words, you can chat with your search engine!\nTry it out at https://t.co/ut3wdOxstL https://t.co/rQCyeJhzto",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/DSP_fact/status/1614617643314552832": {
        "extra-tags": [],
        "date": "2023-01-15",
        "title": "Twitter @DSP_fact",
        "summary": "The Fourier Transform, explained in one sentence \n\nhttps://t.co/QDxpJElJkR https://t.co/hsgDQcGLGK",
        "tags": [
            "twitter",
            "the fourier transform"
        ]
    },
    "https://twitter.com/mervenoyann/status/1613833495175995393": {
        "extra-tags": [],
        "date": "2023-01-13",
        "title": "Twitter @mervenoyann",
        "summary": "spicy take of mine: most of the advices are wrong. I was a chatbot developer for companies and ChatGPT is no use for industry bots. I'll explain why, a thread \ud83e\uddf5 https://t.co/55V78bUYOf",
        "tags": [
            "twitter",
            "chatgpt"
        ]
    },
    "https://twitter.com/hyperfp/status/1602074521279791104": {
        "extra-tags": [],
        "date": "2022-12-11",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty a method that computes embeddings from entity descriptions and mention contexts (Yamada among the authors) https://t.co/ruzkQY5thQ",
        "tags": [
            "yamada",
            "twitter"
        ]
    },
    "https://twitter.com/ylecun/status/1591463668612730880": {
        "extra-tags": [],
        "date": "2022-11-12",
        "title": "Twitter @ylecun",
        "summary": "OK, debates about the necessity or \"priors\" (or lack thereof) in learning systems are pointless.\nHere are some basic facts that all ML theorists and most ML practitioners understand, but a number of folks-with-an-agenda don't seem to grasp.\nThread.\n1/ https://t.co/T6De5EezR5",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1586243955200380929": {
        "extra-tags": [],
        "date": "2022-10-29",
        "title": "Twitter @fishnets88",
        "summary": "When Textual (the TUI framework) v0.2.0 came out earlier this week I noticed that it's been a while since I've done CSS. Mainly because I've gotten so used to tailwindcss.\n\nI couldn't help myself. World ... meet tuilwind.css\n\nhttps://t.co/Abwrx2VnhD https://t.co/O8RIet51o9",
        "tags": [
            "textual",
            "twitter",
            "tui",
            "css",
            "tuilwind"
        ]
    },
    "https://twitter.com/halford_max/status/1586016690642583559": {
        "extra-tags": [],
        "date": "2022-10-28",
        "title": "Twitter @halford_max",
        "summary": "Researchers at Bytedance, the company behind TikTok, have released a paper giving some details of how the sausage is made https://t.co/MGMCtNCWte \ud83d\udc40\ud83c\udf65",
        "tags": [
            "tiktok",
            "twitter",
            "bytedance"
        ]
    },
    "https://twitter.com/hyperfp/status/1582252612183019520": {
        "extra-tags": [],
        "date": "2022-10-18",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty https://t.co/AN5L3oaqRo",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/halford_max/status/1579909993016758273": {
        "extra-tags": [],
        "date": "2022-10-11",
        "title": "Twitter @halford_max",
        "summary": "Short and sweet presentation from @MathesonZander using River + @bytewax for (forest) fire detection from real-time air quality data \u27a1\ufe0f https://t.co/fRnA3k85rA",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/jgmorenof/status/1579758829826629633": {
        "extra-tags": [],
        "date": "2022-10-11",
        "title": "Twitter @jgmorenof",
        "summary": "merci pour la surprise #professorlife #ut3 #ditespas\u00e0mafemme #bd https://t.co/6R53goCGHC",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/MathesonZander/status/1577021675555651591": {
        "extra-tags": [],
        "date": "2022-10-03",
        "title": "Twitter @MathesonZander",
        "summary": "Currently in Austin and I am excited to meet you all tomorrow at the #Current22 conference. Join me as I present an interesting application for Online #MachineLearning using @halford_max\u2019s River library &amp; @bytewax. See you all soon. #apachekafka #streamingdata https://t.co/nfXHUxXZB8",
        "tags": [
            "austin",
            "twitter",
            "apachekafka",
            "river"
        ]
    },
    "https://twitter.com/AnecdotesMaths/status/1576603265940299778": {
        "extra-tags": [
            "r"
        ],
        "date": "2022-10-02",
        "title": "Twitter @AnecdotesMaths",
        "summary": "Le probl\u00e8me du sofa consiste \u00e0 trouver le sofa d'aire maximale que l'on peut d\u00e9placer horizontalement dans un couloir d'un m\u00e8tre de large avec un angle droit. Ce probl\u00e8me n'a toujours pas \u00e9t\u00e9 r\u00e9solu. https://t.co/84aMQNe6Pn",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/mervenoyann/status/1572171384553459713": {
        "extra-tags": [
            "models"
        ],
        "date": "2022-09-20",
        "title": "Twitter @mervenoyann",
        "summary": "Check out the models \ud83d\udc47 \n\ud83c\udf69 https://t.co/UrkP4gAJCe\n\ud83d\udcc4 https://t.co/heBpOwUWuf\n\ud83e\udd89  You can try various models in this space made by @ImpiraHQ https://t.co/C4UBmw1cAt",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/amasad/status/1570598156160897024": {
        "extra-tags": [],
        "date": "2022-09-16",
        "title": "Twitter @amasad",
        "summary": "I grew up in the golden age of SQL injections but GPT3 injections just hit different  \ud83e\udd23 https://t.co/WpOqPWgFBs",
        "tags": [
            "gpt3",
            "twitter",
            "sql"
        ]
    },
    "https://twitter.com/fishnets88/status/1570399699043061760": {
        "extra-tags": [],
        "date": "2022-09-15",
        "title": "Twitter @fishnets88",
        "summary": "I'm working on new features for that tool by the way! https://t.co/uWpFv1dJnW",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/halford_max/status/1561734107746521088": {
        "extra-tags": [],
        "date": "2022-08-22",
        "title": "Twitter @halford_max",
        "summary": "I'm making of list of teams doing online machine learning in production. Please reach out if you are concerned! Even more so if you're using River :)",
        "tags": [
            "river",
            "twitter"
        ]
    },
    "https://twitter.com/karlhigley/status/1561004476068143106": {
        "extra-tags": [],
        "date": "2022-08-20",
        "title": "Twitter @karlhigley",
        "summary": "Many ANN search tools (e.g. FAISS, ScaNN) allow you to provide multiple points as part of the same query.\n\nPuzzled why more retrieval models don\u2019t take advantage of this. Give me 100 neighbors of ten points, not 1000 neighbors of one point! \n\n(Then score and order them.)",
        "tags": [
            "twitter",
            "ann"
        ]
    },
    "https://twitter.com/Rainmaker1973/status/1558494690504282112": {
        "extra-tags": [],
        "date": "2022-08-13",
        "title": "Twitter @Rainmaker1973",
        "summary": "Adam Pickard took DALL-E 2's inpainting feature &amp; 57 prompts, asking the AI to draw its impressions, then combined the results in Adobe After Effects obtaining this: an AI version of Eames' \u00abPowers of Ten\u00bb \n\n[read more + original: https://t.co/AU2xpsNjO7] \nhttps://t.co/MpyMjb3Ag3",
        "tags": [
            "twitter",
            "adobe after effects",
            "eames",
            "dall-e 2",
            "powers of ten",
            "adam pickard"
        ]
    },
    "https://twitter.com/Rainmaker1973/status/1558390491258953729": {
        "extra-tags": [],
        "date": "2022-08-13",
        "title": "Twitter @Rainmaker1973",
        "summary": "This photo taken from Phares des Baleines, \u00cele de R\u00e9, France, shows crossing swells, consisting of near-cnoidal wave trains. The interaction of such near-solitons in shallow water may be modeled through the Kadomtsev\u2013Petviashvili equation [read more: https://t.co/lfRCqlBJFo] https://t.co/f1XX5LDYiK",
        "tags": [
            "\u00eele de r\u00e9,",
            "twitter",
            "phares des baleines",
            "kadomtsev",
            "france",
            "petviashvili"
        ]
    },
    "https://twitter.com/NetflixFR/status/1558139248259112960": {
        "extra-tags": [],
        "date": "2022-08-12",
        "title": "Twitter @NetflixFR",
        "summary": "Brooklyn Nine-Nine saison 8, ce soir, d\u00e8s 00h.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/arankomatsuzaki/status/1556438712598220800": {
        "extra-tags": [],
        "date": "2022-08-08",
        "title": "Twitter @arankomatsuzaki",
        "summary": "Few-shot Learning with Retrieval Augmented Language Model\n\nAtlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming PaLM by 3% despite having 50x fewer parameters.\n\nhttps://t.co/ZrgcZugXNu https://t.co/7RlsLZUzFA",
        "tags": [
            "retrieval",
            "twitter",
            "palm",
            "model\n\natlas"
        ]
    },
    "https://twitter.com/hyperfp/status/1556574558794268673": {
        "extra-tags": [],
        "date": "2022-08-08",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty https://t.co/JgXCI29Dmo",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/pommedeterre33/status/1555283345998741504": {
        "extra-tags": [],
        "date": "2022-08-04",
        "title": "Twitter @pommedeterre33",
        "summary": "#protip to benchmark @PyTorch speed on GPU (from xformer repo, never saw it before):\ncreate a big tensor before starting and just clear it before each PyTorch call (clear GPU L2 cache).\nOn small kernels (exec in &lt; 10ms), makes a *BIG* diff (X2/X3 diff)\n+do not forget to cuda sync https://t.co/fFOCvShNYN",
        "tags": [
            "protip",
            "twitter",
            "xformer",
            "pytorch",
            "gpu"
        ]
    },
    "https://twitter.com/hyperfp/status/1554435536374403072": {
        "extra-tags": [],
        "date": "2022-08-02",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty https://t.co/Zq7kfPHR8R",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/JrmyBoo/status/1552928191291039745": {
        "extra-tags": [],
        "date": "2022-07-29",
        "title": "Twitter @JrmyBoo",
        "summary": "@MathisHammel @raphaelsrty Le plus rapide que j'ai pu obtenir\u00a0:\n\nSmall test took 0.0 ms\nLarge test took 8.1 ms\n\nIl s'av\u00e8re qu'un appel \u00e0 random.randint(1,n) est lent, donc j'ai remplac\u00e9 \u00e7a par un appel \u00e0 random.random() qui est plus rapide, puis j'l'ai transform\u00e9 en entier entre 1 et n https://t.co/MsjNCSIb6K",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/JrmyBoo/status/1552930842657316865": {
        "extra-tags": [],
        "date": "2022-07-29",
        "title": "Twitter @JrmyBoo",
        "summary": "@MathisHammel @raphaelsrty Avec ce code, le cas n=500K s'execute en moins de 300ms\n\nen comparaison, avec numpy (voir image), j'ai:\nSmall test took 0.9 ms\nLarge test took 6.3 ms\n\net le cas n=500k s'execute en 187.5 ms\n\nDonc numpy est plus rapide pour n grand mais plus lent plus n petit https://t.co/LDoJ32eo1l",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/MathisHammel/status/1552661799115390979": {
        "extra-tags": [],
        "date": "2022-07-28",
        "title": "Twitter @MathisHammel",
        "summary": "@raphaelsrty Magnifique ! M\u00eame pas tomb\u00e9 dans le pi\u00e8ge ;)",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/mervenoyann/status/1551962473426714624": {
        "extra-tags": [],
        "date": "2022-07-26",
        "title": "Twitter @mervenoyann",
        "summary": "I'm so tired lately, I literally closed the wrong issue. how can you close the wrong issue?",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/MathisHammel/status/1551945447400415240": {
        "extra-tags": [],
        "date": "2022-07-26",
        "title": "Twitter @MathisHammel",
        "summary": "THREAD : Les 10 sites gratuits qui vont r\u00e9volutionner votre vie de dev.\n\nVoici ma liste ultime des outils en ligne qui m'ont \u00e9conomis\u00e9 des journ\u00e9es enti\u00e8res de boulot \u2935\ufe0f https://t.co/dabbKFKC64",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/carbonfact/status/1551579545429475330": {
        "extra-tags": [],
        "date": "2022-07-25",
        "title": "Twitter @carbonfact",
        "summary": "Thrilled to welcome @mru2dev to @carbonfact! David was previously CTO of Pretto, a leading online mortgage broker. He coded the first app in 2016 and scaled it to 1B\u20ac lended to French homeowners. \nDavid now wants to cut carbon. We could not be more excited to have him\ud83c\udf89 https://t.co/7xf8YuFkWA",
        "tags": [
            "pretto",
            "twitter",
            "david"
        ]
    },
    "https://twitter.com/jomaoppa/status/1550966113458077697": {
        "extra-tags": [],
        "date": "2022-07-23",
        "title": "Twitter @jomaoppa",
        "summary": "how programmers overprepare for job interviews https://t.co/f5ip8nraib",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/hyperfp/status/1550797185171787776": {
        "extra-tags": [],
        "date": "2022-07-23",
        "title": "Twitter @hyperfp",
        "summary": "passing message @raphaelsrty https://t.co/okrTIpoJM9",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/lucasgvazquez/status/1550416693683699712": {
        "extra-tags": [],
        "date": "2022-07-22",
        "title": "Twitter @lucasgvazquez",
        "summary": "Kaggle is hard. It's easy to start a competition, clone a baseline get the same results as everyone else, get stuck and bored because it takes 9+ hours to train.\n\nBut you noticed how @jeremyphoward consistently get top results? @Fra_Pochetti and I decided to try out his method \ud83e\uddf5",
        "tags": [
            "twitter",
            "kaggle"
        ]
    },
    "https://twitter.com/JFPuget/status/1550476564055220226": {
        "extra-tags": [],
        "date": "2022-07-22",
        "title": "Twitter @JFPuget",
        "summary": "My most common debugging practice with pytorch: \n\nprint tensor shapes at various points in the code.\n\n The other one is to run on cpu to get better error messages. Only use in last resort as it takes too long.",
        "tags": [
            "pytorch",
            "twitter"
        ]
    },
    "https://twitter.com/fchollet/status/1548840379759308800": {
        "extra-tags": [],
        "date": "2022-07-18",
        "title": "Twitter @fchollet",
        "summary": "Please don't show `from xyz import *` in your official docs / code examples",
        "tags": [
            "twitter",
            "xyz"
        ]
    },
    "https://twitter.com/elan_learns/status/1547239542003867649": {
        "extra-tags": [],
        "date": "2022-07-13",
        "title": "Twitter @elan_learns",
        "summary": "Interested in Knowledge Graphs? Come check out our poster at #NAACL2022 at 2:15pm for our work on\n\nStATIK: Structure and Text for Inductive Knowledge Graph Completion\n\npaper: https://t.co/5LMr4fl3zj https://t.co/zTM5VAhUKQ",
        "tags": [
            "statik",
            "twitter",
            "naacl2022"
        ]
    },
    "https://twitter.com/danieldazac/status/1548376453393235970": {
        "extra-tags": [],
        "date": "2022-07-16",
        "title": "Twitter @danieldazac",
        "summary": "When we can easily find rich descriptions of entities in knowledge graphs, why not use them when learning embeddings?\nGreat to see further progress in learning representations of KGs that incorporate textual descriptions! https://t.co/NCXWFFIWAb",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/suzan/status/1547949855007330308": {
        "extra-tags": [],
        "date": "2022-07-15",
        "title": "Twitter @suzan",
        "summary": "I took notes during #SIGIR2022 and #ICTIR2022 of things that I found interesting or worth a deeper look. A thread \ud83d\udc47\n\n(I obviously did not go to all sessions, this is only a summary of my personal takeaways. I am interested to learn about the highlights I missed.)",
        "tags": [
            "twitter",
            "ictir2022",
            "sigir2022"
        ]
    },
    "https://twitter.com/hyperfp/status/1546780021288620032": {
        "extra-tags": [],
        "date": "2022-07-12",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty presents  his CHERCHE API at \n@SIGIRConf today!\n\"A New Tool to Rapidly Implement Pipelines in Information Retrieval\"\n(Joint work by #Renault and @IRIToulouse)\n#SIGIR2022 Room: \u00abSal\u00f3n de baile\u00bb, Floor: 2, 15:30-17:00\nhttps://t.co/Nq0WJlvOxX",
        "tags": [
            "de baile",
            "twitter",
            "renault",
            "sigir2022"
        ]
    },
    "https://twitter.com/hyperfp/status/1546740802839265282": {
        "extra-tags": [],
        "date": "2022-07-12",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty https://t.co/f4xkwO75I2",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/EmmaGerritse/status/1546412330338205696": {
        "extra-tags": [],
        "date": "2022-07-11",
        "title": "Twitter @EmmaGerritse",
        "summary": "Happy to be at #SIGIR2022 this year! \n\nOn Wednesday, July 13 at 15.00 I'll present our paper:\n\nEntity-aware Transformers for Entity Search\n\nPaper: https://t.co/MvpNh1hsQv \nGithub: https://t.co/uWK6Lz3Mdn\n\nHope to see you there! #SIGIR",
        "tags": [
            "twitter",
            "transformers",
            "github",
            "sigir",
            "sigir2022"
        ]
    },
    "https://twitter.com/AnecdotesMaths/status/1546525043622043648": {
        "extra-tags": [],
        "date": "2022-07-11",
        "title": "Twitter @AnecdotesMaths",
        "summary": "Une puissance de 6 se termine toujours par un 6.\n\nExemples: 6\u00b2 = 36, 6\u00b3 = 216, 6\u2074 = 1296, 6\u2075 = 7776, ...",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/Wingo_Bear/status/1546041279641010178": {
        "extra-tags": [],
        "date": "2022-07-10",
        "title": "Twitter @Wingo_Bear",
        "summary": "\ud83c\udfd4 https://t.co/zkqSn1HHGZ",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/jakevdp/status/1545877570901512193": {
        "extra-tags": [],
        "date": "2022-07-09",
        "title": "Twitter @jakevdp",
        "summary": "A little trick you may not know: to calculate an 18% tip without a calculator, just convert the dollar amount from Celsius to Fahrenheit, subtract 32, and move the decimal point over.\n\nFor example, if the bill is $40: 40C is 104F, minus 32 is 72, so 18% is $7.20\n\nFollow me for mo",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/jobergum/status/1545843922974478336": {
        "extra-tags": [
            "go"
        ],
        "date": "2022-07-09",
        "title": "Twitter @jobergum",
        "summary": "Updated. Let\u2019s go 2022! https://t.co/Zv8twgCqIK",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/AnecdotesMaths/status/1545800310597222403": {
        "extra-tags": [],
        "date": "2022-07-09",
        "title": "Twitter @AnecdotesMaths",
        "summary": "Si vous jouez n fois \u00e0 un jeu o\u00f9 la probabilit\u00e9 de gagner est 1/n, la probabilit\u00e9 de perdre toutes les parties est environ de 1/e \u2248 0,37.\n\nEx: si vous jouez 38 fois \u00e0 la roulette am\u00e9ricaine (38 cases), vous avez environ 37% de chances de ne gagner aucune partie.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/Wingo_Bear/status/1545773212189052928": {
        "extra-tags": [
            "c"
        ],
        "date": "2022-07-09",
        "title": "Twitter @Wingo_Bear",
        "summary": "Au fait, question en particulier pour les copains cr\u00e9ateurs, est-ce qu\u2019on a le droit d\u2019\u00eatre en live twitch en voiture si c\u2019est le passager qui tient bien le t\u00e9l\u00e9phone ? \n\nJ\u2019aimerais trop live la route des trolls mais c\u2019est peut-\u00eatre risqu\u00e9",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1545695290606473218": {
        "extra-tags": [],
        "date": "2022-07-09",
        "title": "Twitter @fishnets88",
        "summary": "I added an introduction to Github Actions to calmcode. It's not doing anything fancy, but it does explain why using the cron schedule makes it much easier to maintain a data science-y Python library.\n\nhttps://t.co/ILXtzDJLQF",
        "tags": [
            "twitter",
            "python",
            "github actions"
        ]
    },
    "https://twitter.com/svpino/status/1545377180951072769": {
        "extra-tags": [],
        "date": "2022-07-08",
        "title": "Twitter @svpino",
        "summary": "Many people new to machine learning have no idea that labeling data is a problem they need to think about.\n\nTo be clear: \"labeled datasets\" aren't a thing in the real world.\n\nHere is an excellent approach to getting past this problem:\n\n1 of 11",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/vedanujg/status/1544925973635690497": {
        "extra-tags": [],
        "date": "2022-07-07",
        "title": "Twitter @vedanujg",
        "summary": "Excited to share our No Language Left Behind project. A single multilingual model capable of translating between any pair across 200+ languages. This long \ud83e\uddf5attempts to discuss some of the technical contributions of NLLB.\n(1/n)\nhttps://t.co/hWqH4F2ZEN",
        "tags": [
            "nllb",
            "twitter",
            "no",
            "left"
        ]
    },
    "https://twitter.com/fishnets88/status/1545032091812913153": {
        "extra-tags": [],
        "date": "2022-07-07",
        "title": "Twitter @fishnets88",
        "summary": "Yep. `pip install bulk` does the trick \ud83d\ude04 https://t.co/YvV2FnjOHO",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/jgmorenof/status/1544233533056614401": {
        "extra-tags": [],
        "date": "2022-07-05",
        "title": "Twitter @jgmorenof",
        "summary": "First keynote from Evangelos Kanoulas just finished but more talks are coming https://t.co/pRWpNRKtjv https://t.co/XCxMs7xuHW",
        "tags": [
            "twitter",
            "evangelos kanoulas"
        ]
    },
    "https://twitter.com/jgmorenof/status/1544226970707365888": {
        "extra-tags": [],
        "date": "2022-07-05",
        "title": "Twitter @jgmorenof",
        "summary": "CIRCLE2022 just started... Here the link YouTube if you want to join us https://t.co/pRWpNRKtjv https://t.co/726TqUKWZq",
        "tags": [
            "twitter",
            "youtube",
            "circle2022"
        ]
    },
    "https://twitter.com/jgmorenof/status/1544053078315470849": {
        "extra-tags": [],
        "date": "2022-07-04",
        "title": "Twitter @jgmorenof",
        "summary": "First evening at CIRCLE2022 and everything seems very promising https://t.co/n6lTURzQ3C",
        "tags": [
            "twitter",
            "circle2022"
        ]
    },
    "https://twitter.com/mervenoyann/status/1543653348284432384": {
        "extra-tags": [],
        "date": "2022-07-03",
        "title": "Twitter @mervenoyann",
        "summary": "sunday at office \ud83d\udc97\u2728 https://t.co/ppoe2PaxXz",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/mervenoyann/status/1543653567831179264": {
        "extra-tags": [],
        "date": "2022-07-03",
        "title": "Twitter @mervenoyann",
        "summary": "as a disclaimer I don't support working on sundays or hustle culture, the office has air conditioner and I don't have anything else to do today, binge watched st4 already",
        "tags": [
            "twitter",
            "st4"
        ]
    },
    "https://twitter.com/mervenoyann/status/1543569899565490177": {
        "extra-tags": [],
        "date": "2022-07-03",
        "title": "Twitter @mervenoyann",
        "summary": "this was the old hugging face employee uniform before @huggingface started doing computer vision, RL, audio and tabular data ML (circa 1870) I found it in an antiques store https://t.co/QN0icoHkms",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/gcabanac/status/1542987206096084992": {
        "extra-tags": [],
        "date": "2022-07-01",
        "title": "Twitter @gcabanac",
        "summary": "The \u2018Problematic Paper Screener\u2019 flags 9,257 articles cited 55k times as problematic with 7 detectors \ud83d\ude35. Your human re-assessment welcome @PubPeer for 5,821 papers with tortured phrases. Let's depollute the scientific literature \ud83d\udd0e https://t.co/hbwmuilCOw https://t.co/uM0WSyQdxh https://t.co/DAulXgW5ah",
        "tags": [
            "problematic paper screener",
            "twitter"
        ]
    },
    "https://twitter.com/guolin_ke/status/1542306983117611009": {
        "extra-tags": [],
        "date": "2022-06-30",
        "title": "Twitter @guolin_ke",
        "summary": "LightGBM is in the top 10 of the NeurIPS papers in recent 5 years, https://t.co/7q4xVyqaoB https://t.co/VRYTGIN7ta",
        "tags": [
            "lightgbm",
            "twitter",
            "neurips"
        ]
    },
    "https://twitter.com/data_topology/status/1542394278638489600": {
        "extra-tags": [],
        "date": "2022-06-30",
        "title": "Twitter @data_topology",
        "summary": "Augmented SBERT ... https://t.co/WnxDjHyCjt\nto transfer domain-specific knowledge\n\nNew real-time Colab, \npython code explained step-by-step.\nTo the best of my knowledge. \n\nNew video:\nhttps://t.co/eYpZyT45gV https://t.co/bXBhaS8mK1",
        "tags": [
            "sbert",
            "twitter",
            "colab"
        ]
    },
    "https://twitter.com/nedbat/status/1540686497556246529": {
        "extra-tags": [],
        "date": "2022-06-25",
        "title": "Twitter @nedbat",
        "summary": "Python has a __file__ global that tells you the current file name.  There's no built-in for the current line number, but you can define a __line__() function for that too:\n(btw bat is https://t.co/X5GldLtfRM) https://t.co/YVg8I9dpmn",
        "tags": [
            "twitter",
            "python"
        ]
    },
    "https://twitter.com/mervenoyann/status/1540325986998394880": {
        "extra-tags": [],
        "date": "2022-06-24",
        "title": "Twitter @mervenoyann",
        "summary": "passive aggressive regressor is performing better with partial_fit compared to fit with same number of iterations https://t.co/0LJqZORM9T",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/mervenoyann/status/1540301369986498561": {
        "extra-tags": [],
        "date": "2022-06-24",
        "title": "Twitter @mervenoyann",
        "summary": "TIL about incremental training types at scikit-learn https://t.co/90rny9epg5",
        "tags": [
            "twitter",
            "scikit"
        ]
    },
    "https://twitter.com/fishnets88/status/1540305619386761216": {
        "extra-tags": [],
        "date": "2022-06-24",
        "title": "Twitter @fishnets88",
        "summary": "@mervenoyann Partial Fit models have some *very* cool properties \ud83d\ude09\n\nhttps://t.co/QKFL9YRpVQ",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/Snowden/status/1538461942750355458": {
        "extra-tags": [],
        "date": "2022-06-19",
        "title": "Twitter @Snowden",
        "summary": "Allez voter. Si vous \u00eates jeune, si vous souffrez, ne laissez pas le syst\u00e8me vous ignorer. Voter! Le pouvoir ne vous sera jamais simplement offert, il doit \u00eatre gagn\u00e9. Pour votre avenir, et l'avenir du monde, allez voter! \n#legislatives2022",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/halford_max/status/1534684351601975296": {
        "extra-tags": [],
        "date": "2022-06-08",
        "title": "Twitter @halford_max",
        "summary": "We organized an unofficial retreat this week with the River team. Managed to get mypy running, write some benchmarks, as well as make plans for the future. Meeting IRL was fun! \ud83d\udcab https://t.co/jB7zGanYRq",
        "tags": [
            "river",
            "twitter"
        ]
    },
    "https://twitter.com/halford_max/status/1533924676153507842": {
        "extra-tags": [],
        "date": "2022-06-06",
        "title": "Twitter @halford_max",
        "summary": "We've just published some benchmarks covering all the models in River. We measure predictive performance, speed, as well as memory usage. You can't improve what you can't measure! https://t.co/Z4xnWBD2Hd",
        "tags": [
            "river",
            "twitter"
        ]
    },
    "https://twitter.com/jayelmnop/status/1532503307666612224": {
        "extra-tags": [],
        "date": "2022-06-02",
        "title": "Twitter @jayelmnop",
        "summary": "TIL in 2009 two Berkeley undergrads flipped a coin *40,000* times (1hr/day for a semester) to see whether a coin flip was truly random (it's biased towards the side facing up pre-flip!)\n\nGives a new meaning to the term \"undergraduate research project\"...\n\nhttps://t.co/2VCD0pTumT https://t.co/jBiJmT7Gh7",
        "tags": [
            "twitter",
            "berkeley"
        ]
    },
    "https://twitter.com/ElCuistoo/status/1532070924589015040": {
        "extra-tags": [],
        "date": "2022-06-01",
        "title": "Twitter @ElCuistoo",
        "summary": "\u00c0 mon pain au chocolat de percer maintenant. https://t.co/lejMuTZXxO",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/Nils_Reimers/status/1531918137171533824": {
        "extra-tags": [],
        "date": "2022-06-01",
        "title": "Twitter @Nils_Reimers",
        "summary": "\ud83e\udd68GPL goes multi-lingual \ud83c\uddfa\ud83c\uddf3\n\nGPL is a great method to adapt dense retrieval models to any domain without labeled data.\n\nSo far GPL was only available for English. Matthias from @ml6team published today a nice article how to adapt GPL to other languages:\nhttps://t.co/r2ZlpTESWO https://t.co/8ZMupXD6mo",
        "tags": [
            "gpl",
            "twitter",
            "matthias"
        ]
    },
    "https://twitter.com/nedbat/status/1531610946413666306": {
        "extra-tags": [],
        "date": "2022-05-31",
        "title": "Twitter @nedbat",
        "summary": "Python dict subclasses can define __missing__: it's called when a key is missing.  Instead of hiding a dict in a function as a cache, how about hiding a function in a dict!? A Fibonacci dictionary: https://t.co/N2hE8p76lO",
        "tags": [
            "twitter",
            "python",
            "fibonacci dictionary"
        ]
    },
    "https://twitter.com/NetflixFR/status/1531712887064088581": {
        "extra-tags": [],
        "date": "2022-05-31",
        "title": "Twitter @NetflixFR",
        "summary": "Direction Neo-Tokyo.\n\nAKIRA (1988), dispo \u00e0 minuit. https://t.co/kd87TSw67m",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/halford_max/status/1530632316351021059": {
        "extra-tags": [],
        "date": "2022-05-28",
        "title": "Twitter @halford_max",
        "summary": "River 0.11 is released :) https://t.co/fOjJMtzu5I",
        "tags": [
            "twitter",
            "river 0.11"
        ]
    },
    "https://twitter.com/fishnets88/status/1530443807258558468": {
        "extra-tags": [],
        "date": "2022-05-28",
        "title": "Twitter @fishnets88",
        "summary": "TIL: deadlink. It's a tool that helps find dead links in documentation. It can even replace redirected links if you really want. \n\nhttps://t.co/j3g56oeSfm",
        "tags": [
            "twitter",
            "deadlink"
        ]
    },
    "https://twitter.com/PatrickKidger/status/1530194789945200640": {
        "extra-tags": [],
        "date": "2022-05-27",
        "title": "Twitter @PatrickKidger",
        "summary": "Me: I should be doing math/ML research.\n\nAlso me: distracted from research making fun open-source projects.\n\nAlso also me: distracted from open-source by plotting the entire history of their GitHub stars! \u2b50Which will break 1k first? Will Diffrax overtake Equinox again? etc...!\u26a1\ufe0f https://t.co/b2SlDOmA5V",
        "tags": [
            "github",
            "twitter",
            "diffrax",
            "equinox"
        ]
    },
    "https://twitter.com/fishnets88/status/1530061176863870977": {
        "extra-tags": [],
        "date": "2022-05-27",
        "title": "Twitter @fishnets88",
        "summary": "Big shoutout to @ucodery for implementing a registry for mktestdocs! That means that the project now also supports bash codeblocks or ... any other langauge you can write a checker for! \n\nVersion 0.2.0 is out now. \n\nhttps://t.co/zhs2AiGTfy",
        "tags": [
            "twitter",
            "mktestdocs"
        ]
    },
    "https://twitter.com/__femb0t/status/1529618328884486144": {
        "extra-tags": [],
        "date": "2022-05-26",
        "title": "Twitter @__femb0t",
        "summary": "https://t.co/vVrF8ZELch",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/danieldazac/status/1528719907826933767": {
        "extra-tags": [],
        "date": "2022-05-23",
        "title": "Twitter @danieldazac",
        "summary": "Excited for my first in-person conference at #ACL2022!  \nIf you are interested in how GANs can be used to detect mentions of entities without labeled data, I'll be presenting SlotGAN (https://t.co/R8UAnGglmS) on Friday at the Structured Prediction workshop.\n\nBelow a summary \ud83d\udc47",
        "tags": [
            "gans",
            "acl2022",
            "twitter",
            "slotgan"
        ]
    },
    "https://twitter.com/JFPuget/status/1527585713314668544": {
        "extra-tags": [],
        "date": "2022-05-20",
        "title": "Twitter @JFPuget",
        "summary": "@charles_irl I used this idea in a Kaggle competition. It led to the best single model of the competition: https://t.co/Ajzw4d3lGy",
        "tags": [
            "twitter",
            "kaggle"
        ]
    },
    "https://twitter.com/_akhaliq/status/1526738002621472768": {
        "extra-tags": [],
        "date": "2022-05-18",
        "title": "Twitter @_akhaliq",
        "summary": "SKILL: Structured Knowledge Infusion for Large Language Models\nabs: https://t.co/vbExGmg4hx https://t.co/3hVTWxLVE1",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/hyperfp/status/1527049911459622914": {
        "extra-tags": [],
        "date": "2022-05-18",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty:  \"a method to infuse structured knowledge into LMs, by directly training T5 models on triples of knowledge graphs... No alignment between the KG and text corpus is required\" https://t.co/zTNJYvYxTr",
        "tags": [
            "lms",
            "twitter"
        ]
    },
    "https://twitter.com/AdilZtn/status/1526952598963728386": {
        "extra-tags": [
            "pytorch"
        ],
        "date": "2022-05-18",
        "title": "Twitter @AdilZtn",
        "summary": "@PyTorch @raphaelsrty",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/PyTorch/status/1526944876478144512": {
        "extra-tags": [],
        "date": "2022-05-18",
        "title": "Twitter @PyTorch",
        "summary": "We\u2019re excited to announce support for GPU-accelerated PyTorch training on Mac! Now you can take advantage of Apple silicon GPUs to perform ML workflows like prototyping and fine-tuning. Learn more: https://t.co/8VmtnhfrZy https://t.co/iSFWUiXhSW",
        "tags": [
            "twitter",
            "mac",
            "pytorch",
            "apple silicon",
            "gpu"
        ]
    },
    "https://twitter.com/tunguz/status/1525172729359613954": {
        "extra-tags": [],
        "date": "2022-05-13",
        "title": "Twitter @tunguz",
        "summary": "I heard that when you finish a PhD in computer science, they take you to a special room and explain that you must never use recursion in real life. Its only purpose is to make programming hard for undergrads.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/litcapital/status/1524801141506187264": {
        "extra-tags": [],
        "date": "2022-05-12",
        "title": "Twitter @litcapital",
        "summary": "\u201cIt appears your stablecoin wasn\u2019t so stable, Mr. Bond\u201d https://t.co/5wtZfDnZAr",
        "tags": [
            "twitter",
            "stablecoin",
            "mr. bond"
        ]
    },
    "https://twitter.com/jamescalam/status/1524411778200969223": {
        "extra-tags": [],
        "date": "2022-05-11",
        "title": "Twitter @jamescalam",
        "summary": "Say hello to the future of NLP topic modeling!\n\n\ud83e\udd16 NLP transformer embedding\n\ud83e\udde0 Advanced dim reduction + clustering with UMAP and HDBSCAN\n\ud83d\udd16 Identifies topics using c-TF-IDF\n\n@MaartenGr's BERTopic does all this + more in a few lines of code \ud83d\udc4f\nhttps://t.co/Dk6SuTGh28\n\n#NLProc",
        "tags": [
            "twitter",
            "nlproc",
            "umap",
            "nlp",
            "hdbscan"
        ]
    },
    "https://twitter.com/fishnets88/status/1524302046417108994": {
        "extra-tags": [
            "embeddings"
        ],
        "date": "2022-05-11",
        "title": "Twitter @fishnets88",
        "summary": "This story has been out for a while, but it's still kind of mind-boggling just in how many different ways word embeddings can throw me off. \n\nhttps://t.co/urcDz3wgnH",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/Grimkujow_/status/1524419573516222465": {
        "extra-tags": [],
        "date": "2022-05-11",
        "title": "Twitter @Grimkujow_",
        "summary": "Mdrr m\u00eame Overwatch 2 a plus de changements https://t.co/ChOMDlXAaq",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/heykahn/status/1524422842950975489": {
        "extra-tags": [],
        "date": "2022-05-11",
        "title": "Twitter @heykahn",
        "summary": "The longest running study on happiness:\n\nHarvard's 84 year old Study of Adult Development.\n\nHere are 7 lessons from the study to help you live a happier life:",
        "tags": [
            "twitter",
            "harvard"
        ]
    },
    "https://twitter.com/giffmana/status/1524372243207364608": {
        "extra-tags": [],
        "date": "2022-05-11",
        "title": "Twitter @giffmana",
        "summary": "Most people have absolutely no sense for the insane diversity of things covered in O(billion) web images.\n\nI'm not sure it is meaningful to talk about ood, distribution shift, generalisation, etc. anymore at that scale.\n\nIt will take the collective us some time to digest this. https://t.co/t19yG2Gzkf",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/AdilZtn/status/1524002828297121794": {
        "extra-tags": [],
        "date": "2022-05-10",
        "title": "Twitter @AdilZtn",
        "summary": "@visualizevalue @raphaelsrty",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1523550848747511813": {
        "extra-tags": [],
        "date": "2022-05-09",
        "title": "Twitter @fishnets88",
        "summary": "TIL that scikit-learn has a neural network implementation. I also learned it has some likeable properties, like partial_fit() and sparse support, that I did not expect. \n\nI also ran a benchmark, readable here:\nhttps://t.co/icDQsuTggI",
        "tags": [
            "twitter",
            "scikit"
        ]
    },
    "https://twitter.com/OutOfCohen/status/1521897266747002880": {
        "extra-tags": [],
        "date": "2022-05-04",
        "title": "Twitter @OutOfCohen",
        "summary": "Compilation de Marc qui s'incruste dans des pubs https://t.co/fIzdNuVOxK",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fchollet/status/1522604791385366529": {
        "extra-tags": [],
        "date": "2022-05-06",
        "title": "Twitter @fchollet",
        "summary": "It's when you're young (and still have all roads open to you) that you have the most need to think long term. But that's also when you're least capable of doing so.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/pratik_ratadiya/status/1522078363271110656": {
        "extra-tags": [],
        "date": "2022-05-05",
        "title": "Twitter @pratik_ratadiya",
        "summary": "Facebook as a social media company may have done controversial stuff, but Facebook as a tech company has been simply exceptional:\n\n- React JS\n- GraphQL\n- PyTorch\n- ONNX\n\nVery few companies that have led the development of such game changing, open-source tools across domains.",
        "tags": [
            "twitter",
            "onnx",
            "facebook",
            "js\n- graphql"
        ]
    },
    "https://twitter.com/JFPuget/status/1521923175285633024": {
        "extra-tags": [],
        "date": "2022-05-04",
        "title": "Twitter @JFPuget",
        "summary": "Here are my solution highlights https://t.co/U9vA80C9tF https://t.co/CPaHLNdjAT",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/mattturck/status/1521626927060201472": {
        "extra-tags": [],
        "date": "2022-05-03",
        "title": "Twitter @mattturck",
        "summary": "When your startup never gets acquired https://t.co/LFwdu44cIF",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/OcRasoir/status/1521249031212703744": {
        "extra-tags": [],
        "date": "2022-05-02",
        "title": "Twitter @OcRasoir",
        "summary": "1. L'\u00e9chantillon est relativement petit (20 r\u00e9ponses apr\u00e8s; 21 r\u00e9ponses avant) ce qui laisse trop de place au bruit statistique.\n\n2. Une session de 20 minutes, c'est trop court pour avoir un effet significatif. Des QCM bay\u00e9siens sur tout un semestre marcheraient mieux.\n\n15/19",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/OcRasoir/status/1521248058348118016": {
        "extra-tags": [
            "r"
        ],
        "date": "2022-05-02",
        "title": "Twitter @OcRasoir",
        "summary": "Aux @RECtlse, nous avions propos\u00e9 une petite exp\u00e9rience pour r\u00e9ponde \u00e0 la question suivante :\n\"Les QCM bay\u00e9siens am\u00e9liorent-ils nos jugements ?\"\n\nOn vous fait un retour \u2b07\ufe0f\u2b07\ufe0f\n\n(version courte : nos r\u00e9sultats ne montrent presque rien, mais \u00e7a veut pas dire grand chose)\n\n1/19",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/Snowden/status/1521573829881049091": {
        "extra-tags": [
            "time"
        ],
        "date": "2022-05-03",
        "title": "Twitter @Snowden",
        "summary": "Neither a law nor a court can truly justify the revocation of a human right; the most fundamental of our freedoms are inabrogable. The repression of such an essential liberty may be effective, for a time, but it cannot be legitimate.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1521006658561916929": {
        "extra-tags": [],
        "date": "2022-05-02",
        "title": "Twitter @fishnets88",
        "summary": "It's been approximately 5 years since I first read this blogpost.\n\nhttps://t.co/CcEuhCxtFL",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/Yangoliatko_/status/1520464669386944512": {
        "extra-tags": [],
        "date": "2022-04-30",
        "title": "Twitter @Yangoliatko_",
        "summary": "The moment a women in South Wales met the Ukrainian family she was taking in.\n\n\ud83c\uddfa\ud83c\udde6\ud83e\udef6\ud83c\udff4\udb40\udc67\udb40\udc62\udb40\udc77\udb40\udc6c\udb40\udc73\udb40\udc7f https://t.co/bm1ltzzdjw",
        "tags": [
            "twitter",
            "south wales"
        ]
    },
    "https://twitter.com/renard_alpin/status/1519965269669683200": {
        "extra-tags": [
            "c"
        ],
        "date": "2022-04-29",
        "title": "Twitter @renard_alpin",
        "summary": "C'est HONTEUX @castorama_fr https://t.co/oLYnN5kZDb",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/hyperfp/status/1520393014476255233": {
        "extra-tags": [],
        "date": "2022-04-30",
        "title": "Twitter @hyperfp",
        "summary": "Oasis de Bilma, #Niger https://t.co/EJoyH0eD6f",
        "tags": [
            "oasis",
            "twitter",
            "bilma",
            "niger"
        ]
    },
    "https://twitter.com/AllanDeneuville/status/1520081833219825665": {
        "extra-tags": [],
        "date": "2022-04-29",
        "title": "Twitter @AllanDeneuville",
        "summary": "Pendant que certains s'\u00e9gratinent sur le salaire des MCFs, les doctorants voient ce genre d'annonce. \nLe seuil de pauvret\u00e9 est \u00e0 1 102 euros. \n\ud83e\udd21 https://t.co/Tbr3Qj8imv",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/philipvollet/status/1519538002132975616": {
        "extra-tags": [],
        "date": "2022-04-28",
        "title": "Twitter @philipvollet",
        "summary": "Top2Vec is an algorithm for topic modeling and semantic search. It automatically detects topics present in text and generates jointly embedded topic, document and word vectors.\n\nGitHub https://t.co/NECU37gNbB\nPaper https://t.co/lgIkbTvO1s https://t.co/Q6qjGe4due",
        "tags": [
            "top2vec",
            "twitter",
            "github"
        ]
    },
    "https://twitter.com/Nils_Reimers/status/1519391329633148940": {
        "extra-tags": [],
        "date": "2022-04-27",
        "title": "Twitter @Nils_Reimers",
        "summary": "@ramsri_goutham Yes, you can train other models. It is important to train with dot product. The all-* models have a normalization layer, so you don't get dot scores but cosine similarity. Remove the normalization layer and it works.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/newsycombinator/status/1518711709040578560": {
        "extra-tags": [
            "algorithm"
        ],
        "date": "2022-04-25",
        "title": "Twitter @newsycombinator",
        "summary": "twitter/the-algorithm https://t.co/TihqmvOsf2",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/CedricCuaz/status/1517439006279180288": {
        "extra-tags": [],
        "date": "2022-04-22",
        "title": "Twitter @CedricCuaz",
        "summary": "How to get a factorization model sensitive to hierarchical partitioning on graphs, for most unsupervised tasks you can imagine? Use the semi-relaxed Gromov-Wasserstein divergence! #OptimalTransport\n\nMeet me at the Poster session 1 at #iclr2022 next Monday to discuss about it \ud83d\ude09 https://t.co/bIPBMTe6pu",
        "tags": [
            "twitter",
            "gromov-wasserstein",
            "iclr2022"
        ]
    },
    "https://twitter.com/mervenoyann/status/1516424033801277446": {
        "extra-tags": [],
        "date": "2022-04-19",
        "title": "Twitter @mervenoyann",
        "summary": "just putting this out there, you can host any dataset on @huggingface Hub, even datasets above 100 GB \nofc for free",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/Lab445/status/1515584059916918784": {
        "extra-tags": [],
        "date": "2022-04-17",
        "title": "Twitter @Lab445",
        "summary": "@AzaronOff @Marineuh_ Merci Azaron. Penses-tu pouvoir leur adresser un t shirt de ta m\u00e8re assez rapidement ?",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/michiyasunaga/status/1511382882173915137": {
        "extra-tags": [],
        "date": "2022-04-05",
        "title": "Twitter @michiyasunaga",
        "summary": "Excited to share LinkBERT\ud83d\udd17: new language models capturing document link knowledge, e.g. hyperlinks of the web! Achieve SOTA on various open-domain QA and BioNLP tasks\ud83d\ude4c\n\nPaper, model, code: https://t.co/v8pDWa8HnD\n\nw/ @percyliang @jure @StanfordNLP @StanfordAILab #ACL2022\n[1/n] https://t.co/T0lLz6H8W6",
        "tags": [
            "qa",
            "bionlp",
            "twitter",
            "sota",
            "acl2022"
        ]
    },
    "https://twitter.com/Kammeto/status/1511124358160388099": {
        "extra-tags": [
            "r"
        ],
        "date": "2022-04-04",
        "title": "Twitter @Kammeto",
        "summary": "@Rubiu5 \ud83c\uddeb\ud83c\uddf7 R A T I O \ud83c\uddeb\ud83c\uddf7",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/SahilBloom/status/1510249267352457223": {
        "extra-tags": [],
        "date": "2022-04-02",
        "title": "Twitter @SahilBloom",
        "summary": "The most dangerous mental errors (you don\u2019t know you\u2019re making):",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/mervenoyann/status/1509887112652468252": {
        "extra-tags": [
            "language model"
        ],
        "date": "2022-04-01",
        "title": "Twitter @mervenoyann",
        "summary": "\u201cfirst slightly conscious language model\u201d \ud83d\ude02 https://t.co/ZTxb6F6M41",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/dr646464/status/1509560753195470852": {
        "extra-tags": [],
        "date": "2022-03-31",
        "title": "Twitter @dr646464",
        "summary": "#aupaysbasqueonestpasdeschochottes\nVu ce matin, \u00ab\u00a0il s\u2019est fait mal en plaquant mardi soir au rugby, on s\u2019est dit qu\u2019il \u00e9tait douillet mais il a l\u2019air d\u2019avoir mal\u00a0\u00bb\u2026.\nY\u2019avait pas bcp de doutes cliniques non plus. https://t.co/LWnqSlAYWd",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/LuckInTheShell/status/1509571786454753286": {
        "extra-tags": [],
        "date": "2022-03-31",
        "title": "Twitter @LuckInTheShell",
        "summary": "Il existe un univers parall\u00e8le o\u00f9 apr\u00e8s avoir vann\u00e9 Jada Smith, Chris Rock descend de la sc\u00e8ne et gifle Will Smith pour avoir ri \u00e0 sa blague. Quelque minutes plus tard, Javier Bardem remporte l'oscar du meilleur acteur",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/PatrickKidger/status/1509190106757976071": {
        "extra-tags": [],
        "date": "2022-03-30",
        "title": "Twitter @PatrickKidger",
        "summary": "I've been doing this for several months now too: \"workationing\" full-time. (aka digital nomadism)\n\nI'm currently in Chania, Greece! In the past six months I've been through Paris, Copenhagen, Berlin, Rome, ...\n\nWould \ud83d\udcaf recommend. Anyone else tried this? https://t.co/Dopk5iSpwD",
        "tags": [
            "rome",
            "twitter",
            "chania",
            "greece",
            "copenhagen",
            "berlin",
            "paris"
        ]
    },
    "https://twitter.com/michael_galkin/status/1507066679590887427": {
        "extra-tags": [],
        "date": "2022-03-24",
        "title": "Twitter @michael_galkin",
        "summary": "Prepare your GNNs - with the team @keenuniverse, we are launching an open challenge \ud83c\udfc6 for inductive link prediction on KGs where inference is done over a new graph with unseen entities\n\nGithub: https://t.co/t4bxW6BBTj\nBlog with details: https://t.co/EdTgKHtBeg",
        "tags": [
            "gnns",
            "github",
            "twitter",
            "kgs"
        ]
    },
    "https://twitter.com/tydsh/status/1508211678466387972": {
        "extra-tags": [],
        "date": "2022-03-27",
        "title": "Twitter @tydsh",
        "summary": "Our ICLR'22 paper (https://t.co/SVWWryW431) studies the phenomenon of dimensional collapsing in contrastive learning, in which the learned representation has a few dimensions collapsed to zero. Thanks all co-authors (@jingli9111, Pascal Vincent and @ylecun) for the great work! 1/",
        "tags": [
            "twitter",
            "pascal vincent",
            "iclr"
        ]
    },
    "https://twitter.com/fishnets88/status/1507294842061209603": {
        "extra-tags": [],
        "date": "2022-03-25",
        "title": "Twitter @fishnets88",
        "summary": "I'll be iterating on DoubtLab soon-ish. It's a library that allows you to find bad labels in your data. In case folks are into giving it a spin ... I'd love to start getting some feedback! \n\nhttps://t.co/QRUerlA8o2",
        "tags": [
            "twitter",
            "doubtlab"
        ]
    },
    "https://twitter.com/hyperfp/status/1507385633928622084": {
        "extra-tags": [],
        "date": "2022-03-25",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty : la ville rose ;-) https://t.co/bqVz1C2CoT",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/carbonfact/status/1507371286003175435": {
        "extra-tags": [],
        "date": "2022-03-25",
        "title": "Twitter @carbonfact",
        "summary": "Carbonfact team reunited for the first time \ud83d\ude0d https://t.co/2xuB8YbGT2",
        "tags": [
            "twitter",
            "carbonfact"
        ]
    },
    "https://twitter.com/mervenoyann/status/1506021664441602051": {
        "extra-tags": [],
        "date": "2022-03-21",
        "title": "Twitter @mervenoyann",
        "summary": "join this awesome sprint, made with love by @_nateraw \ud83d\udc97\ud83e\udd17 we\u2019ll collaboratively create great GAN projects and build demos based on them \ud83c\udf0c https://t.co/GW8aK5vKoP",
        "tags": [
            "twitter",
            "gan"
        ]
    },
    "https://twitter.com/vsoch/status/1506062313438081027": {
        "extra-tags": [],
        "date": "2022-03-22",
        "title": "Twitter @vsoch",
        "summary": "Have you ever wanted to put some machine learning thing online, but \"ohno, it's TOO BIG?\" You likely need some complex setup to update models in batches, and some modified database strategy to handle really big data. This was me about a month ago, and I didn't like these options. https://t.co/5nHNZAKpzX",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/attn/status/1504423551717163008": {
        "extra-tags": [],
        "date": "2022-03-17",
        "title": "Twitter @attn",
        "summary": ".@Schwarzenegger has a message for the Russian people. https://t.co/AIOxoCQyrX",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/hyperfp/status/1504818746774413312": {
        "extra-tags": [],
        "date": "2022-03-18",
        "title": "Twitter @hyperfp",
        "summary": "Je l'aime bien, Schwarzenegger https://t.co/kWrrMW7qHO",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/JFPuget/status/1504510284672688131": {
        "extra-tags": [],
        "date": "2022-03-17",
        "title": "Twitter @JFPuget",
        "summary": "Best is to explicitly declare the device at the start of scripts or notebooks, for instance:\n\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\ndevice = torch.device('cuda')\n\nThen people can replace \"cuda\" with \"cpu\" or change the GPU number if using several. https://t.co/qd8ATR3dd4",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/RaphaelleBacque/status/1503472589070548995": {
        "extra-tags": [
            "r"
        ],
        "date": "2022-03-14",
        "title": "Twitter @RaphaelleBacque",
        "summary": "Les vrais territoires perdus de la R\u00e9publique, ce sont ces lyc\u00e9es d\u2019excellence si difficiles d\u2019acc\u00e8s aux enfants de familles modestes et de province. Un t\u00e9moignage accablant\u2026 https://t.co/JnNi1VUDOA",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/VincentMcaigne/status/1502570301841891331": {
        "extra-tags": [],
        "date": "2022-03-12",
        "title": "Twitter @VincentMcaigne",
        "summary": "EN M\u00caME TEMPS / kervern,Delepine / @jocohenlebon https://t.co/QjHbruMt0N",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/carrigmat/status/1502319813510766599": {
        "extra-tags": [],
        "date": "2022-03-11",
        "title": "Twitter @carrigmat",
        "summary": "Hey all! @huggingface needs some help from community contributors to make our codebase a lot simpler and more maintainable. There are two big changes we want to make to almost every model class, and even if they're simple in isolation, it's a lot of work across the codebase! \ud83e\uddf5",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/cecilegres/status/1501978699087941639": {
        "extra-tags": [],
        "date": "2022-03-10",
        "title": "Twitter @cecilegres",
        "summary": "Do you know Antoine Dupont? https://t.co/Mc6N64HYIt",
        "tags": [
            "twitter",
            "antoine dupont"
        ]
    },
    "https://twitter.com/julien_c/status/1501142634252881923": {
        "extra-tags": [],
        "date": "2022-03-08",
        "title": "Twitter @julien_c",
        "summary": "Are you interested in Inference optimization?\n\n\ud83d\udd25 Step 1: export your model to ONNX directly from transformers: https://t.co/fpkNozOCHZ\n\n\ud83c\udfce Step 2: Use our new repo Optimum https://t.co/GcgwzGzf2R to optimize your ONNX (operator fusion etc), including to a target hardware https://t.co/a8cs8PmLRe",
        "tags": [
            "twitter",
            "onnx"
        ]
    },
    "https://twitter.com/KyivIndependent/status/1499808377244995592": {
        "extra-tags": [],
        "date": "2022-03-04",
        "title": "Twitter @KyivIndependent",
        "summary": "\u26a1\ufe0fThree Ukrainian supermarket chains - Silpo, Novus, and Varus - stop selling Coca-Cola products as the company continues to operate in Russia.",
        "tags": [
            "silpo",
            "novus",
            "twitter",
            "coca-cola",
            "russia",
            "varus"
        ]
    },
    "https://twitter.com/andreivaitovich/status/1499512675403022343": {
        "extra-tags": [],
        "date": "2022-03-03",
        "title": "Twitter @andreivaitovich",
        "summary": "Vous voyez une base militaire ? Moi non plus. #UkraineRussianWar https://t.co/qmr6qIWxJH",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/edgedatabase/status/1499842655840190468": {
        "extra-tags": [],
        "date": "2022-03-04",
        "title": "Twitter @edgedatabase",
        "summary": "EdgeDB stands with Ukraine. We unequivocally condemn the unprovoked Russian military invasion of a peaceful sovereign country.\n\nHere\u2019s how to help: \ud83d\udc47",
        "tags": [
            "ukraine",
            "twitter",
            "edgedb"
        ]
    },
    "https://twitter.com/r_netsec/status/1499840058517450753": {
        "extra-tags": [],
        "date": "2022-03-04",
        "title": "Twitter @r_netsec",
        "summary": "List of companies that have cut business ties with Russia. https://t.co/ghIAWI3Qkj",
        "tags": [
            "twitter",
            "russia"
        ]
    },
    "https://twitter.com/Qofficiel/status/1499097150348746753": {
        "extra-tags": [],
        "date": "2022-03-02",
        "title": "Twitter @Qofficiel",
        "summary": "Hier, le ministre des Affaires \u00e9trang\u00e8res de Poutine s\u2019est exprim\u00e9 \u00e0 la tribune de l\u2019ONU\u2026et la t\u00e9l\u00e9 russe s\u2019est livr\u00e9e \u00e0 un monumental montage qui est pass\u00e9 totalement inaper\u00e7u  \u2b07\ufe0f\n\n#Quotidien https://t.co/iBRkph5Rrj",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/naverlabseurope/status/1498308080878116867": {
        "extra-tags": [],
        "date": "2022-02-28",
        "title": "Twitter @naverlabseurope",
        "summary": "Join this week's live \ud83d\udce2Open seminar with @Nils_Reimers of @huggingface on 'Unsupervised domain adaptation for neural search'. Registration is open! #NLProc \n\ud83d\udcc5 Thur Mar 3rd 11am CET (Paris)\nRegister: https://t.co/ARFxTXxXr1\nOpen seminars: https://t.co/iZENihdBNJ https://t.co/8lYl0Y62ZY",
        "tags": [
            "nlproc",
            "twitter",
            "paris"
        ]
    },
    "https://twitter.com/gcabanac/status/1498606433037955073": {
        "extra-tags": [],
        "date": "2022-03-01",
        "title": "Twitter @gcabanac",
        "summary": "The \u2018Problematic Paper Screener\u2019 has just turned one \ud83c\udf82. It flags 5,716 articles as problematic with 7 detectors. They are cited 38k times \ud83d\ude12 Only 1,516 reported on @PubPeer. Let's depollute the scientific literature: 3,226 tortured papers to re-assess \ud83d\udd0e https://t.co/fSmzEq49x9 https://t.co/xkeYFeLCPh",
        "tags": [
            "problematic paper screener",
            "twitter"
        ]
    },
    "https://twitter.com/victordeboer/status/1498206917621166080": {
        "extra-tags": [],
        "date": "2022-02-28",
        "title": "Twitter @victordeboer",
        "summary": "Very happy and a bit proud to find this on my desk this morning!! I'm sure @wxwilcke you'll do great at your PhD defense! #semanticweb #knowledgegraph #MachineLearning https://t.co/FO8lcoBb1s",
        "tags": [
            "twitter",
            "machinelearning",
            "semanticweb"
        ]
    },
    "https://twitter.com/hyperfp/status/1495914332554764289": {
        "extra-tags": [],
        "date": "2022-02-22",
        "title": "Twitter @hyperfp",
        "summary": "human-machine hybrid tool for construction of knowledge bases using extractive search\n\n@raphaelsrty https://t.co/MVhyHuERAm",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/phillipstroebel/status/1494800812438134786": {
        "extra-tags": [],
        "date": "2022-02-18",
        "title": "Twitter @phillipstroebel",
        "summary": "@emnlpmeeting \"with a culture that embraces diversity while emphasizing mutual tolerance and privacy\" - sorry, in what world are you living? Oppression of women, death penalty on homosexuality ... if you want to organise the EMNLP there, go ahead, but please don't whitewash the UAE!",
        "tags": [
            "twitter",
            "uae",
            "emnlp"
        ]
    },
    "https://twitter.com/successar_nlp/status/1494757908625584130": {
        "extra-tags": [],
        "date": "2022-02-18",
        "title": "Twitter @successar_nlp",
        "summary": "@emnlpmeeting Would you recommend transgender members of NLP community to travel there ? https://t.co/zIkxOCnPVX",
        "tags": [
            "nlp",
            "twitter"
        ]
    },
    "https://twitter.com/IranzoSanchez/status/1494791183809359872": {
        "extra-tags": [],
        "date": "2022-02-18",
        "title": "Twitter @IranzoSanchez",
        "summary": "@emnlpmeeting Terribly dissapointing. It is unnaceptable to hold conferences in places where basic human rights are not respected. These regimes should be ostracized instead of legitimized. I will NOT be attending EMNLP22 and hope many others do the same.",
        "tags": [
            "twitter",
            "emnlp22"
        ]
    },
    "https://twitter.com/RichardSocher/status/1494379085170032640": {
        "extra-tags": [],
        "date": "2022-02-17",
        "title": "Twitter @RichardSocher",
        "summary": "Our amazing AI and ranking team just pushed a new ranker that now incorporates not just semantics, string matching, etc. but also popularity of answers and hence brings way more and better results, eg. a coding question a la\nhttps://t.co/Uz7iSC8Wbg https://t.co/COPnrZgjkP",
        "tags": [
            "twitter",
            "ai"
        ]
    },
    "https://twitter.com/JinaAI_/status/1493943444854943747": {
        "extra-tags": [],
        "date": "2022-02-16",
        "title": "Twitter @JinaAI_",
        "summary": "We're proud to announce the all-new Jina 3\ud83d\ude80\n\n\u2699\ufe0f Executor Sandbox:Try before you \"Buy\"\n\ud83d\udc0bFlows: Out-of-the-box integration with @Docker  compose &amp; @kubernetesio\n\ud83d\udcddDocArray:Visualize, share doc stores, &amp; more\n\nTry now! https://t.co/D2Fhp3z8kK\nRead more: https://t.co/0g739h07xu https://t.co/gZZfxQY5VP",
        "tags": [
            "jina",
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1493604501399474186": {
        "extra-tags": [],
        "date": "2022-02-15",
        "title": "Twitter @fishnets88",
        "summary": "There's a few tiny tools that make the day-to-day devstuff nicer. Today I learned about a new one: pur! It's a cli that updates all version numbers in a requirements.txt file. \n\nhttps://t.co/N7boU5QOwu",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/lexfridman/status/1492603675340156933": {
        "extra-tags": [],
        "date": "2022-02-12",
        "title": "Twitter @lexfridman",
        "summary": "People need love more than they need advice. Most people know the right thing to do, they just need someone to believe in them.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/dlouapre/status/1492445287889129476": {
        "extra-tags": [],
        "date": "2022-02-12",
        "title": "Twitter @dlouapre",
        "summary": "Nouvelle vid\u00e9o ! Je d\u00e9fonce WORDLE \ud83d\udfe9\u2b1b\ud83d\udfe8\u2b1b\ud83d\udfe8 Je vous montre comment programmer un algorithme qui joue de fa\u00e7on optimale !\nhttps://t.co/3oswZeCiZS https://t.co/A2O4Lc41KZ",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/jamescalam/status/1492154700908892167": {
        "extra-tags": [],
        "date": "2022-02-11",
        "title": "Twitter @jamescalam",
        "summary": "Awesome answer on how to do doc-level embeddings from @Nils_Reimers - TLDR don't \ud83d\ude09 https://t.co/74nrbUX2OC",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/danieldekok/status/1491858289226887174": {
        "extra-tags": [],
        "date": "2022-02-10",
        "title": "Twitter @danieldekok",
        "summary": "We submitted a patch to PyTorch that makes spaCy transformer models ~10-20% faster on M1 Macs (USE_SLEEF_FOR_ARM_VEC256=1). A more detailed write-up will follow later.\n\nhttps://t.co/OUaQOLbEgN",
        "tags": [
            "pytorch",
            "twitter",
            "m1 macs"
        ]
    },
    "https://twitter.com/memecrashes/status/1491426456794464258": {
        "extra-tags": [],
        "date": "2022-02-09",
        "title": "Twitter @memecrashes",
        "summary": "https://t.co/0WZG9mVCKM",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/PatrickKidger/status/1491069456185200640": {
        "extra-tags": [],
        "date": "2022-02-08",
        "title": "Twitter @PatrickKidger",
        "summary": "\u26a1\ufe0f My PhD thesis is on arXiv! \u26a1\ufe0f\n\nTo quote my examiners it is \"the textbook of neural differential equations\" - across ordinary/controlled/stochastic diffeqs.\n\nw/ unpublished material:\n- generalised adjoint methods\n- symbolic regression\n- + more!\n\nhttps://t.co/Pm5l6FhEED\n\nv\ud83e\uddf5 1/n https://t.co/VLMm4nl1vU",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/XciD_/status/1491099286041337856": {
        "extra-tags": [],
        "date": "2022-02-08",
        "title": "Twitter @XciD_",
        "summary": "So happy to open the first Huggingface office in Lyon \ud83e\udd17 @huggingface @Wojo_co https://t.co/54mgy2OpZx",
        "tags": [
            "twitter",
            "lyon",
            "huggingface"
        ]
    },
    "https://twitter.com/fishnets88/status/1490613616399290371": {
        "extra-tags": [],
        "date": "2022-02-07",
        "title": "Twitter @fishnets88",
        "summary": "Wrote a blog post on how one might integrate @JinaAI_ or Lunr into @Rasa_HQ to build a recipe recommending chatbot. It's a pretty nice demo of the difference between classic search vs. contextualized search.\n\nhttps://t.co/4SHsshKQSZ",
        "tags": [
            "twitter",
            "lunr"
        ]
    },
    "https://twitter.com/tsitsulin_/status/1490407896823250956": {
        "extra-tags": [],
        "date": "2022-02-06",
        "title": "Twitter @tsitsulin_",
        "summary": "Only in Moscow there are billboard ads for Python packages.\n\nBonus: Moscow State\u2019s main building in the background. https://t.co/goqTYbGhuM",
        "tags": [
            "twitter",
            "python",
            "moscow",
            "moscow state"
        ]
    },
    "https://twitter.com/fishnets88/status/1489194323778416646": {
        "extra-tags": [],
        "date": "2022-02-03",
        "title": "Twitter @fishnets88",
        "summary": "Huge disk-saver. Instead of:\n\ngit clone\n\nYou can run:\n\ngit clone --depth &lt;depth&gt; -b &lt;branch&gt; &lt;repo_url&gt;\n\nThis way, you don't download the entire history but only the recent changes. TIL: https://t.co/M0XxGE2mUp",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/pydatapdx/status/1488931722343239680": {
        "extra-tags": [],
        "date": "2022-02-02",
        "title": "Twitter @pydatapdx",
        "summary": "We're a week out from our Feb meetup - we're welcoming @halford_max to discuss the River streaming ML project RSVP at meetup (https://t.co/HZteuvfAXB)",
        "tags": [
            "twitter",
            "rsvp"
        ]
    },
    "https://twitter.com/fishnets88/status/1488777965404139521": {
        "extra-tags": [],
        "date": "2022-02-02",
        "title": "Twitter @fishnets88",
        "summary": "It's getting more and more likely that I'll reimplement the calmcode search with https://t.co/EsDKZ2gubX. It's refreshingly minimal, lightweight and seems easy to iterate/ AB-test with. Shoutout to  @yera_ee\n\nhttps://t.co/15ZzOtV9Xk",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/IRIToulouse/status/1488532768728436744": {
        "extra-tags": [],
        "date": "2022-02-01",
        "title": "Twitter @IRIToulouse",
        "summary": "\u23e9 La troisi\u00e8me vid\u00e9o YouTube de notre s\u00e9rie d'interviews est en ligne ! \n\n\ud83c\udf1f Andreas HERZIG et Mathieu SERRURIER, chercheurs au sein du d\u00e9partement #IA, nous \u00e9clairent sur les enjeux de l'#IntelligenceArtificielle. \n\n#AI #DeepLearning @ANITI_Toulouse \n\nhttps://t.co/2Z2FoJIcpH",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/Nils_Reimers/status/1487014203483377664": {
        "extra-tags": [],
        "date": "2022-01-28",
        "title": "Twitter @Nils_Reimers",
        "summary": "Next, I tested the text-search models. Here the results look well for a dense model.\n\nHowever, when compared to the state-of-the-art sparse model of SpladeV2, which is 2600x smaller, you just get an 0.1 improvement. \n\n\ud83d\udcb0 Encoding costs? $1,000,000 for GPT-3 vs. $3 for SpladeV2 https://t.co/8vWerhJfB9",
        "tags": [
            "spladev2",
            "twitter",
            "gpt-3"
        ]
    },
    "https://twitter.com/Nils_Reimers/status/1487014213327499264": {
        "extra-tags": [],
        "date": "2022-01-28",
        "title": "Twitter @Nils_Reimers",
        "summary": "My advice: \n\ud83d\udcb0 Safe the $1,000,000 you would need to spend to encode your corpus with GPT-3\n\ud83d\udcc4 Spent $1000 and annotate task specific data\n\ud83c\udd93Fine-tune an open model\n\ud83c\udf89 Use the $999,000 saving to treat your team",
        "tags": [
            "twitter",
            "gpt-3"
        ]
    },
    "https://twitter.com/Nils_Reimers/status/1487014207589605379": {
        "extra-tags": [],
        "date": "2022-01-28",
        "title": "Twitter @Nils_Reimers",
        "summary": "When evaluated on 6 (query/questions, paragraph)-tasks, the OpenAI 2.7B &amp; 6.7B parameter models perform on par with an open 110M parameter model (MPNet). Again, encoding costs are about 1000 higher. https://t.co/kE6X7SRsHg",
        "tags": [
            "twitter",
            "openai",
            "mpnet"
        ]
    },
    "https://twitter.com/Nils_Reimers/status/1487014199637200899": {
        "extra-tags": [],
        "date": "2022-01-28",
        "title": "Twitter @Nils_Reimers",
        "summary": "I tested the text similarity models on 14 datasets from different domains (emails, papers, online communities) on various tasks (clustering, retrieval, paraphrase mining).\n\nThe 175B model is actually worse than a tiny MiniLM 22M parameter model that can run in your browser. https://t.co/YwtvOs5Ihb",
        "tags": [
            "twitter",
            "minilm"
        ]
    },
    "https://twitter.com/ylecun/status/1487075662548643842": {
        "extra-tags": [],
        "date": "2022-01-28",
        "title": "Twitter @ylecun",
        "summary": "An extensive comparison of OpenAI's GPT-3 embeddings with a number of (smaller) freely-available open models on a number of NLP benchmarks.\nBottom line: don't be hypnotized by hype and size.\n\nBlog post: https://t.co/RSiqJIJY6G https://t.co/XcGv1LUU0p",
        "tags": [
            "nlp",
            "twitter",
            "openai",
            "gpt-3"
        ]
    },
    "https://twitter.com/halford_max/status/1486483112146178048": {
        "extra-tags": [],
        "date": "2022-01-26",
        "title": "Twitter @halford_max",
        "summary": "@DiawAhad I first picked \u00ab\u00a0creme\u00a0\u00bb as in \u00ab\u00a0incremental\u00a0\u00bb. Then when we merged the new team agreed on \u00ab\u00a0river\u00a0\u00bb because it conveys the idea of flowing data :)",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/DiawAhad/status/1486333331419639808": {
        "extra-tags": [],
        "date": "2022-01-26",
        "title": "Twitter @DiawAhad",
        "summary": "https://t.co/ELasjJVFx6",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/DiawAhad/status/1486323397105754118": {
        "extra-tags": [
            "tweet"
        ],
        "date": "2022-01-26",
        "title": "Twitter @DiawAhad",
        "summary": "@raphaelsrty Yes I know \ud83e\udd14 It\u2019s my next tweet.\nI want to share this, just to retrace the process.\nOops!",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/DiawAhad/status/1486333257759211526": {
        "extra-tags": [],
        "date": "2022-01-26",
        "title": "Twitter @DiawAhad",
        "summary": "River is a Python library for online machine learning. It is the result of a merger between Creme and Scikit-multiflow.\n\nAbout the choice of the names Creme and River, I let two of the authors explain them to us \ud83d\ude09.\n\n@AdilZtn @raphaelsrty \n\n#River #programming #coding #MLOps https://t.co/HGHNFPuqxm",
        "tags": [
            "river",
            "multiflow",
            "creme",
            "twitter",
            "python",
            "scikit",
            "creme and river",
            "mlops"
        ]
    },
    "https://twitter.com/DiawAhad/status/1486254905803395074": {
        "extra-tags": [],
        "date": "2022-01-26",
        "title": "Twitter @DiawAhad",
        "summary": "creme is a Python library for online machine learning. All the tools in the library can be updated with a single observation at a time, and can therefore be used to learn from streaming data.\n\n#coding #learning #programming #MLOps #programmer #Python \n\nhttps://t.co/0LCxRqZQCL https://t.co/s8lEO3o0ip",
        "tags": [
            "twitter",
            "python",
            "mlops"
        ]
    },
    "https://twitter.com/JinaAI_/status/1484464808963411977": {
        "extra-tags": [],
        "date": "2022-01-21",
        "title": "Twitter @JinaAI_",
        "summary": "\ud83d\udc40Seeing is believing. \nHere's how you can build your own #Embedding Projector: https://t.co/qhNsIUsiYf\n\n#neuralnetworks #opensource #neuralsearch #deeplearning #machinelearning #visualization https://t.co/aOF0ZurBww",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/gcabanac/status/1484182214736711690": {
        "extra-tags": [],
        "date": "2022-01-20",
        "title": "Twitter @gcabanac",
        "summary": "\u00ab Problematic Paper Screener : d\u00e9tection d'expressions tortur\u00e9es r\u00e9v\u00e9latrices d'articles frauduleux \u00bb interview diffus\u00e9e dans le 19/20 de @F3Occitanie du 07/02/2022. Une recherche @UT3PaulSabatier @IRIToulouse. Compl\u00e9ments : https://t.co/OkFHIfnMgL et https://t.co/LvROSUfpoJ https://t.co/nAtBvpP6Yd",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/deepset_ai/status/1484201532644024321": {
        "extra-tags": [],
        "date": "2022-01-20",
        "title": "Twitter @deepset_ai",
        "summary": "Haystack v1.1.0 is out! Big thanks to all contributors (11 new!) and the whole community! \u2764\ufe0f Highlights: Advanced extraction of text&amp;tables from PDFs, model distillation to speed up QA \ud83d\ude80, pipeline evaluation to identify bottlenecks \ud83d\udd75\ufe0f, etc. Release notes: https://t.co/2lrmoWHzAK",
        "tags": [
            "twitter",
            "pdfs",
            "haystack"
        ]
    },
    "https://twitter.com/a_erdem4/status/1483379758331269123": {
        "extra-tags": [],
        "date": "2022-01-18",
        "title": "Twitter @a_erdem4",
        "summary": "Two common model evaluation mistakes that I observe on Kaggle:\nSome Kagglers use early stopping and calculate their CV with it. It makes the CV score optimistic, especially if there is significant variance between the epochs.",
        "tags": [
            "twitter",
            "kaggle"
        ]
    },
    "https://twitter.com/AkariAsai/status/1471326208126193665": {
        "extra-tags": [],
        "date": "2021-12-16",
        "title": "Twitter @AkariAsai",
        "summary": "A powerful retriever+pre-trained generator (eg. DPR+T5) often relies on spurious cues / generates hallucinations. \nOur \ud835\udd56\ud835\udd67\ud835\udd5a\ud835\udd55\ud835\udd56\ud835\udd5f\ud835\udd65\ud835\udd5a\ud835\udd52\ud835\udd5d\ud835\udd5a\ud835\udd65\ud835\udd6a-guided generator learns to focus and generate on the right passages and shows large improvements in QA/fact verification/dialogue\ud83d\udc47 https://t.co/RqNbC8ftfi",
        "tags": [
            "twitter",
            "dpr"
        ]
    },
    "https://twitter.com/AkariAsai/status/1483183272255377412": {
        "extra-tags": [],
        "date": "2022-01-17",
        "title": "Twitter @AkariAsai",
        "summary": "Our BPR (ACL 2021, code is publicly available at Github) substantially reduces the index size (e.g., DPR consumes 65 GB while BPR only uses 2GB) without accuracy drop, and enables us to scale up to *billions of article* \ud83d\udcda\nGreat blog post on building a search engine using BPR! https://t.co/J0V3F8cRsB",
        "tags": [
            "github",
            "twitter",
            "acl",
            "bpr"
        ]
    },
    "https://twitter.com/MathisHammel/status/1483109523325861893": {
        "extra-tags": [],
        "date": "2022-01-17",
        "title": "Twitter @MathisHammel",
        "summary": "Dans une \u00e9tude de l'universit\u00e9 Carnegie-Mellon, il a \u00e9t\u00e9 estim\u00e9 que 87% des am\u00e9ricains sont identifiables de mani\u00e8re unique \u00e0 partir du trio code postal + genre + date de naissance.\n\nEt devinez quelles donn\u00e9es sont collect\u00e9es par l'appli Elyze qui affirme que c'est anonyme ? \ud83d\ude07 https://t.co/m2KxjX1QGH",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/JFPuget/status/1482779091698814976": {
        "extra-tags": [],
        "date": "2022-01-16",
        "title": "Twitter @JFPuget",
        "summary": "Can't agree more.  Model validation done wrong and machine learning becomes dangerous.\n\nI am sure we can always learn how to do it better, but kaggle competitions is the best way to get experience quickly in model validation IMHO. https://t.co/SzHGpSOsGw",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/tomgoldsteincs/status/1482058897489686528": {
        "extra-tags": [],
        "date": "2022-01-14",
        "title": "Twitter @tomgoldsteincs",
        "summary": "Language models have their own version of the Y2K bug.  The GPT-2 tokenizer has a separate word for every year from 1959-2020, but there's no word for 2021 or 2022.   BERT can individually tokenize every year from 1707-2021, but not 1706 or 2022.",
        "tags": [
            "y2k",
            "twitter",
            "gpt-2 tokenizer",
            "bert"
        ]
    },
    "https://twitter.com/_lewtun/status/1480998753578409984": {
        "extra-tags": [],
        "date": "2022-01-11",
        "title": "Twitter @_lewtun",
        "summary": "Lately I've been exporting \ud83e\udd17 Transformers models to ONNX and the API that @MorganFunto designed is really nice!\n\nHere's an example to export DistilBERT to ONNX and then run inference with @onnxruntime -- as you can see it's just one line of code to export a supported model \ud83e\udd2f https://t.co/TZPn65lPxC",
        "tags": [
            "twitter",
            "onnx",
            "api",
            "transformers",
            "distilbert"
        ]
    },
    "https://twitter.com/_lewtun/status/1480998758091571203": {
        "extra-tags": [],
        "date": "2022-01-11",
        "title": "Twitter @_lewtun",
        "summary": "Each supported model comes with a set of *features* that enable you to export models for different types of topologies or tasks. \n\nAs shown in the table, each feature is associated with a different \"auto class\" in \ud83e\udd17 Transformers https://t.co/Ak6ElFYioA",
        "tags": [
            "twitter",
            "transformers"
        ]
    },
    "https://twitter.com/tbsmartens/status/1480610122032467972": {
        "extra-tags": [],
        "date": "2022-01-10",
        "title": "Twitter @tbsmartens",
        "summary": "Cherche https://t.co/h1GW6kJNwt",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/hn_frontpage/status/1480517134434328582": {
        "extra-tags": [],
        "date": "2022-01-10",
        "title": "Twitter @hn_frontpage",
        "summary": "Neural Search for medium sized corpora\nL: https://t.co/D3o6eGqisT\nC: https://t.co/wsFInhcorD",
        "tags": [
            "twitter",
            "neural"
        ]
    },
    "https://twitter.com/halford_max/status/1479641965415174145": {
        "extra-tags": [],
        "date": "2022-01-08",
        "title": "Twitter @halford_max",
        "summary": "I made a Python package to measure string edit distances based on keyboard layouts -&gt; https://t.co/sQZd33oP4Z. It should be useful for spelling correction as well as analyzing keystroke dynamics \ud83e\udd13",
        "tags": [
            "twitter",
            "python"
        ]
    },
    "https://twitter.com/halford_max/status/1479239617576521730": {
        "extra-tags": [],
        "date": "2022-01-06",
        "title": "Twitter @halford_max",
        "summary": "I wrote a thought piece on how online machine learning inverts the fit/predict paradigm. It's related to the \"log and wait\" approach Faire has advocated in https://t.co/YxDMfLYSue. Here it is https://t.co/uixl5R4YA2. Enjoy!",
        "tags": [
            "faire",
            "twitter"
        ]
    },
    "https://twitter.com/RichardSocher/status/1478920617068044298": {
        "extra-tags": [
            "apps"
        ],
        "date": "2022-01-06",
        "title": "Twitter @RichardSocher",
        "summary": "Big Release today! Dark mode! Region settings for the world \ud83c\uddfa\ud83c\uddf8\ud83c\udde9\ud83c\uddea\ud83c\udde7\ud83c\uddf7\ud83c\uddee\ud83c\uddf3\ud83c\udf0d\ud83c\udf0f\ud83c\udf0e. Much improved speed. How to apps and much more :) https://t.co/qlvDj9WNS2",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/UT3PaulSabatier/status/1478661267112013827": {
        "extra-tags": [],
        "date": "2022-01-05",
        "title": "Twitter @UT3PaulSabatier",
        "summary": "Parmi les 10 personnalit\u00e9s qui ont marqu\u00e9 la science en 2021, nous avons interview\u00e9 Guillaume Cabanac, enseignant-chercheur \u00e0 l\u2019universit\u00e9, surnomm\u00e9 \u00ab d\u00e9tective de la tromperie \u00bb par @Nature il d\u00e9busque les fausses publications scientifiques #Nature10\n\nhttps://t.co/hWXJSAIKTI https://t.co/dcYsQ2xTMT",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/UT3PaulSabatier/status/1478392956763660289": {
        "extra-tags": [
            "science"
        ],
        "date": "2022-01-04",
        "title": "Twitter @UT3PaulSabatier",
        "summary": "#RevuedePresse Un chercheur toulousain distingu\u00e9 par la revue @Nature pour ses travaux sur les fausses \u00e9tudes scientifiques \n\n@F3Occitanie sur Guillaume Cabanac parmi les 10 personnalit\u00e9s qui ont marqu\u00e9 la science en 2021 @gcabanac #Natures10\n@IRIToulouse\nhttps://t.co/jpEte0rGFX",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/DynamicWebPaige/status/1477011811341942788": {
        "extra-tags": [],
        "date": "2021-12-31",
        "title": "Twitter @DynamicWebPaige",
        "summary": "@github \"Well-maintained READMes, guides, and repos can result in as much as a 50% increase in productivity.\n\nMeanwhile, automation was found to contribute as much as 43% to improvements in productivity within proprietary dev environments and 27% in OSS projects.\"\nhttps://t.co/sGzkPLokKx",
        "tags": [
            "twitter",
            "oss"
        ]
    },
    "https://twitter.com/n_kozodoi/status/1476866158770987051": {
        "extra-tags": [],
        "date": "2021-12-31",
        "title": "Twitter @n_kozodoi",
        "summary": "What\u2019s your plan for 2022? https://t.co/M8pHziv7ca",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/GaganGulyani/status/1476425581738614785": {
        "extra-tags": [],
        "date": "2021-12-30",
        "title": "Twitter @GaganGulyani",
        "summary": "@digitalocean I can do it in one line of code.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/newsycombinator/status/1475618703643484160": {
        "extra-tags": [],
        "date": "2021-12-28",
        "title": "Twitter @newsycombinator",
        "summary": "I made my first web0 website today. It's so cool it just works https://t.co/AE9mb8E859",
        "tags": [
            "twitter",
            "web0"
        ]
    },
    "https://twitter.com/davewongillies/status/1474871224652025857": {
        "extra-tags": [],
        "date": "2021-12-25",
        "title": "Twitter @davewongillies",
        "summary": "@mgdm Never forget https://t.co/jRADZwcVaC",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1473205353575862274": {
        "extra-tags": [],
        "date": "2021-12-21",
        "title": "Twitter @fishnets88",
        "summary": "TIL: Github Copilot can generate SQL injections and a bunch of other vulnerable code. \n\nhttps://t.co/tTS9w9KItd https://t.co/eFa4sIaKhE",
        "tags": [
            "sql",
            "twitter",
            "github copilot"
        ]
    },
    "https://twitter.com/hyperfp/status/1473241186232066058": {
        "extra-tags": [],
        "date": "2021-12-21",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty https://t.co/MIWsMI2dfF",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fchollet/status/1472264178782158850": {
        "extra-tags": [],
        "date": "2021-12-18",
        "title": "Twitter @fchollet",
        "summary": "I've been working on a new research project these past few days. I have to stay: it's far more enjoyable to use Keras to implement complex, highly unusual ideas than to do standard workflows. Implementing basic stuff is boring, but implementing unusual stuff feels fun &amp; elegant.",
        "tags": [
            "twitter",
            "keras"
        ]
    },
    "https://twitter.com/ringo_ring/status/1470815566160179201": {
        "extra-tags": [],
        "date": "2021-12-14",
        "title": "Twitter @ringo_ring",
        "summary": "nature has actually contacted me for comment about accusations that Sci-Hub is a threat, here is my full response / it is clear that academic publishers care about their money, not about security of other people https://t.co/f3LvOK46lf",
        "tags": [
            "sci-hub",
            "twitter"
        ]
    },
    "https://twitter.com/johnhewtt/status/1469431303002935298": {
        "extra-tags": [],
        "date": "2021-12-10",
        "title": "Twitter @johnhewtt",
        "summary": "Ever added new words to the vocabulary of your language model only to generate from it and have it generate gibberish?\n\nIn a technical blog post I detail why this happens, and that representing new words as an average of existing words solves the problem.\n\nhttps://t.co/EFwc66xdtY https://t.co/UhgyTr4xT7",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/esalesk/status/1469560797848383490": {
        "extra-tags": [],
        "date": "2021-12-11",
        "title": "Twitter @esalesk",
        "summary": "@johnhewtt Awesome and clear blog post!\nAveraging from existing vocabulary is  also something we tried when expanding subword vocabularies during MT training, though for us, using component subwords rather than the full vocabulary, which works surprisingly well!\n\nhttps://t.co/FYsNfCsMlG",
        "tags": [
            "twitter",
            "mt"
        ]
    },
    "https://twitter.com/timvink/status/1469661550378201088": {
        "extra-tags": [],
        "date": "2021-12-11",
        "title": "Twitter @timvink",
        "summary": "Released a new MkDocs plugin to make it easier to work with charts \ud83c\udf89 https://t.co/NfAEYd0kEv #vegalite #mkdocs https://t.co/vfH6tlYz6k",
        "tags": [
            "twitter",
            "mkdocs"
        ]
    },
    "https://twitter.com/fperez_org/status/1469197319253815299": {
        "extra-tags": [],
        "date": "2021-12-10",
        "title": "Twitter @fperez_org",
        "summary": "20y ago today, as a failing physics grad student I posted @IPythonDev 0.2.0. This opened the door to an incredible community: @ellisonbg, @minrk, @Mbussonn, the late John Hunter... who made @ProjectJupyter &amp; scientific python possible.\n\nToo many to name..\n\nhttps://t.co/e2L4DesP4L https://t.co/m8BhQ1PEJA",
        "tags": [
            "john hunter",
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1468143323802218500": {
        "extra-tags": [],
        "date": "2021-12-07",
        "title": "Twitter @fishnets88",
        "summary": "TIL: linkrot is a pretty big problem.\n\nhttps://t.co/xVw35doM4v",
        "tags": [
            "twitter",
            "linkrot",
            "til"
        ]
    },
    "https://twitter.com/DynamicWebPaige/status/1467372583213416453": {
        "extra-tags": [],
        "date": "2021-12-05",
        "title": "Twitter @DynamicWebPaige",
        "summary": "Am loving the simplicity of these static timeline plots, generated with Python from just a blob of JSON:\n\nhttps://t.co/wZO2V7sJuM\n\nDo you have any favorite libraries for similar plots (with the caveat that I'd like to have something just as simple)? https://t.co/2aVuAHqFhA",
        "tags": [
            "twitter",
            "python",
            "json"
        ]
    },
    "https://twitter.com/hyperfp/status/1466111277277798402": {
        "extra-tags": [],
        "date": "2021-12-01",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty https://t.co/WDkeKGQlxD",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/agarwl_/status/1432800830621687817": {
        "extra-tags": [],
        "date": "2021-08-31",
        "title": "Twitter @agarwl_",
        "summary": "tl;dr: Our findings call for a change in how we evaluate performance on deep RL benchmarks, for which we present more reliable protocols, easily applicable with *even a handful of runs*, to prevent unreliable results from stagnating the field.\n\nhttps://t.co/qMyDEiNqR6 (1/N) https://t.co/ShXAGNRhBM",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/AdilZtn/status/1464269788792954880": {
        "extra-tags": [],
        "date": "2021-11-26",
        "title": "Twitter @AdilZtn",
        "summary": "@ak92501 @raphaelsrty",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/idangazit/status/1463259046849355783": {
        "extra-tags": [],
        "date": "2021-11-23",
        "title": "Twitter @idangazit",
        "summary": "\"I just want to buy a fucking GPU, year two\". Fuck you, crypto, NFTs, and everything blockchain. https://t.co/wkwx7Q4tCO",
        "tags": [
            "crypto",
            "twitter",
            "nfts",
            "gpu"
        ]
    },
    "https://twitter.com/jamonholmgren/status/1462955101899788294": {
        "extra-tags": [],
        "date": "2021-11-23",
        "title": "Twitter @jamonholmgren",
        "summary": "Bots being passive-aggressive with each other. https://t.co/mcasQMrT66",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fchollet/status/1463043626489974785": {
        "extra-tags": [],
        "date": "2021-11-23",
        "title": "Twitter @fchollet",
        "summary": "Overfitting comes from the fact that your test data differs in subtle (and not so subtle) ways from your training data. The more the test data deviates, the earlier overfitting occurs during training, and the more severe it becomes as time passes.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/DjevaLoperka/status/1462748830735208448": {
        "extra-tags": [
            "authentication"
        ],
        "date": "2021-11-22",
        "title": "Twitter @DjevaLoperka",
        "summary": "Who needs two-factor authentication? \ud83e\udd37 https://t.co/THheS5eOev",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/JinaAI_/status/1462748749726375937": {
        "extra-tags": [
            "us"
        ],
        "date": "2021-11-22",
        "title": "Twitter @JinaAI_",
        "summary": "\ud83d\udcb0$30M Series A, nothing standard. But we never say we are standard ones. \ud83d\ude80 Work in #opensource and #neuralsearch? Join us and submit your applications today! https://t.co/3JIJjzTSsC Hiring in \ud83c\udde9\ud83c\uddea\ud83c\uddea\ud83c\uddfa\ud83c\uddfa\ud83c\uddf8\n\nhttps://t.co/LZWVMqfzwQ",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/rlmcelreath/status/1462719976259919879": {
        "extra-tags": [],
        "date": "2021-11-22",
        "title": "Twitter @rlmcelreath",
        "summary": "It has been 951 days since Bill Gates gifted every stats teacher with this finely distilled tweet. It's so good, because Gates is not dumb. There is nothing dumb about not understanding conditional probability. It's only human. https://t.co/RaJB7WtvUi",
        "tags": [
            "gates",
            "bill gates",
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1461401471665254408": {
        "extra-tags": [],
        "date": "2021-11-18",
        "title": "Twitter @fishnets88",
        "summary": "It's true, there's a lot to look forward to! https://t.co/DndF8nf77P",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1461250092648837124": {
        "extra-tags": [],
        "date": "2021-11-18",
        "title": "Twitter @fishnets88",
        "summary": "TIL: you can add `--color=yes` to a pytest call in GitHub Actions and it'll show you the color again. https://t.co/se7dZ6JDVH",
        "tags": [
            "github",
            "twitter"
        ]
    },
    "https://twitter.com/spacy_io/status/1457996044806012929": {
        "extra-tags": [],
        "date": "2021-11-09",
        "title": "Twitter @spacy_io",
        "summary": "NEW BLOG POST: Introducing spaCy v3.2!\n\nSince v3.1 we\u2019ve added:\n\ud83e\udd84 Usability improvements for custom training &amp; scoring\n\ud83d\udd25 Improved performance on Apple M1 &amp; Nvidia GPUs\n\ud83c\udf38 Support for floret vectors: Finnish and Korean example projects with benchmarks!\n\nhttps://t.co/G2SfyWpCdd",
        "tags": [
            "apple m1",
            "twitter",
            "nvidia gpus",
            "spacy"
        ]
    },
    "https://twitter.com/svpino/status/1457453235397070850": {
        "extra-tags": [],
        "date": "2021-11-07",
        "title": "Twitter @svpino",
        "summary": "Quick Sort implementation in a single line of code using Python.\n\n(This is just for fun. Don't do this.) https://t.co/24ZJTkIx3g",
        "tags": [
            "twitter",
            "python"
        ]
    },
    "https://twitter.com/mervenoyann/status/1457412906681872390": {
        "extra-tags": [],
        "date": "2021-11-07",
        "title": "Twitter @mervenoyann",
        "summary": "spoiler of what I\u2019m going to release for this semester (+deep learning cheatsheets) https://t.co/drzPSigFxZ",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/nathanbenaich/status/1457286166588297218": {
        "extra-tags": [],
        "date": "2021-11-07",
        "title": "Twitter @nathanbenaich",
        "summary": "I take no credit for this genius illustration. https://t.co/rh6qeVyOFW",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/CedricCuaz/status/1456935608060874753": {
        "extra-tags": [],
        "date": "2021-11-06",
        "title": "Twitter @CedricCuaz",
        "summary": "Fanboy mode (ON) https://t.co/9EHWOuLWb5",
        "tags": [
            "fanboy",
            "twitter"
        ]
    },
    "https://twitter.com/NielsRogge/status/1456659187149283328": {
        "extra-tags": [],
        "date": "2021-11-05",
        "title": "Twitter @NielsRogge",
        "summary": "Beautiful work by @mishig25, showcasing DETR for instance segmentation of images https://t.co/hT8TqJyhd7",
        "tags": [
            "detr",
            "twitter",
            "beautiful work"
        ]
    },
    "https://twitter.com/RomanYurchak/status/1456679049603002373": {
        "extra-tags": [],
        "date": "2021-11-05",
        "title": "Twitter @RomanYurchak",
        "summary": "Using open-source packages is now so commonplace, it's sometimes difficult to keep in mind how much of the progress builds on prior work.\n\nThread \ud83d\udc47",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/timothy_lkh_/status/1454667547513483265": {
        "extra-tags": [],
        "date": "2021-10-31",
        "title": "Twitter @timothy_lkh_",
        "summary": "Update: somehow if you don't start from a clean environment, you only get ~50% of the speed-up. SpaCy performance on M1 is bonkers now! (benchmark code and setup: https://t.co/SHOYfA3tW7) https://t.co/GWtZjiuZQo https://t.co/GSrQwnrtsK",
        "tags": [
            "twitter",
            "spacy",
            "m1"
        ]
    },
    "https://twitter.com/Gillesvdwiele/status/1454109978822856711": {
        "extra-tags": [
            "tweet"
        ],
        "date": "2021-10-29",
        "title": "Twitter @Gillesvdwiele",
        "summary": "Please upvote this tweet... https://t.co/8bwV2r0sYk",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1453274202837065742": {
        "extra-tags": [],
        "date": "2021-10-27",
        "title": "Twitter @fishnets88",
        "summary": "I have recently been accused of writing the shortest Jupyter plugin ever and ... they're not wrong. \n\nIn case you didn't know, you can use `pip install drawdata` to draw datasets in Jupyter which is great for teaching. \n\nhttps://t.co/yjQ08uaQh7 https://t.co/qFwAcT2F7l",
        "tags": [
            "twitter",
            "jupyter"
        ]
    },
    "https://twitter.com/jamescalam/status/1452997929631944719": {
        "extra-tags": [],
        "date": "2021-10-26",
        "title": "Twitter @jamescalam",
        "summary": "Latest article with @Pinecone_io on the latest and greatest in fine-tuning sentence transformer models \u2014 super easy with @Nils_Reimers sentence-transformers library! \ud83d\ude04\n\nhttps://t.co/OBx6WQoBWX\n\n#MachineLearning #DataScience #NLProc #learntocode #100DaysOfCode #100DaysOfMLCode https://t.co/feKrAhcnl9",
        "tags": [
            "nlproc",
            "twitter"
        ]
    },
    "https://twitter.com/hyperfp/status/1453151198849159171": {
        "extra-tags": [],
        "date": "2021-10-27",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty: \"sentence embeddings: fine-tune your models with MultipleNegativesRanking loss, and do it with the sentence-transformers library\" https://t.co/TuiQObayKC",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fchollet/status/1452665815812087812": {
        "extra-tags": [],
        "date": "2021-10-25",
        "title": "Twitter @fchollet",
        "summary": "Announcement: my book Deep Learning with Python (2nd edition) has been released.\n\n500 pages of code examples, theory, context, practical tips... If you want to really understand how deep learning works, why it matters, and how to use it, this is your book!\nhttps://t.co/LvbEy5A0k8",
        "tags": [
            "deep learning",
            "twitter",
            "python"
        ]
    },
    "https://twitter.com/pybites/status/1451246209151217673": {
        "extra-tags": [],
        "date": "2021-10-21",
        "title": "Twitter @pybites",
        "summary": "Another #Python Standard Library gem: the operator module. Its itemgetter class lets you grab multiple items from its operand: https://t.co/S7dxU2ICgh",
        "tags": [
            "twitter",
            "python standard library"
        ]
    },
    "https://twitter.com/PyTorch/status/1451229118486368256": {
        "extra-tags": [],
        "date": "2021-10-21",
        "title": "Twitter @PyTorch",
        "summary": "PyTorch 1.10 is here! \n\nHighlights include updates for:\n- CUDA Graphs APIs updates\n- Several frontend APIs moved to Stable\n- Automatic fusion in JIT Compiler support for CPU/GPUs\n-  Android NNAPI now in beta\n\nBlog: https://t.co/jMRDpA6T22\nRelease: https://t.co/QuaxJA2xfh",
        "tags": [
            "cuda graphs",
            "twitter",
            "nnapi",
            "pytorch",
            "jit",
            "android"
        ]
    },
    "https://twitter.com/fishnets88/status/1451119479287648260": {
        "extra-tags": [],
        "date": "2021-10-21",
        "title": "Twitter @fishnets88",
        "summary": "I'd love to dabble more in Rust, but it's indeed stuff like *this* that makes it intimidating if you've mainly been doing python/R/js all these years. https://t.co/q5tcbINxdk",
        "tags": [
            "twitter",
            "rust"
        ]
    },
    "https://twitter.com/svlevine/status/1450516139784564737": {
        "extra-tags": [],
        "date": "2021-10-19",
        "title": "Twitter @svlevine",
        "summary": "To make an existing model more robust at test time: augment a single test image in many ways, finetune model so that predictions on augmented images \"agree\", minimizing marginal entropy. This is the idea behind MEMO (w/ Marvin Zhang &amp; @chelseabfinn): https://t.co/UjO6oJIXs8\n\n\ud83e\uddf5&gt; https://t.co/2kPfZxoG8d",
        "tags": [
            "memo",
            "twitter",
            "marvin zhang"
        ]
    },
    "https://twitter.com/mervenoyann/status/1450539050008522766": {
        "extra-tags": [],
        "date": "2021-10-19",
        "title": "Twitter @mervenoyann",
        "summary": "but working with a group of smartest people in ML doesn't relieve it, it inspired me to become even better in my job, I did question myself a lot, but in the end I thought that everyone has something different to offer and that's what matters.",
        "tags": [
            "twitter",
            "ml"
        ]
    },
    "https://twitter.com/mervenoyann/status/1450536592918077448": {
        "extra-tags": [],
        "date": "2021-10-19",
        "title": "Twitter @mervenoyann",
        "summary": "I opened up about my imposter syndrome. I learnt to embrace it to fuel me to learn and improve myself, if you manage to not turn into an anxiety you can let it fuel you \ud83d\udca5 https://t.co/E6Ri3G3QFO",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/hyperfp/status/1449275362358267906": {
        "extra-tags": [],
        "date": "2021-10-16",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty https://t.co/3KbbZq2hrg",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/AnecdotesMaths/status/1448302977865879560": {
        "extra-tags": [],
        "date": "2021-10-13",
        "title": "Twitter @AnecdotesMaths",
        "summary": "C\u00e9dric Villani, avant d'\u00eatre d\u00e9put\u00e9, \u00e9tait un math\u00e9maticien mondialement reconnu. Il a m\u00eame re\u00e7u la m\u00e9daille Fields (plus haute distinction math\u00e9matique) en 2010. https://t.co/yRpEgVdUqG",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/aureliengeron/status/1447807415915716610": {
        "extra-tags": [],
        "date": "2021-10-12",
        "title": "Twitter @aureliengeron",
        "summary": "My favorite Python 3.5 feature: the matrix multiplication operator @\n\ud83d\udc47Python features thread\ud83d\udc47 https://t.co/NnKDgggkCf",
        "tags": [
            "twitter",
            "matrix",
            "python 3.5"
        ]
    },
    "https://twitter.com/hyperfp/status/1445903459379662849": {
        "extra-tags": [],
        "date": "2021-10-07",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty DPR and entities https://t.co/ioAeknHfnl",
        "tags": [
            "twitter",
            "dpr"
        ]
    },
    "https://twitter.com/hyperfp/status/1445904522618892289": {
        "extra-tags": [],
        "date": "2021-10-07",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty Retrieval-based NLP https://t.co/4gXXp6Xmj6",
        "tags": [
            "nlp",
            "twitter"
        ]
    },
    "https://twitter.com/le_science4all/status/1445118302573969422": {
        "extra-tags": [
            "r"
        ],
        "date": "2021-10-04",
        "title": "Twitter @le_science4all",
        "summary": "Des employ\u00e9s sont pay\u00e9s pour troller les entreprises concurrentes sur les r\u00e9seaux sociaux \ud83d\ude05\n\nBienvenue en 2021.\nhttps://t.co/n33FEslbBD https://t.co/1NAQqpiO9x",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/NathanBLawrence/status/1443223625105092622": {
        "extra-tags": [],
        "date": "2021-09-29",
        "title": "Twitter @NathanBLawrence",
        "summary": "I am a connoisseur of weird robot vacuum issues, because they all seem to get at least one thing slightly wrong, but this might be my favorite. \n\nI discover this happened almost every morning. https://t.co/cdXYTZX6dP",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/le_science4all/status/1443087216432058368": {
        "extra-tags": [],
        "date": "2021-09-29",
        "title": "Twitter @le_science4all",
        "summary": "Paper accepted at @NeurIPSConf #NeurIPS2021!! \ud83e\udd73\n\nOur paper proves lower and upper bounds on what any learning algorithm can guarantee in a realistic adversarial environment, with heterogeneous data providers, some of them being arbitrary malicious.\nhttps://t.co/ntTcKB52Qg https://t.co/iJ2YvCZDlB",
        "tags": [
            "twitter",
            "neurips2021"
        ]
    },
    "https://twitter.com/Liv_Boeree/status/1443242200003981325": {
        "extra-tags": [],
        "date": "2021-09-29",
        "title": "Twitter @Liv_Boeree",
        "summary": "okay Austin rocks. I\u2019m sold. https://t.co/S7EFlqyXxV",
        "tags": [
            "austin",
            "twitter"
        ]
    },
    "https://twitter.com/leejnhk/status/1441445536515584004": {
        "extra-tags": [],
        "date": "2021-09-24",
        "title": "Twitter @leejnhk",
        "summary": "Do we always need sentence vectors for sentence retrieval and passage vectors for passage retrieval?\ud83e\udd14 Our #EMNLP2021 paper suggests that phrase vectors can serve as a basic building block\ud83e\uddf1 for \"multi-granularity\" retrieval!\n\n\ud83d\udcdc https://t.co/G3PinnO3ng\n\ud83d\udcbe https://t.co/QkzeJ5wl8o https://t.co/RnEtmhKmwn",
        "tags": [
            "twitter",
            "emnlp2021"
        ]
    },
    "https://twitter.com/philipvollet/status/1440942856743571458": {
        "extra-tags": [],
        "date": "2021-09-23",
        "title": "Twitter @philipvollet",
        "summary": "River is a Python library for online machine learning. It is the result of a merger between creme and scikit-multiflow. River's ambition is to be the go-to library for doing machine learning on streaming data.\n\nhttps://t.co/5bF4QizOqB https://t.co/sDLSc6c5GX",
        "tags": [
            "river",
            "twitter",
            "python"
        ]
    },
    "https://twitter.com/deepset_ai/status/1438444854388854786": {
        "extra-tags": [],
        "date": "2021-09-16",
        "title": "Twitter @deepset_ai",
        "summary": "1/7 #Haystack 0.10.0 is out! This is an important release, containing significant improvements to many components, and some cutting-edge scalability features :) Super grateful to everyone who contributed! See the thread for highlights https://t.co/bwBzWKUmvd #nlproc #ml #ai",
        "tags": [
            "nlproc",
            "twitter",
            "haystack"
        ]
    },
    "https://twitter.com/PyTorch/status/1437838231505096708": {
        "extra-tags": [],
        "date": "2021-09-14",
        "title": "Twitter @PyTorch",
        "summary": "Want to make your inference code in PyTorch run faster? Here\u2019s a quick thread on doing exactly that. \n\n1. Replace https://t.co/OG6jlroK1O_grad() with the \u2728torch.inference_mode()\u2728 context manager. https://t.co/EbKarCvrGT",
        "tags": [
            "pytorch",
            "twitter"
        ]
    },
    "https://twitter.com/mervenoyann/status/1437543365998956546": {
        "extra-tags": [
            "good"
        ],
        "date": "2021-09-13",
        "title": "Twitter @mervenoyann",
        "summary": "I just started writing my thesis, wish me good luck \ud83c\udf40 \ud83e\udd17 https://t.co/uzx8Ukpgtc",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1437776132431044624": {
        "extra-tags": [],
        "date": "2021-09-14",
        "title": "Twitter @fishnets88",
        "summary": "@alanmnichol My predictions. \n\n2022: we'll provide tools to help find bad examples\n2023: we'll provide a proper ui to make it easier to define rules on your own data\n2024: (my hope) we'll work with simpler models that are easier to steer where we want them to be",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/alanmnichol/status/1437736272605454342": {
        "extra-tags": [],
        "date": "2021-09-14",
        "title": "Twitter @alanmnichol",
        "summary": "ML startups\n\n2016: we'll help you tune your hyperparameters!\n2021: we'll help you create a decent dataset",
        "tags": [
            "twitter",
            "ml startups"
        ]
    },
    "https://twitter.com/mervenoyann/status/1436345216693411840": {
        "extra-tags": [],
        "date": "2021-09-10",
        "title": "Twitter @mervenoyann",
        "summary": "\ud83e\udd41  I'll start working at @huggingface \ud83e\udd17 as a developer advocate next week to build things that'll help developers \ud83e\udd41",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/pgroth/status/1436326589080408065": {
        "extra-tags": [],
        "date": "2021-09-10",
        "title": "Twitter @pgroth",
        "summary": "Using transformers to improve knowledge graphs @danieldazac #transformersatwork https://t.co/2ClFati2ZQ",
        "tags": [
            "twitter",
            "transformersatwork"
        ]
    },
    "https://twitter.com/fishnets88/status/1435516990584762368": {
        "extra-tags": [],
        "date": "2021-09-08",
        "title": "Twitter @fishnets88",
        "summary": "Note, I've designed the course such that it should also be useful for non-Rasa developers. Sure, we focus on Rasa a bit. But if you'd like to generally learn more about Docker &amp; Kubernetes ... this material is meant to help you too! https://t.co/DlDkz77SX7",
        "tags": [
            "rasa",
            "twitter",
            "docker",
            "kubernetes"
        ]
    },
    "https://twitter.com/DocPepper_FR/status/1435137791248248833": {
        "extra-tags": [],
        "date": "2021-09-07",
        "title": "Twitter @DocPepper_FR",
        "summary": "L'autre jour j'ai vu un patient, le bras en echarpe. L'\u00e9paule douloureuse. Il me dit avoir fait la roue lors d'une soir\u00e9e et s'\u00eatre mal r\u00e9ceptionn\u00e9. Il avait super mal, malgr\u00e9 l'alcool. Le soir, enfin, le matin, il s'est couch\u00e9 et son pote lui a fait remarqu\u00e9 que y'a un...\n1/",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/philipvollet/status/1434759164576899074": {
        "extra-tags": [],
        "date": "2021-09-06",
        "title": "Twitter @philipvollet",
        "summary": "borb is a library for reading, creating and manipulating PDF files in python.\n\nhttps://t.co/PInU39UJVS\nhttps://t.co/Q25fh14ZIa https://t.co/zVfozyqDla",
        "tags": [
            "pdf",
            "twitter",
            "python",
            "borb"
        ]
    },
    "https://twitter.com/huggingface/status/1432717993637818383": {
        "extra-tags": [],
        "date": "2021-08-31",
        "title": "Twitter @huggingface",
        "summary": "Document parsing meets \ud83e\udd17 Transformers! \n\n\ud83d\udcc4#LayoutLMv2 and #LayoutXLM by @MSFTResearch are now available! \ud83d\udd25 \n\nThey're capable of parsing document images (like PDFs) by incorporating text, layout, and visual information, as in the @gradio demo below \u2b07\ufe0f\n\nhttps://t.co/Hr0kFIPHXW https://t.co/x8JqXKbEyg",
        "tags": [
            "layoutlmv2",
            "twitter",
            "transformers",
            "layoutxlm",
            "pdfs"
        ]
    },
    "https://twitter.com/github/status/1432433008703901697": {
        "extra-tags": [],
        "date": "2021-08-30",
        "title": "Twitter @github",
        "summary": "Build beautiful custom maps from OpenStreetMap data? Yes please, and thanks @marceloprates_!  \ud83d\uddfa https://t.co/lXDwijx37h https://t.co/meRwx56QDx",
        "tags": [
            "twitter",
            "openstreetmap"
        ]
    },
    "https://twitter.com/AdilZtn/status/1432583577963802625": {
        "extra-tags": [],
        "date": "2021-08-31",
        "title": "Twitter @AdilZtn",
        "summary": "@ak92501 @raphaelsrty",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fchollet/status/1432438589313282048": {
        "extra-tags": [
            "learning"
        ],
        "date": "2021-08-30",
        "title": "Twitter @fchollet",
        "summary": "My 4-month old and I are teaching things to each other, but he's learning much faster than I do.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/willmcgugan/status/1431609470212247552": {
        "extra-tags": [],
        "date": "2021-08-28",
        "title": "Twitter @willmcgugan",
        "summary": "The next version of Rich will have a print_json function which takes raw JSON and pretty prints it.\n\nThere isn't currently a straightforward way of doing this with Rich.\n\nThe output is guaranteed to be parsable JSON, so you could write the output to a .json file if you needed to. https://t.co/iCqZsizMSb",
        "tags": [
            "twitter",
            "json",
            "rich"
        ]
    },
    "https://twitter.com/algoritmic/status/1431534032039424000": {
        "extra-tags": [
            "music"
        ],
        "date": "2021-08-28",
        "title": "Twitter @algoritmic",
        "summary": "Composing Music With Complex Networks https://t.co/kUgNXZaT8W https://t.co/bsx4CtiMJJ",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1431529492640587777": {
        "extra-tags": [],
        "date": "2021-08-28",
        "title": "Twitter @fishnets88",
        "summary": "Another TIL! \n\nI found a paper about poke2vec, a text embedding trained on fan fiction to find descriptions of Pokemon. I didn't expect it ... but the paper had some genuinely proper lessons.\n\nhttps://t.co/ThsjjAacV7",
        "tags": [
            "twitter",
            "poke2vec",
            "pokemon"
        ]
    },
    "https://twitter.com/dlouapre/status/1431271490792706052": {
        "extra-tags": [],
        "date": "2021-08-27",
        "title": "Twitter @dlouapre",
        "summary": "Nouvelle vid\u00e9o ! COMMENT BIEN D\u00b7\u00c9CRIRE LES NOMBRES R\u00c9ELS ? C\u2019est bient\u00f4t la rentr\u00e9e, alors on se revigore les neurones, en se demandant comment approximer les nombres ayant une infinit\u00e9 de d\u00e9cimales. https://t.co/eD8dlG3QLm https://t.co/eaU9bhcJ1l",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/jobergum/status/1430609722139463688": {
        "extra-tags": [],
        "date": "2021-08-25",
        "title": "Twitter @jobergum",
        "summary": "@srchvrs Yes, it is https://t.co/nzT4QwBnFX. Available on Huggingface as well",
        "tags": [
            "twitter",
            "huggingface"
        ]
    },
    "https://twitter.com/fishnets88/status/1431183472278839299": {
        "extra-tags": [],
        "date": "2021-08-27",
        "title": "Twitter @fishnets88",
        "summary": "I decided to sponsor a few Github projects that I use this month. Which is why there's a fancy banner on the FastAPI docs. \n\nIt's a bit strange, since I'm mainly sponsoring @tiangolo for the Typer project, but I'm happy to support him  either way. \n\nhttps://t.co/1GNXerekaj",
        "tags": [
            "github",
            "twitter",
            "fastapi"
        ]
    },
    "https://twitter.com/fchollet/status/1430657433400070243": {
        "extra-tags": [
            "deep"
        ],
        "date": "2021-08-25",
        "title": "Twitter @fchollet",
        "summary": "Actually, the earth is flat (and it's only 128 blocks deep from sky to bedrock)",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/PatrickKidger/status/1430504339378999299": {
        "extra-tags": [],
        "date": "2021-08-25",
        "title": "Twitter @PatrickKidger",
        "summary": "Here's a fun picture I made today!\n\nA continuous normalising flow (https://t.co/UYowMmB0fH) continuously deforms one distribution into another distribution.\n\nThe lines show how particles from the base distribution are perturbed until they approximate the target distribution. https://t.co/g3fbYFgcxf",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/jhaushofer/status/1430437879424167938": {
        "extra-tags": [],
        "date": "2021-08-25",
        "title": "Twitter @jhaushofer",
        "summary": "Here's my setup for hybrid teaching on Zoom \u2013\u00a0I hope you find it useful! \nFull video here: https://t.co/OTVGtL5rjz https://t.co/5kiFN2rvPN",
        "tags": [
            "twitter",
            "zoom"
        ]
    },
    "https://twitter.com/Qofficiel/status/1430441880131604480": {
        "extra-tags": [],
        "date": "2021-08-25",
        "title": "Twitter @Qofficiel",
        "summary": "Bon, il faut rentrer maintenant, hein ! \ud83d\ude48\n\n\u23f0 Retrouvez #Quotidien, lundi 30 ao\u00fbt, \u00e0 partir de 18h25 sur @TMCtv ! https://t.co/dvZG2koiUh",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/andrew_n_carr/status/1429904944765685769": {
        "extra-tags": [],
        "date": "2021-08-23",
        "title": "Twitter @andrew_n_carr",
        "summary": "Python trick of the day, easily get the first and last element of a list https://t.co/fy9zAgucuf",
        "tags": [
            "twitter",
            "python"
        ]
    },
    "https://twitter.com/huggingface/status/1428339307278913545": {
        "extra-tags": [],
        "date": "2021-08-19",
        "title": "Twitter @huggingface",
        "summary": "20,000+ machine learning models connected to 3,000+ apps? Hugging Face meets Zapier! \ud83e\udd2f\ud83e\udd2f\ud83e\udd2f\n\nWith the Hugging Face API, you can now easily connect models right into apps like Gmail, Slack, Twitter, and more: https://t.co/rOQI1LL0zE\n\n[1/2] https://t.co/BbZVs5FarO",
        "tags": [
            "slack",
            "twitter",
            "zapier",
            "gmail"
        ]
    },
    "https://twitter.com/hyperfp/status/1425540085022347273": {
        "extra-tags": [],
        "date": "2021-08-11",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty https://t.co/8hWERX6DhM",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/JFPuget/status/1426169275698405377": {
        "extra-tags": [],
        "date": "2021-08-13",
        "title": "Twitter @JFPuget",
        "summary": "I love this kind of error message when using pytorch:\n\n ** On entry to GEMM_EX  parameter number 12 had an illegal value",
        "tags": [
            "pytorch",
            "twitter",
            "gemm_ex"
        ]
    },
    "https://twitter.com/shelley_rohar/status/1425489885671141376": {
        "extra-tags": [],
        "date": "2021-08-11",
        "title": "Twitter @shelley_rohar",
        "summary": "@GaloAndStuff You also definitely don't want to be installing the https://t.co/cBuf01CT3d chrome extension. After all, who needs a super handy #openaccess tool that allows you, with a click of a button, to access any closed article? https://t.co/MdprnmmtA8",
        "tags": [
            "twitter",
            "chrome"
        ]
    },
    "https://twitter.com/GaloAndStuff/status/1424818560417902604": {
        "extra-tags": [],
        "date": "2021-08-09",
        "title": "Twitter @GaloAndStuff",
        "summary": "Once again, I DO NOT recommend students go to libgen (dot) rs and download books for their upcoming courses. I am NOT advocating for getting and sharing free pdfs of required texts. DON'T DO IT.",
        "tags": [
            "twitter",
            "libgen"
        ]
    },
    "https://twitter.com/mervenoyann/status/1423962307428765699": {
        "extra-tags": [],
        "date": "2021-08-07",
        "title": "Twitter @mervenoyann",
        "summary": "nope https://t.co/L9eoC7aKB2",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1423900777341456391": {
        "extra-tags": [],
        "date": "2021-08-07",
        "title": "Twitter @fishnets88",
        "summary": "This was a *very* satisfying page to make. \n\nhttps://t.co/PIX2nPgbgc",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/rtroncy/status/1423369290028359684": {
        "extra-tags": [],
        "date": "2021-08-05",
        "title": "Twitter @rtroncy",
        "summary": "\u201cKnowledge Graphs in Natural Language Processing @ ACL 2021\u201d by Michael Galkin\nhttps://t.co/sIxcgOI6cf #acl",
        "tags": [
            "language processing",
            "twitter",
            "acl",
            "michael galkin"
        ]
    },
    "https://twitter.com/hyperfp/status/1423609381824913408": {
        "extra-tags": [],
        "date": "2021-08-06",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty https://t.co/4NmkDuwFdt",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fchollet/status/1423094224627535879": {
        "extra-tags": [],
        "date": "2021-08-05",
        "title": "Twitter @fchollet",
        "summary": "In the tech industry, you often encounter the idea that following best practices slows you down, that good hackers ship fast by all means necessary. IMO at any time scale longer than a weekend, *not* following best practices slows you down.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/JayAlammar/status/1422564662340726784": {
        "extra-tags": [],
        "date": "2021-08-03",
        "title": "Twitter @JayAlammar",
        "summary": "Ecstatic and honored that https://t.co/UjzqsejWc0 was published as an #ACL2021NLP demo paper!\n\nEcco: An Open Source Library for the Explainability of Transformer Language Models\nhttps://t.co/p35rWY1fyA\n\nv0.0.15 is out now!\nhttps://t.co/Gs1N3VYNH7 https://t.co/f5ML86oqnw",
        "tags": [
            "twitter",
            "acl2021nlp"
        ]
    },
    "https://twitter.com/osanseviero/status/1422655215967289351": {
        "extra-tags": [],
        "date": "2021-08-03",
        "title": "Twitter @osanseviero",
        "summary": "@JayAlammar Congratulations! This is very cool!\n\nI think this could make for a very nice demo with Hugging Face Spaces. \n\nhttps://t.co/2fu66MXmVf",
        "tags": [
            "hugging face spaces",
            "twitter"
        ]
    },
    "https://twitter.com/AnecdotesMaths/status/1422573585399439361": {
        "extra-tags": [],
        "date": "2021-08-03",
        "title": "Twitter @AnecdotesMaths",
        "summary": "Un nombre est divisible par 3 si la somme de ses chiffres est elle-m\u00eame divisible par 3.\n\nExemple: 126 est divisible par 3 car la somme de ses chiffres est 1 + 2 + 6 = 9, qui est divisible par 3.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/philipvollet/status/1422090602703998976": {
        "extra-tags": [],
        "date": "2021-08-02",
        "title": "Twitter @philipvollet",
        "summary": "Topic Modeling with Contextualized Embeddings. \n\nA new topic modeling family which supports many different languages\n\nhttps://t.co/bQr7JdhPgx https://t.co/PdP9Fi4GRy",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/JackTindale/status/1415677994237001732": {
        "extra-tags": [],
        "date": "2021-07-15",
        "title": "Twitter @JackTindale",
        "summary": "I keep finding new things to love about this. https://t.co/nIbjX1brEM",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/ikuyamada/status/1413170841160884236": {
        "extra-tags": [],
        "date": "2021-07-08",
        "title": "Twitter @ikuyamada",
        "summary": "I am very glad to know that BPR presented in our ACL 2021 paper works very well not only on QA tasks but also on the MS MARCO IR task!\ud83d\ude0d\nCode to train and evaluate BPR on MS MARCO is available at: https://t.co/9jnDV3RliP\nThanks for this amazing work, @Nthakur20 &amp; @Nils_Reimers! https://t.co/NNu9O9GpIB",
        "tags": [
            "acl",
            "ms marco",
            "twitter",
            "marco",
            "bpr"
        ]
    },
    "https://twitter.com/fchollet/status/1411114342574039040": {
        "extra-tags": [],
        "date": "2021-07-03",
        "title": "Twitter @fchollet",
        "summary": "The main reason to use a particular framework is because of how it shapes your code and how it shapes the development process you go through when producing that code. A good framework is one that results in readable, maintainable, concise code, and a painless development process.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fchollet/status/1410712068701835264": {
        "extra-tags": [],
        "date": "2021-07-01",
        "title": "Twitter @fchollet",
        "summary": "Whatever you end up doing, remember that problem-solving in general, and programming in particular, are supposed to be fun.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/joulee/status/1410469053827751939": {
        "extra-tags": [],
        "date": "2021-07-01",
        "title": "Twitter @joulee",
        "summary": "Someone on your team says: \u201cOur goal should be to move Metric X up Y% this half.\u201d\n\nYour inclination is to nod, say \u201cCool\u201d and get on with the actual building. \n\nBut pause! \n\nThe goals you agree to determine what you build. So consider them carefully.\n\n8 questions to ask\ud83d\udc47",
        "tags": [
            "twitter",
            "metric x"
        ]
    },
    "https://twitter.com/kritipraks/status/1409857129519869956": {
        "extra-tags": [],
        "date": "2021-06-29",
        "title": "Twitter @kritipraks",
        "summary": "Kullback-Leibler divergence is not the same as Leibler-Kullback divergence",
        "tags": [
            "leibler",
            "twitter",
            "kullback"
        ]
    },
    "https://twitter.com/arampell/status/1409306135467749378": {
        "extra-tags": [],
        "date": "2021-06-28",
        "title": "Twitter @arampell",
        "summary": "Selection bias (and its cousin, survivorship bias), or why you should be very skeptical of most surveys and \u201cstudies\u201d https://t.co/c5c5d4hTER",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/chipro/status/1409301480771702788": {
        "extra-tags": [],
        "date": "2021-06-28",
        "title": "Twitter @chipro",
        "summary": "The challenge for ML in production is to generalize to constantly changing edge cases. 2 main approaches:\n\n1. Use massive data because more data can lead to better generalization\n\n2. Build infra that allows models to learn to adapt in real-time\n\nHmu if you're excited about #2!",
        "tags": [
            "twitter",
            "ml"
        ]
    },
    "https://twitter.com/ylecun/status/1404855667874320397": {
        "extra-tags": [],
        "date": "2021-06-15",
        "title": "Twitter @ylecun",
        "summary": "Unlike Beaujolais Nouveau though, PyTorch does get better over time.",
        "tags": [
            "pytorch",
            "twitter",
            "beaujolais nouveau"
        ]
    },
    "https://twitter.com/andrewgwils/status/1403368940013772804": {
        "extra-tags": [],
        "date": "2021-06-11",
        "title": "Twitter @andrewgwils",
        "summary": "Does knowledge distillation really work? \nWhile distillation can improve student generalization, we show it is extremely difficult to achieve good agreement between student and teacher.\n\nhttps://t.co/VpK6Xy2q3S \nWith @samscub,  @Pavel_Izmailov,  @polkirichenko, Alex Alemi. 1/10 https://t.co/SuX1uuvukG",
        "tags": [
            "alex alemi",
            "twitter"
        ]
    },
    "https://twitter.com/mervenoyann/status/1403012723311587329": {
        "extra-tags": [],
        "date": "2021-06-10",
        "title": "Twitter @mervenoyann",
        "summary": "\ud83e\uddb9\ud83c\udffb\u200d\u2640\ufe0f https://t.co/ZhPwVWxnp9",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/halford_max/status/1403091624062861314": {
        "extra-tags": [],
        "date": "2021-06-10",
        "title": "Twitter @halford_max",
        "summary": "At Alan we are able to process health insurance documents and reimburse users in under a minute. I wrote an overview of this works in a blog post :) https://t.co/BvXwSLGLik",
        "tags": [
            "twitter",
            "alan"
        ]
    },
    "https://twitter.com/clara__meister/status/1402656308030017545": {
        "extra-tags": [],
        "date": "2021-06-09",
        "title": "Twitter @clara__meister",
        "summary": "Human languages exhibit certain statistical tendencies, e.g., the type-token relationship, but do language models learn these distributions during training?\n\nIn our ACL paper, we provide a framework for quantifying an answer to this question https://t.co/ExXlAhvd68",
        "tags": [
            "twitter",
            "acl"
        ]
    },
    "https://twitter.com/chamii22/status/1402697522938740741": {
        "extra-tags": [],
        "date": "2021-06-09",
        "title": "Twitter @chamii22",
        "summary": "If you are working with hyperbolic embeddings and need a PCA method to visualize and process your data, check-out our recent HoroPCA work!\n\nPaper: https://t.co/exfJgdJI1g\nCode: https://t.co/jEjfzCYSmG https://t.co/uxGHR9TH7t",
        "tags": [
            "twitter",
            "pca",
            "horopca"
        ]
    },
    "https://twitter.com/lilianweng/status/1400856064795451393": {
        "extra-tags": [],
        "date": "2021-06-04",
        "title": "Twitter @lilianweng",
        "summary": "Contrastive learning aims to learn representation such that similar samples stay close, while dissimilar ones are far apart. It can be applied to supervised / unsupervised data and has been shown to achieve good results on various tasks.\n\n\ud83d\udcda A long read: https://t.co/TZiaVA0qNS",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1401082294228639747": {
        "extra-tags": [],
        "date": "2021-06-05",
        "title": "Twitter @fishnets88",
        "summary": "Anybody working with timeseries/dates and human-behavior, you may enjoy `workalender`. It sure makes it a whole lot easier to remember when it is carnaval. \n\nhttps://t.co/2U3cEIB2Aj",
        "tags": [
            "twitter",
            "carnaval"
        ]
    },
    "https://twitter.com/ikuyamada/status/1400337133106130945": {
        "extra-tags": [],
        "date": "2021-06-03",
        "title": "Twitter @ikuyamada",
        "summary": "\ud83d\ude80Neural passage retrieval with substantially reduced memory size\ud83d\ude80\n\nBPR presented in our #acl2021nlp paper drastically reduces the memory size of the SOTA retriever (DPR) without a loss of QA accuracy\n\nPaper: https://t.co/PjUZNmVGMV\nCode/Model: https://t.co/JqPOzQlapk\n\n\ud83d\udc47Threads https://t.co/GYwJgw5V3V",
        "tags": [
            "sota",
            "twitter",
            "bpr"
        ]
    },
    "https://twitter.com/hyperfp/status/1400397830800482308": {
        "extra-tags": [
            "yamada"
        ],
        "date": "2021-06-03",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty tu te rappelles Yamada, n'est-ce pas ?\nhttps://t.co/6IQOx0i5DK https://t.co/WZvKG4vGCZ",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/TsinghuaNLP/status/1399641244251213824": {
        "extra-tags": [],
        "date": "2021-06-01",
        "title": "Twitter @TsinghuaNLP",
        "summary": "A sememe is defined as the minimum semantic unit of human languages. Some linguists believe that the meanings of all words can be expressed by a limited set of sememes. Many languages have no sememe knowledge bases (SKBs), and the construction of SKBs is very expensive. (1/3) https://t.co/vuYTqQz99A",
        "tags": [
            "twitter",
            "skbs"
        ]
    },
    "https://twitter.com/AdilZtn/status/1399651479993081857": {
        "extra-tags": [],
        "date": "2021-06-01",
        "title": "Twitter @AdilZtn",
        "summary": "@TsinghuaNLP @raphaelsrty",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/paperswithdata/status/1397110603878121475": {
        "extra-tags": [],
        "date": "2021-05-25",
        "title": "Twitter @paperswithdata",
        "summary": "\ud83c\udf66VANiLLa: A dataset of 100k questions for question answering over knowledge graphs offering answers in natural language sentences.\n\nThe answer sentences in this dataset are syntactically and semantically closer to the question than to the triple fact.\n\nhttps://t.co/5P4lUQ8nhl https://t.co/TTK25rFI2V",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/AndrewYNg/status/1396922136808202241": {
        "extra-tags": [],
        "date": "2021-05-24",
        "title": "Twitter @AndrewYNg",
        "summary": "Would love your feedback on this: AI Systems = Code (model/algorithm) + Data. Most academic benchmarks/competitions hold the Data fixed, and let teams work on the Code. Thinking of organizing something where we hold the Code fixed, and ask teams to work on the Data. (1/2)",
        "tags": [
            "twitter",
            "ai"
        ]
    },
    "https://twitter.com/fishnets88/status/1396131654465433600": {
        "extra-tags": [
            "packages"
        ],
        "date": "2021-05-22",
        "title": "Twitter @fishnets88",
        "summary": "Or other questions like \"which packages have circular dependencies?\"",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1396131578099732482": {
        "extra-tags": [],
        "date": "2021-05-22",
        "title": "Twitter @fishnets88",
        "summary": "Ah man. Neo4j is just so *fun* to toy with. I'm currently doing queries on python package dependencies and the query language just invites questions like \"whats the longest dependency chain in python?\"",
        "tags": [
            "twitter",
            "python",
            "neo4j"
        ]
    },
    "https://twitter.com/ilyaeck/status/1393132270806966272": {
        "extra-tags": [],
        "date": "2021-05-14",
        "title": "Twitter @ilyaeck",
        "summary": "Attention may be all you *want*, but what you *need* is effective token mixing!\nIn which we replace Transformers' self-attention with FFT and it works nearly as well but faster/cheaper. \nhttps://t.co/GiUvHkB3SK\nBy James Lee-Thorpe, Joshua Ainslie, @santiontanon and myself, sorta https://t.co/FDijWMBF2n",
        "tags": [
            "fft",
            "joshua ainslie",
            "twitter",
            "transformers",
            "james lee-thorpe"
        ]
    },
    "https://twitter.com/fchollet/status/1393660759331192843": {
        "extra-tags": [],
        "date": "2021-05-15",
        "title": "Twitter @fchollet",
        "summary": "When I set out to write the 2nd edition of Deep Learning with Python, I thought it would be roughly the same length, and about 50% new content. Now that the draft is done: it's almost 2x longer and it's ~75% new content.\n\nOverall it's a lot more in-depth than the first edition.",
        "tags": [
            "deep learning",
            "twitter",
            "python"
        ]
    },
    "https://twitter.com/CedricCuaz/status/1391686974587772929": {
        "extra-tags": [],
        "date": "2021-05-10",
        "title": "Twitter @CedricCuaz",
        "summary": "\ud83e\udd73\ud83e\udd73 I am delighted to share that our  paper \"Online Graph Dictionary Learning\"  has been accepted to @icmlconf . https://t.co/oFlUs9GLWP\nThis paper is the fruit of an enriching joint work with @t_vayer, @RFlamary , @Marco_Corneli  and @nicolas_courty . \ud83d\ude4c\ud83d\ude4f https://t.co/ycvVYFPrvd",
        "tags": [
            "twitter",
            "online graph dictionary learning"
        ]
    },
    "https://twitter.com/Croyades/status/1391442223787544581": {
        "extra-tags": [],
        "date": "2021-05-09",
        "title": "Twitter @Croyades",
        "summary": "@AnecdotesMaths @nightcall_66 Zut, rat\u00e9 \ud83d\ude05 https://t.co/O6EynirsGg",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/hyperfp/status/1389467854119047176": {
        "extra-tags": [],
        "date": "2021-05-04",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty https://t.co/LMHGl69wfD",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/seb_ruder/status/1387886948438708224": {
        "extra-tags": [],
        "date": "2021-04-29",
        "title": "Twitter @seb_ruder",
        "summary": "Types of ML / NLP Papers https://t.co/mdPMGUXL70",
        "tags": [
            "nlp",
            "twitter"
        ]
    },
    "https://twitter.com/charlottejee/status/1387722711766650884": {
        "extra-tags": [],
        "date": "2021-04-29",
        "title": "Twitter @charlottejee",
        "summary": "I've found it, the perfect explanation of NFTs https://t.co/4JfZteww8P",
        "tags": [
            "twitter",
            "nfts"
        ]
    },
    "https://twitter.com/mervenoyann/status/1385698705165193229": {
        "extra-tags": [],
        "date": "2021-04-23",
        "title": "Twitter @mervenoyann",
        "summary": "trying to write some natural language inputs and letting AI turn it into code reminded me of this: https://t.co/QWkzkHJEy8 https://t.co/9X59Cuv0kN",
        "tags": [
            "twitter",
            "ai"
        ]
    },
    "https://twitter.com/AnnaGHughes/status/1384237741127262214": {
        "extra-tags": [
            "machine learning"
        ],
        "date": "2021-04-19",
        "title": "Twitter @AnnaGHughes",
        "summary": "machine learning https://t.co/63JpB7RogP",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/huggingface/status/1379805752509005825": {
        "extra-tags": [],
        "date": "2021-04-07",
        "title": "Twitter @huggingface",
        "summary": "Need more fine-tuning data? A prompt is worth a thousand data points. @srush_nlp + @Fluke_Ellington show steering models with GPT3-style prompts during fine-tuning outperforms standard linear classifiers in their #naacl2021 paper. See our interactive blog https://t.co/lIDHiq2359 https://t.co/n1gWC2Xf4t",
        "tags": [
            "gpt3",
            "twitter",
            "naacl2021"
        ]
    },
    "https://twitter.com/AdilZtn/status/1380623484561334279": {
        "extra-tags": [],
        "date": "2021-04-09",
        "title": "Twitter @AdilZtn",
        "summary": "@PatrickKidger You're welcome \ud83d\ude07",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/PatrickKidger/status/1380621746647277571": {
        "extra-tags": [],
        "date": "2021-04-09",
        "title": "Twitter @PatrickKidger",
        "summary": "So I got a lot of requests about making torchtyping available for lower versions of Python. (Rather than requiring Python 3.9+).\n\nI am happy to report that it now works with Python 3.7+!\n\nBig thanks to @AdilZtn for adding this in.\n\nhttps://t.co/Wz29cSHCxQ https://t.co/9xtD7EKfLl",
        "tags": [
            "twitter",
            "python",
            "python 3."
        ]
    },
    "https://twitter.com/halford_max/status/1380070025634197505": {
        "extra-tags": [],
        "date": "2021-04-08",
        "title": "Twitter @halford_max",
        "summary": "Hey @pypi, could one of your admins apply the name transfer mentionned in this issue: https://t.co/mhBpsl7f61? We've been waiting for a couple of months, even though the name transfer has been approved :)",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/UnderscoreTalk/status/1377210676813438977": {
        "extra-tags": [
            "c"
        ],
        "date": "2021-03-31",
        "title": "Twitter @UnderscoreTalk",
        "summary": "Ah non c\u2019est nous ! https://t.co/qBtXfZFkwd",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/morgymcg/status/1372959921113825281": {
        "extra-tags": [],
        "date": "2021-03-19",
        "title": "Twitter @morgymcg",
        "summary": "\ud83d\ude0d This @huggingface  tip to prevent colab from disconnecting \n \n`\nfunction ConnectButton(){\n    console.log(\"Connect pushed\"); \n    document.querySelector(\"#top-toolbar &gt; colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n}\n\nsetInterval(ConnectButton,60000);\n` https://t.co/ibzQ92Tbro",
        "tags": [
            "twitter",
            "setinterval",
            "connectbutton",
            "connect",
            "shadowroot",
            "queryselector"
        ]
    },
    "https://twitter.com/fishnets88/status/1372837760957100035": {
        "extra-tags": [],
        "date": "2021-03-19",
        "title": "Twitter @fishnets88",
        "summary": "Did another iteration on `mktestdocs` the other day. You should now be able to consider the code in markdown files in `mkdocs` to also act as unit tests! \n\nhttps://t.co/zhs2AiGTfy",
        "tags": [
            "twitter",
            "mktestdocs",
            "mkdocs"
        ]
    },
    "https://twitter.com/hyperfp/status/1372547138803077121": {
        "extra-tags": [],
        "date": "2021-03-18",
        "title": "Twitter @hyperfp",
        "summary": "\"Text is the API for humans\"\n\n#NLP (in a pres @huggingface by @jeffboudier)",
        "tags": [
            "nlp",
            "twitter",
            "api"
        ]
    },
    "https://twitter.com/fishnets88/status/1369210928990978050": {
        "extra-tags": [],
        "date": "2021-03-09",
        "title": "Twitter @fishnets88",
        "summary": "Detecting names is *super* hard. Why? Because language models are often pre-trained on news articles that focus on a region. If you want to detect names locally, you also need to detect them globally. \n\nIn this video, I'll explore this in more detail.  \n\nhttps://t.co/OoMqSqBWmI",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/jgmorenof/status/1366425680486031364": {
        "extra-tags": [],
        "date": "2021-03-01",
        "title": "Twitter @jgmorenof",
        "summary": "An excellent talk from Naveen Saini about his recent work on cross-lingual summarization at @IRIToulouse  @embeddiaproject #CIMI #Toulouse https://t.co/yopa6mFVHU",
        "tags": [
            "twitter",
            "cimi",
            "toulouse",
            "naveen saini"
        ]
    },
    "https://twitter.com/L_badikho/status/1364659494328950788": {
        "extra-tags": [],
        "date": "2021-02-24",
        "title": "Twitter @L_badikho",
        "summary": "@timnitGebru Equally symbolic is that the rare/only AI YouTuber who understood what's at stake now for society, beyond the narrow US-centered point of view, is a non-native-English speaking YouTuber, who had to switch language of his channel and do it again in English:\nhttps://t.co/4C2wbL3jOn",
        "tags": [
            "us",
            "youtuber",
            "twitter",
            "ai"
        ]
    },
    "https://twitter.com/JFPuget/status/1363129388372475906": {
        "extra-tags": [],
        "date": "2021-02-20",
        "title": "Twitter @JFPuget",
        "summary": "I am not surprised by what happens at Google AI Ethics team.  Google is a corporation with a Wall Street CFO, and it most probably expects loyalty from its employees.  This is incompatible with having a team whose mission is to publicly critic (when need be) Google product. 1/2",
        "tags": [
            "wall street cfo",
            "twitter",
            "google ai ethics",
            "google"
        ]
    },
    "https://twitter.com/le_science4all/status/1363126946792636416": {
        "extra-tags": [],
        "date": "2021-02-20",
        "title": "Twitter @le_science4all",
        "summary": "C'est avec exasp\u00e9ration que je suis en train de #doomscroller mon feed twitter. \n\nEn dehors des acad\u00e9miques et Googlers directement concern\u00e9s, et d'une poign\u00e9e de usual suspects, j'ai l'horrible sentiment que tout le monde s'en fout. #AIEthics \n\nJe me sens d\u00e9sesp\u00e9r\u00e9. https://t.co/H6N7Dsxvxi",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/AdilZtn/status/1361388065328140294": {
        "extra-tags": [],
        "date": "2021-02-15",
        "title": "Twitter @AdilZtn",
        "summary": "Shameless self-promotion, if you are interested in reinforcement learning (RL), you may want to know about masking. Great achievements such as Alphastar, Open Ai Five, or Hide and seek use masking.\nHere is my new blog post.\nhttps://t.co/7O51wDaSKC",
        "tags": [
            "hide",
            "twitter",
            "open ai five",
            "alphastar"
        ]
    },
    "https://twitter.com/halford_max/status/1361328458828288001": {
        "extra-tags": [],
        "date": "2021-02-15",
        "title": "Twitter @halford_max",
        "summary": "Here's a video I made to explain to a data science team using River how to use River transformers. They insisted on me sharing it! https://t.co/lV6wVKmJr0 #productivity #video",
        "tags": [
            "river",
            "twitter",
            "river transformers"
        ]
    },
    "https://twitter.com/fishnets88/status/1360140355195310080": {
        "extra-tags": [],
        "date": "2021-02-12",
        "title": "Twitter @fishnets88",
        "summary": "GridSearch is Not Enough. Part 3. \n\nWhat Overfitting Looks Like. \n\nhttps://t.co/yKnPdE8RHM",
        "tags": [
            "twitter",
            "gridsearch"
        ]
    },
    "https://twitter.com/J_Puerini/status/1354622608000614402": {
        "extra-tags": [],
        "date": "2021-01-28",
        "title": "Twitter @J_Puerini",
        "summary": "Wall Street clearly underestimated a generation raised on highly coordinated Friday night World of Warcraft raids.",
        "tags": [
            "wall street",
            "twitter",
            "world of warcraft"
        ]
    },
    "https://twitter.com/fishnets88/status/1354798914910621698": {
        "extra-tags": [],
        "date": "2021-01-28",
        "title": "Twitter @fishnets88",
        "summary": "For the horde! https://t.co/7ambjSsFta",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/AnecdotesMaths/status/1352632842321330181": {
        "extra-tags": [
            "diff"
        ],
        "date": "2021-01-22",
        "title": "Twitter @AnecdotesMaths",
        "summary": "La somme de deux entiers cons\u00e9cutifs est \u00e9gale \u00e0 la diff\u00e9rence de leurs carr\u00e9s.\nExemple: 13 + 14 = 27 et 14\u00b2 - 13\u00b2 = 27.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/luc_damas/status/1352541074401718272": {
        "extra-tags": [
            "php"
        ],
        "date": "2021-01-22",
        "title": "Twitter @luc_damas",
        "summary": "La prochaine fois, je demande 42 lignes et 73 colonnes...\n#prof #d\u00e9prime #correctiondecopies #php #algo https://t.co/fuaHHbB1GS",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/hyperfp/status/1352589141826031616": {
        "extra-tags": [],
        "date": "2021-01-22",
        "title": "Twitter @hyperfp",
        "summary": "Pas de masques, pas de tests, pas de vaccins, et maintenant pas de CARAMBARS ?\ny'en a marre",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/pgroth/status/1350400529239855104": {
        "extra-tags": [],
        "date": "2021-01-16",
        "title": "Twitter @pgroth",
        "summary": "Good news: Our paper \u201cInductive Entity Representations from Text via Link Prediction\u201d has been accepted into @TheWebConf 2021. \ud83c\udf89 work of @danieldazac with @michaelcochez @INDE_LAB_AMS @krr_vu preprint: https://t.co/4Q69NYYA9F",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/halford_max/status/1348683930422149129": {
        "extra-tags": [],
        "date": "2021-01-11",
        "title": "Twitter @halford_max",
        "summary": "Converting one person to online machine learning at a time! https://t.co/03tGF7pS4W",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/halford_max/status/1343495505964371968": {
        "extra-tags": [],
        "date": "2020-12-28",
        "title": "Twitter @halford_max",
        "summary": "@chipro An important thing to understand is that latency is highly dominated by network speed. A faster model has virtually no impact if most of the processing time (as perceived by the user) is taken up by networking time.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/chipro/status/1343376937893433346": {
        "extra-tags": [],
        "date": "2020-12-28",
        "title": "Twitter @chipro",
        "summary": "Based on conversations with a dozen companies doing real-time ML in both US and China, I wrote about online inference and online learning -- their use cases, solutions, and challenges.\n\nMachine learning is going real-time, and most companies aren't ready.\n\nhttps://t.co/42kaLA7ryH",
        "tags": [
            "us",
            "twitter",
            "china"
        ]
    },
    "https://twitter.com/AdilZtn/status/1346527112900050944": {
        "extra-tags": [],
        "date": "2021-01-05",
        "title": "Twitter @AdilZtn",
        "summary": "I had bet on Open AI to be the first to publish a multi-modal transformer. I was wrong \ud83d\ude43 https://t.co/FNJTn1NA7V",
        "tags": [
            "open ai",
            "twitter"
        ]
    },
    "https://twitter.com/_akhaliq/status/1340844028913479680": {
        "extra-tags": [],
        "date": "2020-12-21",
        "title": "Twitter @_akhaliq",
        "summary": "Understood in Translation: Transformers for Domain Understanding\npdf: https://t.co/tqa3NWgJRD\nabs: https://t.co/yOlX0N8VC6\ngithub: https://t.co/xz7LzhsnvB https://t.co/upXLbUWYNT",
        "tags": [
            "transformers for domain",
            "twitter"
        ]
    },
    "https://twitter.com/AdilZtn/status/1340949547393146880": {
        "extra-tags": [],
        "date": "2020-12-21",
        "title": "Twitter @AdilZtn",
        "summary": "@ak92501 @raphaelsrty",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/JackDMurphy/status/1332306300395544578": {
        "extra-tags": [],
        "date": "2020-11-27",
        "title": "Twitter @JackDMurphy",
        "summary": "The best thing I\u2019ve learned this week is that when squirrels fall/jump - they land like superheroes https://t.co/XuY80hCuNp",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1327529234919731202": {
        "extra-tags": [],
        "date": "2020-11-14",
        "title": "Twitter @fishnets88",
        "summary": "There's a small python library called \"chime\" that will play a sound when-ever an error/success happens on a long running task. It's cute but also ... surprisingly useful. It also allows for mario/zelda themed cues.  Made by Max Halford @halford_max \n\nhttps://t.co/SrhILhYgeg",
        "tags": [
            "zelda",
            "twitter",
            "chime",
            "max halford",
            "mario"
        ]
    },
    "https://twitter.com/gvanrossum/status/1326932991566700549": {
        "extra-tags": [],
        "date": "2020-11-12",
        "title": "Twitter @gvanrossum",
        "summary": "I decided that retirement was boring and have joined the Developer Division at Microsoft. To do what? Too many options to say! But it\u2019ll make using Python better for sure (and not just on Windows :-). There\u2019s lots of open source here. Watch this space.",
        "tags": [
            "microsoft",
            "windows",
            "twitter",
            "developer division",
            "python"
        ]
    },
    "https://twitter.com/PyTorch/status/1326949835132432385": {
        "extra-tags": [],
        "date": "2020-11-12",
        "title": "Twitter @PyTorch",
        "summary": "[Announcing at PyTorch DevDay Livestream: https://t.co/uNSqJwDA0e] \n\nNNAPI support for PyTorch allows Android apps to run computationally intensive NN on the most powerful/efficient parts of the chips in mobile phones, including DSPs and NPUs. Learn more: https://t.co/KUeIBLI32t",
        "tags": [
            "devday",
            "twitter",
            "nnapi",
            "pytorch",
            "npus",
            "dsps",
            "android"
        ]
    },
    "https://twitter.com/twiecki/status/1322485866602921984": {
        "extra-tags": [],
        "date": "2020-10-31",
        "title": "Twitter @twiecki",
        "summary": "TimeSeers: a hierarchical Bayesian time series model based on Facebook's Prophet, written in #PyMC3. https://t.co/xk0YXYkYMS #PyMCon talk: https://t.co/dFszKoVk7O by @MatthijsBrs https://t.co/iqDLqmZ49Y",
        "tags": [
            "timeseers",
            "twitter",
            "facebook",
            "pymcon",
            "bayesian",
            "pymc3"
        ]
    },
    "https://twitter.com/AdilZtn/status/1320813408883716096": {
        "extra-tags": [],
        "date": "2020-10-26",
        "title": "Twitter @AdilZtn",
        "summary": "https://t.co/v1XAU8SP0o",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fishnets88/status/1319568794411565056": {
        "extra-tags": [],
        "date": "2020-10-23",
        "title": "Twitter @fishnets88",
        "summary": "1. Put domain knowledge in a Python function.\n2. Turn function into Scikit-Learn model\n3. Benchmark it using GridSearch.\n\nhttps://t.co/tqTyaSJzeh https://t.co/TsTUjkJHrM",
        "tags": [
            "twitter",
            "python",
            "gridsearch",
            "scikit",
            "benchmark"
        ]
    },
    "https://twitter.com/AdilZtn/status/1318126901517484032": {
        "extra-tags": [],
        "date": "2020-10-19",
        "title": "Twitter @AdilZtn",
        "summary": "If you are interested in knowledge graphs I recommend you to have a look at my friend's library @raphaelsrty https://t.co/p88QmK6oJQ",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/jxfeb/status/1317090514353573889": {
        "extra-tags": [],
        "date": "2020-10-16",
        "title": "Twitter @jxfeb",
        "summary": "We have just released three of our compressed BERT models: TinyBERT, DynaBERT and TernaryBERT (published on #emnlp2020 and #NeurIPS2020 ) on @huggingface at https://t.co/F7uG7t9OGb",
        "tags": [
            "tinybert",
            "ternarybert",
            "dynabert",
            "twitter",
            "neurips2020"
        ]
    },
    "https://twitter.com/ianpanmd/status/1314337987988271110": {
        "extra-tags": [],
        "date": "2020-10-08",
        "title": "Twitter @ianpanmd",
        "summary": "Someone once told me to stop wasting my time on Kaggle because it has no academic value.\n\nMy answer: I don't care. I like Kaggle. It's fun (way more fun than research). I learn a lot. That's why I do it.",
        "tags": [
            "twitter",
            "kaggle"
        ]
    },
    "https://twitter.com/fishnets88/status/1314189840590819329": {
        "extra-tags": [],
        "date": "2020-10-08",
        "title": "Twitter @fishnets88",
        "summary": "What if ... you can draw a machine learning model? \n\nIntroducing: human-learn https://t.co/AlPYRBr41p https://t.co/WK6CfzlaUN",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/halford_max/status/1312796683395268608": {
        "extra-tags": [],
        "date": "2020-10-04",
        "title": "Twitter @halford_max",
        "summary": "Just wrote a blog post on how to classify documents using word embeddings when you don't have access to labeled training data: https://t.co/NnyV48D5h2. Feedback appreciated!",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/michael_galkin/status/1281670158151540736": {
        "extra-tags": [],
        "date": "2020-07-10",
        "title": "Twitter @michael_galkin",
        "summary": "ACL 2020 #acl2020nlp ends this week! If you didn't manage to attend all #KnowledgeGraph related talks - I comprised a review of KG-related papers focusing on question answering, KG embeddings, graph-to-text NLG, some ConvAI and OpenIE \ud83d\ude0a #NLProc \nhttps://t.co/s0p5FdyuJs",
        "tags": [
            "acl",
            "twitter",
            "nlproc",
            "convai",
            "nlg",
            "openie"
        ]
    },
    "https://twitter.com/dataflowr/status/1311728748312526849": {
        "extra-tags": [],
        "date": "2020-10-01",
        "title": "Twitter @dataflowr",
        "summary": "Want to learn automatic differentiation with @PyTorch ? Have a look at https://t.co/z97CvmXdB5 and code your own backprop with #numpy https://t.co/nzkBCRy1m7 https://t.co/Tgh1BIcSmG",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/hyperfp/status/1311388292374507520": {
        "extra-tags": [
            "c"
        ],
        "date": "2020-09-30",
        "title": "Twitter @hyperfp",
        "summary": "\u201cL\u2019\u00e2ge reste le facteur de risque principal de gravit\u00e9 du #COVID19\u201d, et c\u2019est pour \u00e7a que l\u2019homme de #Neandertal y est tr\u00e8s sensible.\nhttps://t.co/0K9ChrVB7m",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/chipro/status/1310952120431063041": {
        "extra-tags": [],
        "date": "2020-09-29",
        "title": "Twitter @chipro",
        "summary": "When talking to people who haven\u2019t deployed ML models, I keep hearing a lot of misperceptions about ML models in production. Here are a few of them.\n\n(1/6)",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/huggingface/status/1310597554774716418": {
        "extra-tags": [],
        "date": "2020-09-28",
        "title": "Twitter @huggingface",
        "summary": "We're excited to announce the\ud83e\udd17Transformers release of the Retrieval-Augmented Generation model in collaboration with @facebookai!\n\nPaper: https://t.co/KgiUdQ8Gzg\nDemo: https://t.co/RigCiHuqTK\n\ud83e\udd17Doc: https://t.co/o2bUBmzLvJ\nBlog post: https://t.co/J18sYTa6Da https://t.co/NBjy4tEjSz",
        "tags": [
            "retrieval",
            "twitter"
        ]
    },
    "https://twitter.com/MetaAI/status/1310595343386464256": {
        "extra-tags": [],
        "date": "2020-09-28",
        "title": "Twitter @MetaAI",
        "summary": "Our Retrieval Augmented Generation #NLP model is now available as part of the @HuggingFace transformer library. The true strength of RAG is in its flexibility. You control what it knows simply by swapping out the documents it uses for knowledge retrieval. https://t.co/M7PM5eWorP https://t.co/51ibozdVeG",
        "tags": [
            "nlp",
            "twitter",
            "rag"
        ]
    },
    "https://twitter.com/AnecdotesMaths/status/1310595725135314945": {
        "extra-tags": [],
        "date": "2020-09-28",
        "title": "Twitter @AnecdotesMaths",
        "summary": "En prenant deux nombres cons\u00e9cutifs de la suite de Fibonacci (1, 2, 3, 5, 8, 13, 21, 34, ...), on peut approximativement convertir des miles en kilom\u00e8tres.\n\nPar exemple, 5 miles valent environ 8 km et 21 miles valent environ 34 km.",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/le_gorafi/status/1309825154403512320": {
        "extra-tags": [],
        "date": "2020-09-26",
        "title": "Twitter @le_gorafi",
        "summary": "\u00ab Vous ne m'aurez pas vivant \u00bb hurle Didier Raoult aux commandes d'un Canadair charg\u00e9 de chloroquine https://t.co/rMO6refTez",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/OriolVinyalsML/status/1233783593626951681": {
        "extra-tags": [],
        "date": "2020-02-29",
        "title": "Twitter @OriolVinyalsML",
        "summary": "Transformers are a special case of Graph Neural Networks. This may be obvious to some, but the following blog post does a good job at explaining these important concepts. https://t.co/H8LT2F7LqC",
        "tags": [
            "twitter",
            "transformers",
            "graph neural networks"
        ]
    },
    "https://twitter.com/le_science4all/status/1304092824506437632": {
        "extra-tags": [
            "ai"
        ],
        "date": "2020-09-10",
        "title": "Twitter @le_science4all",
        "summary": "Donc si je comprends bien, 13% de mes abonn\u00e9s ne sont pas abonn\u00e9s \u00e0 @dlouapre ?\n\nJ'ai pas envie d'insulter mes abonn\u00e9s... Mais qu'est-ce que ces 13% foutent ? Pourquoi suivre un guignol comme moi, quand vous pouvez suivre le boss de la vulgarisation fran\u00e7aise ?? \ud83d\ude1c https://t.co/mxVex2qNrD",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/gabrielpeyre/status/1299572487181615105": {
        "extra-tags": [],
        "date": "2020-08-29",
        "title": "Twitter @gabrielpeyre",
        "summary": "Comparison of the Wasserstein, Hellinger, Kullback-Leibler and reverse KL on the space of Gaussian distributions. https://t.co/QLbZdoG3GC https://t.co/BtekhkMZGK",
        "tags": [
            "kullback",
            "hellinger",
            "leibler",
            "twitter",
            "wasserstein"
        ]
    },
    "https://twitter.com/AdilZtn/status/1296784668361207810": {
        "extra-tags": [],
        "date": "2020-08-21",
        "title": "Twitter @AdilZtn",
        "summary": "@MLearnia La librairie standard",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/Army1Seven/status/1296188204660523009": {
        "extra-tags": [],
        "date": "2020-08-19",
        "title": "Twitter @Army1Seven",
        "summary": "This is a 393-years old Greenland Shark that was located in the Arctic Ocean. It's been wandering the ocean since 1627. It is the oldest living vertebrate known on the planet.\nPhoto by Julius Nielsen. https://t.co/4MFpx5fqcM",
        "tags": [
            "twitter",
            "greenland shark",
            "julius nielsen",
            "arctic ocean"
        ]
    },
    "https://twitter.com/IanOsband/status/1294249291427516416": {
        "extra-tags": [],
        "date": "2020-08-14",
        "title": "Twitter @IanOsband",
        "summary": "Big thanks to @pbloemesquire for a great tutorial:\n\nTransformers from scratch\nhttps://t.co/zBxtnGSZz5\n\nIf (like me) you're excited about #GPT3 but found yourself waving your hands through various NN diagrams on self-attention... this is the cure! \ud83d\ude4c https://t.co/Z37JTPFwc1",
        "tags": [
            "gpt3",
            "twitter",
            "nn",
            "transformers"
        ]
    },
    "https://twitter.com/hyperfp/status/1293826466824822784": {
        "extra-tags": [],
        "date": "2020-08-13",
        "title": "Twitter @hyperfp",
        "summary": "@GoogleAI @jgmorenof @raphaelsrty : bonne pioche de Jos\u00e9. See also https://t.co/iz2OszhB2g",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/balajiln/status/1292263360852103170": {
        "extra-tags": [],
        "date": "2020-08-09",
        "title": "Twitter @balajiln",
        "summary": "Really cool paper that combines Mondrian forests with Polya trees for online density estimation &amp; anomaly detection! :)\n\nInterpretable Anomaly Detection with Mondrian Polya Forests on Data Streams\nCharlie Dickens, Eric Meissner, Pablo G. Moreno, Tom Diethe\nhttps://t.co/AL2PNR0qlb https://t.co/qmzwuKACwZ",
        "tags": [
            "pablo g. moreno",
            "eric meissner",
            "twitter",
            "polya",
            "mondrian",
            "mondrian polya forests",
            "tom diethe",
            "charlie dickens"
        ]
    },
    "https://twitter.com/sacmehtauw/status/1290457130579918853": {
        "extra-tags": [],
        "date": "2020-08-04",
        "title": "Twitter @sacmehtauw",
        "summary": "Do we need multiple heads and wider FFN layers in Transformers? No. \n\nOur work, DeLighT, effectively replaces these components with single-headed attention and light-weight FFN layers.\n\nJoint work with @gh_marjan, @sriniiyer88, @LukeZettlemoyer, and @HannaHajishirzi https://t.co/GHRCmFwbeA",
        "tags": [
            "twitter",
            "delight",
            "ffn",
            "transformers"
        ]
    },
    "https://twitter.com/hyperfp/status/1280167053761155073": {
        "extra-tags": [],
        "date": "2020-07-06",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty &gt; Methods which leverage the strength of both neural and symbolic approaches. Specifically, we augment raw text with symbolic structure about entities and their relations from a knowledge graph, and learn task-specific neural embeddings of the combined data structure",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/hyperfp/status/1280164035917426690": {
        "extra-tags": [],
        "date": "2020-07-06",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty \"differentiable reasoning over Virtual KB\", je trouve \u00e7a tr\u00e8s po\u00e9tique https://t.co/IaRfPiKqgm",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/daibond_alpha/status/1277010952727085058": {
        "extra-tags": [],
        "date": "2020-06-27",
        "title": "Twitter @daibond_alpha",
        "summary": "Tired of Softmax? You may try our neural K-NN. Our SOFT top-k operator enables an efficient end2end training! \n\u201cDifferentiable Top-k Operator with Optimal Transport\u201d \nhttps://t.co/1nLA8eESA6 with @Xiexieyujia, @hanjundai, @MinshuoC, @tourzhao, Hongyuan Zha, Wei Wei, Tomas Pfister https://t.co/AHnnuL4Aks",
        "tags": [
            "k-nn",
            "wei wei",
            "tomas pfister",
            "twitter",
            "softmax",
            "hongyuan zha"
        ]
    },
    "https://twitter.com/hyperfp/status/1277573112780001282": {
        "extra-tags": [
            "top"
        ],
        "date": "2020-06-29",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty soft top-k https://t.co/owbjKlQ7Vb",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/hyperfp/status/1276941949740748800": {
        "extra-tags": [],
        "date": "2020-06-27",
        "title": "Twitter @hyperfp",
        "summary": "\"These results support the hypothesis that a drive to predict future inputs may shape human language processing, and perhaps the way knowledge of language is learned and organized in the brain\"\n#NLP #ComputationalNeuroscience https://t.co/p1xfw53CPq",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/fchollet/status/1271225205617160192": {
        "extra-tags": [
            "open-source"
        ],
        "date": "2020-06-11",
        "title": "Twitter @fchollet",
        "summary": "It's magical what open-source communities are capable of. Defies everything that all the tie-wearing Serious People believe about work and value creation",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/hyperfp/status/1265437839749189634": {
        "extra-tags": [],
        "date": "2020-05-27",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty ;-) https://t.co/CbyJPJcsM5",
        "tags": [
            "twitter"
        ]
    },
    "https://twitter.com/Zergylord/status/1264902860178960384": {
        "extra-tags": [],
        "date": "2020-05-25",
        "title": "Twitter @Zergylord",
        "summary": "I find it quite frustrating that people think that publishing results on all 57 Atari games is a PR stunt. It's far easier to make improvements on a cherry-picked subset. Running on the full suite provides a useful sanity check for generality. \nAnd trust me, the press don't care.",
        "tags": [
            "atari",
            "twitter"
        ]
    },
    "https://twitter.com/fchollet/status/1262788913510223872": {
        "extra-tags": [],
        "date": "2020-05-19",
        "title": "Twitter @fchollet",
        "summary": "We were all inexperienced &amp; unskilled at some point. We have all made mistakes, and we will all make more mistakes in the future. But being a jerk is a choice. Don't be a jerk.",
        "tags": [
            "twitter"
        ]
    },
    "https://lukesalamone.github.io/posts/what-is-temperature/": {
        "extra-tags": [],
        "title": "Hackernews What is Temperature in NLP? (lukesalamone.github.io)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://maxhalford.github.io/blog/ogd-in-sql/": {
        "extra-tags": [],
        "title": "Hackernews Stochastic gradient descent written in SQL (maxhalford.github.io)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://dkb.blog/p/google-search-is-dying": {
        "extra-tags": [],
        "title": "Hackernews Google Search Is Dying (2022) (dkb.blog)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://arxiv.org/abs/2009.01325": {
        "extra-tags": [],
        "title": "Hackernews Learning to summarize from human feedback (2022) (arxiv.org)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://commoncog.com/goodharts-law-not-useful/": {
        "extra-tags": [],
        "title": "Hackernews Limitations of Goodhart's Law (commoncog.com)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://www.w3.org/Provider/Style/URI": {
        "extra-tags": [],
        "title": "Hackernews Cool URIs Don't Change (1998) (w3.org)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://duckdb.org/2023/03/03/json.html": {
        "extra-tags": [],
        "title": "Hackernews DuckDB: Querying JSON files as if they were tables (duckdb.org)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://github.com/openai/openai-python/blob/main/chatml.md": {
        "extra-tags": [],
        "title": "Hackernews ChatML: ChatGPT API expects a structured format, called Chat Markup Language (github.com/openai)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://openai.com/blog/introducing-chatgpt-and-whisper-apis": {
        "extra-tags": [],
        "title": "Hackernews Introducing ChatGPT and Whisper APIs (openai.com)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://github.com/microsoft/unilm": {
        "extra-tags": [],
        "title": "Hackernews Microsoft Kosmos-1: A Multimodal Large Language Model (github.com/microsoft)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://www.infoworld.com/article/3687744/how-to-write-python-extensions-in-rust-with-pyo3.html": {
        "extra-tags": [],
        "title": "Hackernews How to write Python extensions in Rust with PyO3 (infoworld.com)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://github.com/pgvector/pgvector": {
        "extra-tags": [],
        "title": "Hackernews Pgvector: Open-source vector similarity search for Postgres (github.com/pgvector)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://www.everydaydrinking.com/p/where-has-all-the-chartreuse-gone": {
        "extra-tags": [],
        "title": "Hackernews Where has all the Chartreuse gone? (everydaydrinking.com)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://en.wikipedia.org/wiki/Bloom%27s_2_sigma_problem": {
        "extra-tags": [
            "wikipedia"
        ],
        "title": "Hackernews Bloom's 2 sigma problem (wikipedia.org)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://danfitdegree.hashnode.dev/nothing-has-ever-angered-me-more-than-the-google-play-team": {
        "extra-tags": [],
        "title": "Hackernews Two weeks of dealing with Google as a developer (hashnode.dev)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://www.tbray.org/ongoing/When/202x/2022/11/07/Just-Dont": {
        "extra-tags": [],
        "title": "Hackernews Just don\u2019t (tbray.org)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://www.leidenmedievalistsblog.nl/articles/whats-wrong-with-medieval-pigs-in-videogames": {
        "extra-tags": [],
        "title": "Hackernews What\u2019s wrong with medieval pigs in videogames (leidenmedievalistsblog.nl)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://jackrusher.com/strange-loop-2022/": {
        "extra-tags": [],
        "title": "Hackernews Stop Writing Dead Programs (jackrusher.com)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://objective-see.org/tools.html": {
        "extra-tags": [],
        "title": "Hackernews macOS Free and Open-Source Security Tools by Objective-See (objective-see.org)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://wakatime.com/blog/56-building-a-distributed-task-queue-in-python": {
        "extra-tags": [],
        "title": "Hackernews Show HN: WakaQ - a Python distributed task queue (wakatime.com)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://febs.onlinelibrary.wiley.com/doi/10.1002/1873-3468.14473": {
        "extra-tags": [
            "academic"
        ],
        "title": "Hackernews Breaking the silence around academic bullying (wiley.com)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://simonwillison.net/2022/Aug/29/stable-diffusion/": {
        "extra-tags": [
            "diffusion"
        ],
        "title": "Hackernews Stable Diffusion is a big deal (simonwillison.net)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://pudding.cool/2022/08/censorship/": {
        "extra-tags": [],
        "title": "Hackernews The Big [Censored] Theory (pudding.cool)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://tjukanovt.github.io/notable-people": {
        "extra-tags": [
            "github"
        ],
        "title": "Hackernews Map showing birthplaces of \"notable people\" around the world (tjukanovt.github.io)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://github.com/AdilZouitine/online-statistics.rs": {
        "extra-tags": [],
        "title": "Hackernews Online Statistics in Rust (github.com/adilzouitine)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://www.stoutner.com/mojeek-blog-post/": {
        "extra-tags": [],
        "title": "Hackernews Browser developer discusses his expectations of search engines (stoutner.com)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://github.com/browsh-org/browsh": {
        "extra-tags": [],
        "title": "Hackernews Browsh \u2013 A fully-modern text-based browser, rendering to TTY and browsers (github.com/browsh-org)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://www.visiticeland.com/outhorse-your-email/": {
        "extra-tags": [
            "email"
        ],
        "title": "Hackernews Outhorse Your Email (visiticeland.com)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "http://petereliaskraft.net/blog/query-serving-systems": {
        "extra-tags": [],
        "title": "Hackernews Query serving systems: An emerging category of data systems (petereliaskraft.net)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://fly.io/docs/": {
        "extra-tags": [],
        "title": "Continuous Deployment with Fly.io and GitHub Actions",
        "summary": "Documentation and guides from the team at Fly.io.",
        "date": "2023-03-12",
        "tags": []
    },
    "https://platform.openai.com": {
        "extra-tags": [],
        "title": "OpenAI API",
        "summary": "An API for accessing new AI models developed by OpenAI",
        "date": "2023-03-05",
        "tags": [
            "openai platform"
        ]
    },
    "https://quickstarts.snowflake.com/guide/machine_learning_with_snowpark_python/index.html?index=..%2F..index#0": {
        "extra-tags": [
            "machine learning",
            "python"
        ],
        "title": "Machine Learning with Snowpark Python",
        "summary": "",
        "date": "2023-03-04",
        "tags": [
            "snowflake;python;snowpark;machine learning"
        ]
    },
    "http://ieeexplore.ieee.org/document/4633963/": {
        "extra-tags": [],
        "title": "A neural network approach to ordinal regression",
        "summary": "Ordinal regression is an important type of learning, which has properties of both classi\ufb01cation and regression. Here we describe an effective approach to adapt a traditional neural network to learn ordinal categories. Our approach is a generalization of the perceptron method for ordinal regression. On several benchmark datasets, our method (NNRank) outperforms a neural network classi\ufb01cation method. Compared with the ordinal regression methods using Gaussian processes and support vector machines, NNRank achieves comparable performance. Moreover, NNRank has the advantages of traditional neural networks: learning in both online and batch modes, handling very large training datasets, and making rapid predictions. These features make NNRank a useful and complementary tool for large-scale data mining tasks such as information retrieval, web page ranking, collaborative \ufb01ltering, and protein ranking in Bioinformatics. The neural network software is available at: http://www.cs.missouri.edu/\u223cchengji/cheng software.html.",
        "date": "2023-03-04",
        "tags": [
            "ranking"
        ]
    },
    "http://arxiv.org/abs/1709.07604": {
        "extra-tags": [],
        "title": "A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications",
        "summary": "Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximally preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work address these challenges in their solutions. Finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques and application scenarios.",
        "date": "2023-03-04",
        "tags": [
            "computer science - artificial intelligence",
            "knowledge graph"
        ]
    },
    "https://github.com/jwills/dbt-duckdb": {
        "extra-tags": [],
        "title": "jwills/dbt-duckdb",
        "summary": "dbt (http://getdbt.com) adapter for DuckDB (http://duckdb.org)",
        "date": "2023-03-04",
        "tags": []
    },
    "http://orca.st.usm.edu/~zwang/files/rank.pdf": {
        "extra-tags": [
            "neural",
            "regression"
        ],
        "title": "A Neural Network Approach to Ordinal Regression",
        "summary": "",
        "date": "2023-03-04",
        "tags": []
    },
    "//ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/": {
        "extra-tags": [],
        "title": "Encoding cyclical continuous features - 24-hour time",
        "summary": "Some data is inherently cyclical. Time is a rich example of this: minutes, hours, seconds, day of week, week of month, month, season, and so on all follow cycles. Ecological features like tide, astrological features like position in orbit, spatial features like rotation or longitude, visual features like color wheels are all naturally cyclical.",
        "date": "2023-03-04",
        "tags": []
    },
    "https://proceedings.neurips.cc/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html": {
        "extra-tags": [],
        "title": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting",
        "summary": "Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease. Code is available at this repository: https://github.com/thuml/Autoformer.",
        "date": "2023-03-04",
        "tags": []
    },
    "https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/": {
        "extra-tags": [
            "named entity recognition",
            "bert"
        ],
        "title": "Named Entity Recognition with Bert \u2013 Depends on the definition",
        "summary": "",
        "date": "2023-03-04",
        "tags": []
    },
    "http://ieeexplore.ieee.org/document/6823700/": {
        "extra-tags": [],
        "title": "Entity Linking with a Knowledge Base: Issues, Techniques, and Solutions",
        "summary": "The large number of potential applications from bridging Web data with knowledge bases have led to an increase in the entity linking research. Entity linking is the task to link entity mentions in text with their corresponding entities in a knowledge base. Potential applications include information extraction, information retrieval, and knowledge base population. However, this task is challenging due to name variations and entity ambiguity. In this survey, we present a thorough overview and analysis of the main approaches to entity linking, and discuss various applications, the evaluation of entity linking systems, and future directions.",
        "date": "2023-03-04",
        "tags": []
    },
    "http://arxiv.org/abs/1802.01021": {
        "extra-tags": [],
        "title": "DeepType: Multilingual Entity Linking by Neural Type System Evolution",
        "summary": "The wealth of structured (e.g. Wikidata) and unstructured data about the world available today presents an incredible opportunity for tomorrow's Artificial Intelligence. So far, integration of these two different modalities is a difficult process, involving many decisions concerning how best to represent the information so that it will be captured or useful, and hand-labeling large amounts of data. DeepType overcomes this challenge by explicitly integrating symbolic information into the reasoning process of a neural network with a type system. First we construct a type system, and second, we use it to constrain the outputs of a neural network to respect the symbolic structure. We achieve this by reformulating the design problem into a mixed integer problem: create a type system and subsequently train a neural network with it. In this reformulation discrete variables select which parent-child relations from an ontology are types within the type system, while continuous variables control a classifier fit to the type system. The original problem cannot be solved exactly, so we propose a 2-step algorithm: 1) heuristic search or stochastic optimization over discrete variables that define a type system informed by an Oracle and a Learnability heuristic, 2) gradient descent to fit classifier parameters. We apply DeepType to the problem of Entity Linking on three standard datasets (i.e. WikiDisamb30, CoNLL (YAGO), TAC KBP 2010) and find that it outperforms all existing solutions by a wide margin, including approaches that rely on a human-designed type system or recent deep learning-based entity embeddings, while explicitly using symbolic information lets it integrate new entities without retraining.",
        "date": "2023-03-04",
        "tags": [
            "computer science - computation and language"
        ]
    },
    "http://139.129.163.161//": {
        "extra-tags": [],
        "title": "OpenKE - An Open-source Framework for Knowledge Embedding.",
        "summary": "",
        "date": "2023-03-04",
        "tags": []
    },
    "http://arxiv.org/abs/1905.09791": {
        "extra-tags": [],
        "title": "Multi-relational Poincar\\'e Graph Embeddings",
        "summary": "Hyperbolic embeddings have recently gained attention in machine learning due to their ability to represent hierarchical data more accurately and succinctly than their Euclidean analogues. However, multi-relational knowledge graphs often exhibit multiple simultaneous hierarchies, which current hyperbolic models do not capture. To address this, we propose a model that embeds multi-relational graph data in the Poincar\\'e ball model of hyperbolic space. Our Multi-Relational Poincar\\'e model (MuRP) learns relation-specific parameters to transform entity embeddings by M\\\"obius matrix-vector multiplication and M\\\"obius addition. Experiments on the hierarchical WN18RR knowledge graph show that our Poincar\\'e embeddings outperform their Euclidean counterpart and existing embedding methods on the link prediction task, particularly at lower dimensionality.",
        "date": "2023-03-04",
        "tags": [
            "computer science - machine learning",
            "statistics - machine learning"
        ]
    },
    "https://github.com/mklimasz/TransE-PyTorch": {
        "extra-tags": [],
        "title": "mklimasz/TransE-PyTorch",
        "summary": "Implementation of TransE model in PyTorch. Contribute to mklimasz/TransE-PyTorch development by creating an account on GitHub.",
        "date": "2023-03-04",
        "tags": []
    },
    "https://www.aclweb.org/anthology/D15-1174": {
        "extra-tags": [],
        "title": "Representing Text for Joint Embedding of Text and Knowledge Bases",
        "summary": "",
        "date": "2023-03-04",
        "tags": []
    },
    "http://ieeexplore.ieee.org/document/8047276/": {
        "extra-tags": [],
        "title": "Knowledge Graph Embedding: A Survey of Approaches and Applications",
        "summary": "Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can bene\ufb01t a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-the-arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are \ufb01rst introduced. We describe the overall framework, speci\ufb01c model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus speci\ufb01cally on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we brie\ufb02y introduce how KG embedding can be applied to and bene\ufb01t a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth.",
        "date": "2023-03-04",
        "tags": []
    },
    "http://arxiv.org/abs/1911.06136": {
        "extra-tags": [],
        "title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation",
        "summary": "Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models do not utilize the rich text data. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagE Representation (KEPLER), which can not only better integrate factual knowledge into PLMs but also effectively learn KE through the abundant information in text. In KEPLER, we encode textual descriptions of entities with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performance on various NLP tasks, and also works remarkably well as an inductive KE model on the link prediction task. Furthermore, for pre-training KEPLER and evaluating the KE performance, we construct Wikidata5M, a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The dataset can be obtained from https://deepgraphlearning.github.io/project/wikidata5m.",
        "date": "2023-03-04",
        "tags": [
            "computer science - computation and language"
        ]
    },
    "http://arxiv.org/abs/1412.6575": {
        "extra-tags": [],
        "title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases",
        "summary": "We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as \"BornInCity(a,b) and CityInCountry(b,c) => Nationality(a,c)\". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning.",
        "date": "2023-03-04",
        "tags": [
            "computer science - computation and language"
        ]
    },
    "http://arxiv.org/abs/2002.00819": {
        "extra-tags": [],
        "title": "Knowledge Graph Embedding for Link Prediction: A Comparative Analysis",
        "summary": "Knowledge Graphs (KGs) have found many applications in industry and academic settings, which in turn, have motivated considerable research efforts towards large-scale information extraction from a variety of sources. Despite such efforts, it is well known that even state-of-the-art KGs suffer from incompleteness. Link Prediction (LP), the task of predicting missing facts among entities already a KG, is a promising and widely studied task aimed at addressing KG incompleteness. Among the recent LP techniques, those based on KG embeddings have achieved very promising performances in some benchmarks. Despite the fast growing literature in the subject, insufficient attention has been paid to the effect of the various design choices in those methods. Moreover, the standard practice in this area is to report accuracy by aggregating over a large number of test facts in which some entities are over-represented; this allows LP methods to exhibit good performance by just attending to structural properties that include such entities, while ignoring the remaining majority of the KG. This analysis provides a comprehensive comparison of embedding-based LP methods, extending the dimensions of analysis beyond what is commonly available in the literature. We experimentally compare effectiveness and efficiency of 16 state-of-the-art methods, consider a rule-based baseline, and report detailed analysis over the most popular benchmarks in the literature.",
        "date": "2023-03-04",
        "tags": [
            "belle visualisation",
            "computer science - databases",
            "computer science - machine learning",
            "statistics - machine learning"
        ]
    },
    "http://arxiv.org/abs/2010.03496": {
        "extra-tags": [],
        "title": "Inductive Entity Representations from Text via Link Prediction",
        "summary": "We present a method for learning representations of entities, that uses a Transformerbased architecture as an entity encoder, and link prediction training on a knowledge graph with textual entity descriptions. We demonstrate that our approach can be applied effectively for link prediction in different inductive settings involving entities not seen during training, outperforming related state-of-the-art methods (22% MRR improvement on average). We provide evidence that the learned representations transfer to other tasks that do not require \ufb01ne-tuning the entity encoder. In an entity classi\ufb01cation task we obtain an average improvement of 16% accuracy compared with baselines that also employ pre-trained models. For an information retrieval task, signi\ufb01cant improvements of up to 8.8% in NDCG@10 were obtained for natural language queries.",
        "date": "2023-03-04",
        "tags": [
            "computer science - artificial intelligence",
            "computer science - computation and language"
        ]
    },
    "https://gluebenchmark.com/": {
        "extra-tags": [],
        "title": "GLUE Benchmark",
        "summary": "The General Language Understanding Evaluation (GLUE) benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems",
        "date": "2023-03-04",
        "tags": []
    },
    "https://www.aclweb.org/anthology/D19-1005.pdf": {
        "extra-tags": [],
        "tags": [
            "multiple knowledge bases",
            "entities and lm",
            "grounded language learning",
            "good",
            "knowledge augmented language models",
            "knowbert",
            "arxiv doc",
            "knowledge graph augmented language models",
            "emnlp 2019",
            "contextualised word representations",
            "nlp using knowledge graphs",
            "knowledge driven embeddings",
            "kd mkb biblio",
            "allen institute for ai a2i"
        ],
        "title": "[1909.04164] Knowledge Enhanced Contextual Word Representations",
        "summary": "Contextual word representations, typically trained on unstructured, unlabeled\ntext, do not contain any explicit grounding to real world entities and are\noften unable to remember facts about those entities. We propose a general\nmethod to embed multiple knowledge bases (KBs) into large scale models, and\nthereby enhance their representations with structured, human-curated knowledge.\nFor each KB, we first use an integrated entity linker to retrieve relevant\nentity embeddings, then update contextual word representations via a form of\nword-to-entity attention. In contrast to previous approaches, the entity\nlinkers and self-supervised language modeling objective are jointly trained\nend-to-end in a multitask setting that combines a small amount of entity\nlinking supervision with a large amount of raw text. After integrating WordNet\nand a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert)\ndemonstrates improved perplexity, ability to recall facts as measured in a\nprobing task and downstream performance on relationship extraction, entity\ntyping, and word sense disambiguation. KnowBert's runtime is comparable to\nBERT's and it scales to large KBs.",
        "date": "2019-09-09"
    },
    "https://github.com/raphaelsty/knowledge": {
        "extra-tags": [],
        "date": "2023-02-22",
        "title": "knowledge",
        "summary": "Open-source personal bookmarks search engine",
        "tags": [
            "bookmarks",
            "github",
            "twitter",
            "knowledge-base",
            "zotero",
            "python",
            "search-engine",
            "hacker-news"
        ]
    },
    "https://github.com/MagicStack/asyncpg": {
        "extra-tags": [],
        "date": "2016-07-19",
        "title": "asyncpg",
        "summary": "A fast PostgreSQL Database Client Library for Python/asyncio.",
        "tags": [
            "async-programming",
            "database-driver",
            "python-3",
            "high-performance",
            "postgresql",
            "asyncio",
            "python",
            "async-python"
        ]
    },
    "https://github.com/pynecone-io/pynecone": {
        "extra-tags": [],
        "date": "2022-10-25",
        "title": "pynecone",
        "summary": "\ud83d\udd78 Web apps in pure Python \ud83d\udc0d",
        "tags": [
            "python-library",
            "infrastructure",
            "webdev",
            "open-source",
            "fullstack",
            "python",
            "python3",
            "web",
            "framework"
        ]
    },
    "https://crfm.stanford.edu/2023/03/13/alpaca.html": {
        "extra-tags": [],
        "title": "Hackernews Alpaca: A strong open-source instruction-following model (stanford.edu)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-14"
    },
    "https://twitter.com/hyperfp/status/1635791091709386752": {
        "extra-tags": [],
        "date": "2023-03-14",
        "title": "Twitter @hyperfp",
        "summary": "@raphaelsrty the same day as GPT-4 release?",
        "tags": [
            "twitter",
            "gpt-4"
        ]
    },
    "https://twitter.com/fishnets88/status/1635653726403796993": {
        "extra-tags": [],
        "date": "2023-03-14",
        "title": "Twitter @fishnets88",
        "summary": "@ceyda_cinarel For phrases -&gt; sense2vec. \nFor bad spelling -&gt; bytepair or floret \nFor sentences -&gt; sentence-transformers\nJust words: spaCy. \n\nI wrote a library that makes it pretty easy to just use any of them: \n\nhttps://t.co/tTK7QASm4U",
        "tags": [
            "twitter"
        ]
    },
    "https://github.com/tatsu-lab/stanford_alpaca": {
        "extra-tags": [],
        "date": "2023-03-10",
        "title": "stanford_alpaca",
        "summary": "Code and documentation to train Stanford's Alpaca models, and generate the data.",
        "tags": [
            "deep-learning",
            "instruction-following",
            "language-model",
            "python"
        ]
    },
    "https://github.com/openrlbenchmark/openrlbenchmark": {
        "extra-tags": [],
        "date": "2022-05-10",
        "title": "openrlbenchmark",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/jolibrain/vicreg-loss": {
        "extra-tags": [
            "pytorch",
            "loss"
        ],
        "date": "2023-03-14",
        "title": "vicreg-loss",
        "summary": "Pytorch implementation of the VICReg loss.",
        "tags": [
            "python"
        ]
    },
    "https://cdn.openai.com/papers/gpt-4.pdf": {
        "extra-tags": [],
        "title": "",
        "summary": "",
        "date": "2023-03-14",
        "tags": [
            "gpt4",
            "openai",
            "paper"
        ]
    },
    "https://github.com/setzer22/llama-rs": {
        "extra-tags": [],
        "date": "2023-03-13",
        "title": "llama-rs",
        "summary": "Run LLaMA inference on CPU, with Rust \ud83e\udd80\ud83d\ude80\ud83e\udd99",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/MaterializeInc/datagen": {
        "extra-tags": [],
        "date": "2022-12-22",
        "title": "datagen",
        "summary": "Generate authentic looking mock data based on a SQL, JSON or Avro schema and produce to Kafka in JSON or Avro format.",
        "tags": [
            "typescript",
            "avro",
            "kafka",
            "sql"
        ]
    },
    "https://github.com/antimatter15/alpaca.cpp": {
        "extra-tags": [],
        "date": "2023-03-16",
        "title": "alpaca.cpp",
        "summary": "Locally run an Instruction-Tuned Chat-Style LLM ",
        "tags": [
            "c"
        ]
    },
    "https://towardsdatascience.com/silhouette-coefficient-validating-clustering-techniques-e976bb81d10c": {
        "extra-tags": [
            "clustering"
        ],
        "title": "Silhouette Coefficient : Validating clustering techniques",
        "summary": "This is my first medium story, so please ignore any grammatical mistake as i am not a native English speaker.",
        "date": "2023-03-16",
        "tags": [
            "distance",
            "silhouette",
            "similarity"
        ]
    },
    "https://github.com/UKPLab/lagonn": {
        "extra-tags": [],
        "date": "2023-02-16",
        "title": "lagonn",
        "summary": "Source code and data for Like a Good Nearest Neighbor",
        "tags": [
            "python"
        ]
    },
    "http://arxiv.org/abs/2302.08957": {
        "extra-tags": [],
        "title": "Like a Good Nearest Neighbor: Practical Content Moderation with Sentence Transformers",
        "summary": "Modern text classification systems have impressive capabilities but are infeasible to deploy and use reliably due to their dependence on prompting and billion-parameter language models. SetFit (Tunstall et al., 2022) is a recent, practical approach that fine-tunes a Sentence Transformer under a contrastive learning paradigm and achieves similar results to more unwieldy systems. Text classification is important for addressing the problem of domain drift in detecting harmful content, which plagues all social media platforms. Here, we propose Like a Good Nearest Neighbor (LaGoNN), an inexpensive modification to SetFit that requires no additional parameters or hyperparameters but modifies input with information about its nearest neighbor, for example, the label and text, in the training data, making novel data appear similar to an instance on which the model was optimized. LaGoNN is effective at the task of detecting harmful content and generally improves performance compared to SetFit. To demonstrate the value of our system, we conduct a thorough study of text classification systems in the context of content moderation under four label distributions.",
        "date": "2023-03-17",
        "tags": [
            "computer science - computation and language"
        ]
    },
    "https://twitter.com/HaihaoShen/status/1636708494404640768": {
        "extra-tags": [],
        "date": "2023-03-17",
        "title": "Twitter @HaihaoShen",
        "summary": "\ud83c\udfafWe released GPT-J-6B INT8 ONNX models (first time for INT8 ONNX LLM\u2753) with ~4x model size reduction while preserving ~99.9% accuracy of FP32 baseline.\n\ud83d\udd25GPT-J-6B INT8 models are now publicly available at Hugging Face model hub!\nhttps://t.co/votWIhhgcr\nhttps://t.co/eMbGVhyzwV",
        "tags": [
            "gpt-j-6b",
            "twitter",
            "fp32",
            "onnx",
            "-6b"
        ]
    },
    "https://twitter.com/StabilityAI/status/1636706439732555777": {
        "extra-tags": [],
        "date": "2023-03-17",
        "title": "Twitter @StabilityAI",
        "summary": "Stability AI is excited to announce the launch of Stable Diffusion Reimagine! \n\nWe invite users to experiment with images and \u2018reimagine\u2019 their designs through Stable Diffusion.\n\nMore info \u2192 https://t.co/lVxsBvwd0u https://t.co/jXsYe8HJ89",
        "tags": [
            "twitter"
        ]
    },
    "https://viper.cs.columbia.edu/": {
        "extra-tags": [],
        "title": "Hackernews ViperGPT: Visual Inference via Python Execution for Reasoning (columbia.edu)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-18"
    },
    "https://workers.cloudflare.com/": {
        "extra-tags": [
            "build",
            "workers"
        ],
        "title": "Cloudflare Workers\u00ae",
        "summary": "Build your next application with Cloudflare Workers",
        "date": "2023-03-17",
        "tags": [
            "cheap",
            "cloud",
            "state less"
        ]
    },
    "https://github.com/0x6b/libgsqlite": {
        "extra-tags": [],
        "title": "Hackernews Libgsqlite: A SQLite extension which loads a Google Sheet as a virtual table (github.com/0x6b)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-19"
    },
    "https://twitter.com/gcabanac/status/1637552844222988289": {
        "extra-tags": [],
        "date": "2023-03-19",
        "title": "Twitter @gcabanac",
        "summary": "\ud83d\ude35\u200d\ud83d\udcab A 2021 book replete with Tortured Phrases: \u201cArtificial Intelligence and Data Mining Approaches in Security Frameworks\u201d published by @WileyGlobal @scrivpub and sold $42 per chapter. \ud83d\udce2RETRACTION required ASAP @WileyInResearch. See\nhttps://t.co/v3mEaCQ1ss (HT @JanneSeppanen) https://t.co/5Y7yhxEt0c",
        "tags": [
            "twitter"
        ]
    },
    "https://bigjpg.com": {
        "extra-tags": [],
        "title": "Bigjpg - AI Super-Resolution lossless image enlarging / upscaling tool using Deep Convolutional Neural Networks",
        "summary": "Bigjpg - Image Super-Resolution for Anime-style artworks using the Deep Convolutional Neural Networks without quality loss. Photos are also supported.",
        "date": "2023-03-19",
        "tags": [
            "ai",
            "midjourney",
            "tool",
            "upscale"
        ]
    },
    "https://github.com/VieVie31/i-like-paintings": {
        "extra-tags": [],
        "date": "2023-03-15",
        "title": "i-like-paintings",
        "summary": "A package for predicting painting appreciation from images using a linear regressor on top of a frozen CLIP model",
        "tags": [
            "python"
        ]
    },
    "https://github.com/exaloop/codon": {
        "extra-tags": [],
        "date": "2021-09-27",
        "title": "codon",
        "summary": "A high-performance, zero-overhead, extensible Python compiler using LLVM",
        "tags": [
            "c++",
            "compiler",
            "parallel-programming",
            "high-performance",
            "gpu-programming",
            "python",
            "llvm"
        ]
    },
    "https://twitter.com/brdskggs/status/1637114268876144640": {
        "extra-tags": [],
        "title": "Hackernews Anti-recruiter prompt injection attack in LinkedIn profile (twitter.com/brdskggs)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-20"
    },
    "https://en.wikipedia.org/wiki/Jaccard_index": {
        "extra-tags": [],
        "title": "Hackernews Jaccard Index (wikipedia.org)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-20"
    },
    "https://twitter.com/mervenoyann/status/1638298629977800706": {
        "extra-tags": [],
        "date": "2023-03-21",
        "title": "Twitter @mervenoyann",
        "summary": "my prediction is that at the end of the day companies will go with open-source reproductions of RLHF based LLMs (for the sake of reducing dependency &amp; improving sustainability) and closed source services will be used by regular folk https://t.co/t2cVM0FQoW",
        "tags": [
            "twitter",
            "llms",
            "rlhf"
        ]
    },
    "https://github.com/stchris/untangle": {
        "extra-tags": [],
        "date": "2011-06-05",
        "title": "untangle",
        "summary": "Converts XML to Python objects",
        "tags": [
            "python",
            "pypi",
            "xml"
        ]
    },
    "https://github.com/JosePauloSavioli/Lavoisier": {
        "extra-tags": [],
        "date": "2019-11-11",
        "title": "Lavoisier",
        "summary": "Format converter for LCI datasets",
        "tags": [
            "lca",
            "ascv",
            "lci",
            "lavoisier",
            "python",
            "converter",
            "format-converter",
            "inventory",
            "acv",
            "conversion"
        ]
    },
    "https://www.gatesnotes.com/The-Age-of-AI-Has-Begun": {
        "extra-tags": [
            "ai"
        ],
        "title": "Hackernews The Age of AI has begun (gatesnotes.com)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-22"
    },
    "https://evanw.github.io/thumbhash/": {
        "extra-tags": [],
        "title": "Hackernews ThumbHash: A better compact image placeholder hash (evanw.github.io)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-23"
    },
    "https://github.com/RadeonOpenCompute/ROCm": {
        "extra-tags": [],
        "date": "2016-03-18",
        "title": "ROCm",
        "summary": "ROCm -  Open Software Platform for GPU Compute",
        "tags": [
            "ruby"
        ]
    },
    "https://github.com/moonshinelabs-ai/moonshine": {
        "extra-tags": [],
        "date": "2023-02-06",
        "title": "moonshine",
        "summary": "Pretrained remote sensing models for the rest of us.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/typst/typst": {
        "extra-tags": [],
        "date": "2019-09-24",
        "title": "typst",
        "summary": "A new markup-based typesetting system that is powerful and easy to learn.",
        "tags": [
            "compiler",
            "typesetting",
            "markup",
            "rust"
        ]
    },
    "https://github.com/online-ml/orac": {
        "extra-tags": [],
        "date": "2022-03-16",
        "title": "orac",
        "summary": "\ud83e\uddab MLOps for (online) machine learning",
        "tags": [
            "python"
        ]
    },
    "https://github.com/huawei-noah/streamDM-Cpp": {
        "extra-tags": [],
        "date": "2015-06-27",
        "title": "streamDM-Cpp",
        "summary": "stream Machine Learning in C++",
        "tags": [
            "c++"
        ]
    },
    "https://github.com/deel-ai/puncc": {
        "extra-tags": [],
        "date": "2022-07-20",
        "title": "puncc",
        "summary": "\ud83d\udc4b Puncc is a python library for predictive uncertainty quantification using conformal prediction.",
        "tags": [
            "python",
            "conformal-prediction",
            "conformal-inference",
            "conformal-regressors",
            "uncertainty-estimation",
            "uncertainty-quantification"
        ]
    },
    "https://github.com/rustformers/llama-rs": {
        "extra-tags": [],
        "date": "2023-03-13",
        "title": "llama-rs",
        "summary": "Run LLaMA inference on CPU, with Rust \ud83e\udd80\ud83d\ude80\ud83e\udd99",
        "tags": [
            "rust"
        ]
    },
    "https://github.com/GerevAI/gerev": {
        "extra-tags": [
            "search"
        ],
        "date": "2023-03-05",
        "title": "gerev",
        "summary": "\ud83e\udde0 AI-powered search engine for your organization  \ud83d\udd0e",
        "tags": [
            "search-engine",
            "ai",
            "python",
            "docker",
            "chatgpt",
            "llm",
            "workplace-search",
            "bert",
            "chatgpt-plugin",
            "chatgpt-plugins",
            "enterprise-search"
        ]
    },
    "https://github.com/locustio/locust": {
        "extra-tags": [],
        "date": "2011-02-17",
        "title": "locust",
        "summary": "Write scalable load tests in plain Python \ud83d\ude97\ud83d\udca8",
        "tags": [
            "load-test",
            "http",
            "python",
            "performance",
            "benchmarking",
            "load-tests",
            "locust",
            "load-generator",
            "load-testing",
            "performance-testing"
        ]
    },
    "https://github.com/r1chardj0n3s/parse": {
        "extra-tags": [],
        "date": "2011-11-17",
        "title": "parse",
        "summary": "Parse strings using a specification based on the Python format() syntax.",
        "tags": [
            "python"
        ]
    },
    "https://github.com/cgarciae/ciclo": {
        "extra-tags": [],
        "date": "2022-10-10",
        "title": "ciclo",
        "summary": "A functional training loops library for JAX",
        "tags": [
            "python",
            "jax"
        ]
    },
    "https://github.com/zincsearch/zincsearch": {
        "extra-tags": [],
        "date": "2021-12-02",
        "title": "zincsearch",
        "summary": "ZincSearch . A lightweight alternative to elasticsearch that requires minimal resources, written in Go.",
        "tags": [
            "modern",
            "elasticsearch",
            "searchengine",
            "search",
            "go",
            "vuejs",
            "opensearch",
            "golang"
        ]
    },
    "https://github.com/tech-branch/tsr": {
        "extra-tags": [],
        "date": "2023-03-16",
        "title": "tsr",
        "summary": "Simple csv-based timetracker for Raycast and Alfred",
        "tags": [
            "alfred",
            "raycast",
            "osx",
            "productivity",
            "python",
            "timetracker",
            "alfred-workflow",
            "extension"
        ]
    },
    "https://github.com/Lightning-AI/lit-llama": {
        "extra-tags": [
            "llama"
        ],
        "date": "2023-03-22",
        "title": "lit-llama",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://github.com/deel-ai/LARD": {
        "extra-tags": [],
        "date": "2023-02-13",
        "title": "LARD",
        "summary": "A runway dataset and a generator of synthetic aerial images with automatic labeling. ",
        "tags": [
            "jupyter notebook"
        ]
    },
    "https://github.com/jolibrain/wheatley": {
        "extra-tags": [],
        "date": "2023-03-29",
        "title": "wheatley",
        "summary": "",
        "tags": [
            "python"
        ]
    },
    "https://www.pinecone.io/learn/autoencoders/": {
        "extra-tags": [],
        "title": "Hackernews Introduction to Autoencoders (pinecone.io)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-30"
    },
    "http://matrixmultiplication.xyz/": {
        "extra-tags": [
            "matrix"
        ],
        "title": "Matrix Multiplication",
        "summary": "",
        "date": "2023-03-29",
        "tags": []
    },
    "https://github.com/Lightning-AI/torchmetrics": {
        "extra-tags": [],
        "date": "2020-12-22",
        "title": "torchmetrics",
        "summary": "Torchmetrics - Machine learning metrics for distributed, scalable PyTorch applications.",
        "tags": [
            "machine-learning",
            "deep-learning",
            "python",
            "metrics",
            "pytorch",
            "data-science",
            "analyses"
        ]
    },
    "https://sites.research.google/usm/": {
        "extra-tags": [],
        "title": "Hackernews Universal Speech Model (research.google)",
        "tags": [
            "hackernews"
        ],
        "summary": "",
        "date": "2023-03-30"
    },
    "https://github.com/jina-ai/finetuner": {
        "extra-tags": [],
        "date": "2021-08-11",
        "title": "finetuner",
        "summary": ":dart: Task-oriented finetuning for better embeddings on neural search",
        "tags": [
            "similarity-learning",
            "transfer-learning",
            "triplet-loss",
            "finetuning",
            "fine-tuning",
            "python",
            "negative-sampling",
            "metric-learning",
            "few-shot-learning",
            "neural-search",
            "pretrained-models",
            "jina",
            "siamese-network"
        ]
    },
    "https://github.com/datawrapper/datawrapper": {
        "extra-tags": [
            "codebase"
        ],
        "date": "2012-07-06",
        "title": "datawrapper",
        "summary": "This is a read-only mirror of a part of our codebase. ",
        "tags": [
            "tool",
            "visualization",
            "ddj",
            "datawrapper",
            "javascript",
            "mapping"
        ]
    }
}